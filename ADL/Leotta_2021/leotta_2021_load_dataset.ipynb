{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1YgsIveo9CPZT2dqdTMvZTafUd-yVqmhS","authorship_tag":"ABX9TyOeJwwePfW5EfRW61l8Tu+c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"Khc4g511HMYk"},"source":["#leotta_2021_load_dataset.ipynb\n","\n","This is a loader for the Leotta et al. Activities of Daily Living dataset with ankle, hip, and wrist data.   Please see the paper [here](https://sepl.dibris.unige.it/publications/2021-leotta-CARE.pdf) and cite the original dataset if you use this in your work.\n","\n","General load_data_time_series info is available at our [IMICS github repository](https://github.com/imics-lab/load_data_time_series)\n","\n","Example usage:\n","\n","    x_train, y_train, x_test, y_test = leotta_2021_load_dataset()\n","  \n","\n","Developed and tested using colab.research.google.com\n","\n","To save as .py version use File > Download .py.   Note that you will have to comment out the !gdown code and manually download the source zip file when running as .py.\n","\n","Author:  [Lee B. Hinkle](https://userweb.cs.txstate.edu/~lbh31/), [IMICS Lab](https://imics.wp.txstate.edu/), Texas State University, 2023\n","\n","<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.\n","\n","TODOs:\n","* Dropping the 'other' label in train and valid arrays only works when one_hot_encode = True.   Need another method to drop based on integer encoding.\n"]},{"cell_type":"code","metadata":{"id":"q6H67o-YARCx","executionInfo":{"status":"ok","timestamp":1684858985362,"user_tz":300,"elapsed":2236,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["import os\n","import shutil #https://docs.python.org/3/library/shutil.html\n","from shutil import unpack_archive # to unzip\n","import urllib.request # to get files from web w/o !wget\n","import requests #for downloading zip file\n","import pandas as pd\n","import numpy as np\n","import time\n","from datetime import datetime, date # to timestamp log file\n","# from tabulate import tabulate # for verbose tables, showing data\n","import matplotlib.pyplot as plt\n","# from tensorflow.keras.utils import to_categorical # for one-hot encoding\n","# from sklearn.model_selection import train_test_split\n","# from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import OneHotEncoder"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# Load shared transform (xforms) functions and utils from IMICS Public Repo\n","\n"],"metadata":{"id":"GZ3Jm4r354nl"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"FagULQSH-69Z","executionInfo":{"status":"ok","timestamp":1684858985364,"user_tz":300,"elapsed":7,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"outputs":[],"source":["def get_web_file(fname, url):\n","    \"\"\"checks for local file, if none downloads from URL.    \n","    :return: nothing\"\"\"\n","    if (os.path.exists(fname)):\n","        print (\"Local\",fname, \"found, skipping download\")\n","    else:\n","        print(\"Downloading\",fname, \"from\", url)\n","        urllib.request.urlretrieve(url, filename=fname)"]},{"cell_type":"code","source":["try:\n","    import load_data_transforms as xforms\n","except:\n","    get_web_file(fname = 'load_data_transforms.py', url = 'https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/load_data_transforms.py')\n","    import load_data_transforms as xforms\n","\n","try:\n","    import load_data_utils as utils  \n","except:\n","    get_web_file(fname = 'load_data_utils.py', url = 'https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/load_data_utils.py')\n","    import load_data_utils as utils"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t7qLPe1l4h2V","executionInfo":{"status":"ok","timestamp":1684858996669,"user_tz":300,"elapsed":11310,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}},"outputId":"aa99cfde-d254-4244-b5fd-9dd713be0ec1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading load_data_transforms.py from https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/load_data_transforms.py\n","Downloading load_data_utils.py from https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/load_data_utils.py\n"]}]},{"cell_type":"markdown","source":["# Global and Dataset Parameters"],"metadata":{"id":"iDsgBVo_BFkc"}},{"cell_type":"code","metadata":{"id":"1LkvTO5hujPH","executionInfo":{"status":"ok","timestamp":1684858996670,"user_tz":300,"elapsed":8,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["# environment and execution parameters\n","my_dir = '.' # replace with absolute path if desired\n","dataset_dir = os.path.join(my_dir,'dataset') # temp dir for processing\n","\n","interactive = True # for exploring data and functions interactively\n","verbose = True\n","\n","log_info = \"\" # a global to append dataset processing info\n","\n","subj_alloc_dict = dict (train_subj = [1,2,7,8], valid_subj = [3,6], test_subj = [4,5])\n","\n","# The label_map_<dataset> contains a mapping from strings to ints for all\n","# possible labels in the entire dataset.   This allows for predictable conversion\n","# regardless of the slices.\n","label_map_leotta = {\"label\":     {'OTHER':0,'RELAX':1,'KEYBOARD_WRITING':2,\n","                                  'LAPTOP':3,'HANDWRITING':4,'HANDWASHING':5,\n","                                  'FACEWASHING':6,'TEETHBRUSH':7,'SWEEPING':8,\n","                                  'VACUUMING':9,'EATING':10,'DUSTING':11,\n","                                  'RUBBING':12,'DOWNSTAIRS':13,'WALKING':14,\n","                                  'WALKING_FAST':15,'UPSTAIRS_FAST':16,\n","                                  'UPSTAIRS':17}} # from README.txt\n","# List of original channels to drop, torn whether this should be here or a \n","# passed parameter.  Most often I use only vector magnitudes.                                \n","leotta_comp_accel = ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z',\n","                     'hip_accel_x', 'hip_accel_y', 'hip_accel_z',\n","                     'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z']"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"_arxQU-n6nKK","executionInfo":{"status":"ok","timestamp":1684858996671,"user_tz":300,"elapsed":8,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["# runs when saved as .py, skip cell if developing/debugging\n","interactive = False \n","verbose = False # note this can be changed after import using xforms.verbose = True"],"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Please go to https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/G23QTS\n","# to read through the terms and also find the proper citations if you use this dataset.\n","# This is also where you can download directly if the link below fails.\n","\n","# Next line must be commented out before saving as .py - then manual download\n","# is required.\n","# !gdown \"1P5PIYeYvbfL4kQj-P2sm-JquUIddAxec&confirm=t\" # ADL_Leotta_2021.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CgZzIUHoS_BB","executionInfo":{"status":"ok","timestamp":1684859003319,"user_tz":300,"elapsed":6655,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}},"outputId":"3b9a98ce-e26d-4a31-c320-d9a0b74286a4"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1P5PIYeYvbfL4kQj-P2sm-JquUIddAxec&confirm=t\n","To: /content/ADL_Leotta_2021.zip\n","100% 258M/258M [00:04<00:00, 63.1MB/s]\n"]}]},{"cell_type":"code","metadata":{"id":"9oab3XMPgL8Z","executionInfo":{"status":"ok","timestamp":1684859003320,"user_tz":300,"elapsed":12,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def unzip_leotta():\n","    \"\"\"check for local copy, if none unzips the dataset structure in working_dir\"\"\"\n","    if (os.path.isdir(dataset_dir)):\n","        print(\"Using existing Leotta archive in\", dataset_dir)\n","        return\n","    else:\n","        print(\"Unzipping Leotta 2021 dataset into\", dataset_dir)\n","        zip_ffname = os.path.join(my_dir, 'ADL_Leotta_2021.zip')\n","        if (os.path.exists(zip_ffname)):\n","            print(\"Using source file\", zip_ffname)\n","            shutil.unpack_archive(zip_ffname,dataset_dir,'zip')\n","        else:\n","            print(\"ERROR: \", zip_ffname, \" not found\")\n","            print(\"Go to https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/G23QTS\")\n","            print(\"Click access dataset on right, accept terms, download zip and place in current directory\")\n","            print(\"with filename ADL_Leotta_2021.zip, should be a 246.2MB zip file\")\n","            return\n","if interactive:\n","    unzip_leotta()"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"ttKS9Ox6JSed","executionInfo":{"status":"ok","timestamp":1684859003320,"user_tz":300,"elapsed":10,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def df_from_csv (\n","    sub_num, # 1 - 8\n","    sensor_loc): # ankle, hip, wrist\n","    \"\"\"reads Leotta_2021 csv, returns df with accel x/y/z/ttl, label, sub_num\n","    args:\n","        sub_num (int) subject number from 1 to 8\n","        sensor_loc (string) sensor location ankle, hip, or wrist\n","    returns:\n","        An IR1 format dataframe, note labels are int encoded as in the raw dataset\"\"\"\n","    fnameX = sensor_loc + '_X_0' + str(sub_num) +  '.csv'\n","    fnamey = sensor_loc + '_Y_0' + str(sub_num) +  '.csv'\n","    ffnameX = os.path.join(dataset_dir, sensor_loc, fnameX)\n","    ffnamey = os.path.join(dataset_dir, sensor_loc, fnamey)\n","    if verbose:\n","        print ('df_from_csv processing: ', ffnameX, ffnamey)\n","    df = pd.read_csv(ffnameX)\n","    if (sensor_loc == 'wrist'): # Centrepoint device has different header name\n","        df.rename(columns={'Timestamp UTC': 'Timestamp'}, inplace=True)\n","    # the imported Timestamp is an object - need to convert to DateTime\n","    # in order to set the index to DateTime format.  Enables resampling etc.\n","    # Leaving these here - helpful to debug if leveraging this code!\n","        #print(\"*** Start ***\")\n","        #print(type(df.index))\n","        #print(df.info(verbose=True))  \n","    df['Timestamp'] = pd.to_datetime(df['Timestamp']) \n","    df.set_index('Timestamp', drop = True, inplace = True)\n","    if (sensor_loc != 'wrist'): # Centrepoint doesn't have non-accel columnns\n","        df = df.drop(['Temperature','Gyroscope X','Gyroscope Y','Gyroscope Z',\n","                      'Magnetometer X','Magnetometer Y','Magnetometer Z'], axis=1)\n","    df_sqd = df.pow(2)[['Accelerometer X','Accelerometer Y','Accelerometer Z']] #square each accel\n","    df_sum = df_sqd.sum(axis=1) #add sum of squares, new 1 col df\n","    df.loc[:,'accel_ttl'] = df_sum.pow(0.5)-1  # sqrt and remove 1g due to gravity\n","    del df_sqd, df_sum\n","    df.columns = [sensor_loc + '_accel_x', sensor_loc + '_accel_y', sensor_loc + '_accel_z', sensor_loc + '_accel_ttl']\n","    # tighten up the column types for space savings, probably should be function in utils or xforms\n","    # change to 32-bit, credit/ref https://stackoverflow.com/questions/69188132/how-to-convert-all-float64-columns-to-float32-in-pandas\n","    # Select columns with 'float64' dtype  \n","    float64_cols = list(df.select_dtypes(include='float64'))\n","    # The same code again calling the columns\n","    df[float64_cols] = df[float64_cols].astype('float32')\n","    # add activity numbers - number of rows are the same in this dataset\n","    # Why doesn't this work? df['label'] = pd.read_csv(ffnamey, dtype='Int64')\n","    dfy = pd.read_csv(ffnamey)\n","    df['label']=dfy['label'].to_numpy() # this works, above doesn't?\n","    df['label'] = df['label'].astype(np.int8) # change from float to int\n","    del dfy\n","    # add column with subject number\n","    df['sub'] = np.int8(sub_num)\n","    return df\n","if interactive:\n","    snum = 1\n","    df_ankle = df_from_csv(sub_num = snum, sensor_loc = 'ankle')\n","    df_hip = df_from_csv(sub_num = snum, sensor_loc = 'hip')\n","    df_wrist = df_from_csv(sub_num = snum, sensor_loc = 'wrist')\n","    display(df_ankle.info())\n","    display(df_hip.info())\n","    display(df_wrist.info())\n","    display(df_ankle.head())\n","    display(df_hip.head())\n","    display(df_wrist.head())\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"ikZjlyBtk2fj","executionInfo":{"status":"ok","timestamp":1684859003321,"user_tz":300,"elapsed":10,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def df_from_one_sub (sub_num): # 1 - 8\n","    \"\"\"reads 3 csv files for a single subject, combines an returns a single dataframe\"\"\"\n","    my_sub_num = sub_num # not sure necessary but easier to follow...\n","    df_ankle = df_from_csv(sub_num = my_sub_num, sensor_loc = 'ankle')\n","    df_hip = df_from_csv(sub_num = my_sub_num, sensor_loc = 'hip')\n","    #wrist is a bit more complicated since the sample rate is different\n","    df_wrist = df_from_csv(sub_num = my_sub_num, sensor_loc = 'wrist')\n","    df_wrist = xforms.to_fixed_ir1_timedelta(df_wrist,new_time_step='10ms')\n","\n","    if ((df_ankle['label'].equals(df_hip['label']))\n","            and (df_ankle['sub'].equals(df_hip['sub']))\n","            and (df_ankle['label'].equals(df_wrist['label']))\n","            and (df_ankle['sub'].equals(df_wrist['sub']))) :\n","        if verbose:\n","            print('confirmed label and sub match - dropping from ankle and hip')\n","        df_ankle.drop(['label','sub'], axis=1, inplace=True)\n","        df_hip.drop(['label','sub'], axis=1, inplace=True)\n","    else:\n","        print('Error:  label and sub do not match, cannot combine dataframes')\n","        print('label match = ',df_ankle['label'].equals(df_hip['label']))\n","        print('sub match = ',df_ankle['sub'].equals(df_hip['sub']))\n","    df_temp = df_ankle.join(df_hip)\n","    df_final = df_temp.join(df_wrist)\n","    del df_temp\n","    df_final = xforms.convert_ir1_labels_to_strings(df = df_final, label_map = label_map_leotta)\n","    return df_final\n","if interactive:\n","    df_temp = df_from_one_sub (sub_num = 8)\n","    print(type(df_temp.index)) # should be DateTimeIndex\n","    print(df_temp.info(verbose=True))\n","    display(df_temp.head()) "],"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def get_leotta_ir1_dict():\n","    \"\"\"reads the Leotta dataset and converts each \"session file\" to an IR1\n","    dataframe.  The goal here is to capture and convert all raw data into\n","    a 2D dataframe of rows = datetime index of each sample, columns = {channels,\n","    label(s), subject_num}.  Additional methods may be used to drop channels,\n","    and convert the string labels to mapped ints prior to switch to ndarrays.\n","    Args:\n","        none \n","    Returns: a dict containing key = df_name and item = IR1 dataframes.\"\"\"\n","    unzip_leotta()\n","    ir1_df_dict = dict() # an empty dictionary\n","    for i in range(1,9):\n","        ir1_name = \"Leotta_Sub\" + str(i)\n","        if verbose:\n","            print('get_leotta_ir1_dict is processing subject number', i, \"as\", ir1_name)\n","        df_temp = df_from_one_sub (sub_num = i)\n","        ir1_df_dict[ir1_name]=df_temp # key is root name in the file\n","    return ir1_df_dict\n","if interactive:\n","    verbose = False\n","    ir1_dict = get_leotta_ir1_dict()\n","    print('IR1 dataframes:',ir1_dict.keys())\n","    for df_name, df in ir1_dict.items():\n","        display(df.head())\n","        break # just want one\n","    verbose = True"],"metadata":{"id":"p_EmZhbavtzi","executionInfo":{"status":"ok","timestamp":1684859003322,"user_tz":300,"elapsed":10,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# Convert IR1 dataframes to IR2 numpy arrays"],"metadata":{"id":"XLDCXLnCUpRn"}},{"cell_type":"code","source":["# this should be moved to xforms once better tested.\n","def one_hot_by_label_dict(y, label_map_in):\n","    \"\"\"One hot encode using dictionary so that the encoding is consistent\n","    even if some classes are missing from the train set (happens especially\n","    on X-fold Cross-Validation with sparse labels)\n","    params:\n","        y = numpy array with integer encoding\n","        label_map_in = dict with 'label' entry containing all possible classes\n","    returns:\n","        y = one-hot encoded ndarray, # columns = # classes in dict entry.\"\"\"\n","    # ref: https://stackoverflow.com/questions/66644733/how-to-add-your-own-categories-into-the-onehotencoder\n","    if verbose:\n","        print(\"y shape into one_hot_by_label_dict is\",y.shape)\n","        print(\"length of label map\", len(label_map_in['label']), \"equals the max number of classes\")\n","        unique, counts = np.unique(y, return_counts=True)\n","        print(\"y counts\\n\",np.asarray((unique, counts)).T)\n","    #all_categories = np.array([str(i) for i in range(len(label_map['label']))])\n","    all_categories = [[str(i) for i in range(len(label_map_in['label']))]]\n","    enc = OneHotEncoder(categories = all_categories, sparse = False)\n","    y_oh = enc.fit_transform(y)\n","    y_oh = y_oh.astype('uint8')\n","    if verbose:\n","        print(\"Shape of returned array\", y_oh.shape)\n","    return y_oh\n"],"metadata":{"id":"U2j_xaY-MnWj","executionInfo":{"status":"ok","timestamp":1684859003323,"user_tz":300,"elapsed":10,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# this should be moved to xforms once better tested and there should also be\n","# a version for integer encoded labels\n","def drop_ir2_one_hot_column(x, y, sub, ss_times, y_col_index = 0):\n","    \"\"\"Drops all windows where the one-hot column is 1.  Used to get rid of\n","    labels in IR2 that should not be in train or valid set, e.g. 'other'\n","    args:\n","        x,y,sub,ss_times are all IR2 arrays with y already one-hot encoded\n","        y_col_index = the y column to drop\n","    returs:\n","        updated IR2 arrays\"\"\"\n","    remove_index = np.where(y[:,y_col_index]!=1) # the boolean on this is trippy\n","    x = x[remove_index]\n","    y = y[remove_index]\n","    sub = sub[remove_index]\n","    ss_times = ss_times[remove_index]\n","    return x, y, sub, ss_times"],"metadata":{"id":"y0wQbnhOXwAF","executionInfo":{"status":"ok","timestamp":1684859003323,"user_tz":300,"elapsed":9,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def leotta_2021_load_dataset(\n","    incl_val_group = False, # split train into train and validate\n","    one_hot_encode = False, # make y into multi-column one-hot encoded\n","    ):\n","    \"\"\"Loads the Leotta dataset zip from current directory, processes the data,\n","    and returns arrays by separating into _train, _validate, and _test arrays\n","    for X and y based on split_sub dictionary.\"\"\"\n","    # dataset parameters, these are set as globals in the xforms code\n","    xforms.time_steps = 300 # three seconds at 100Hz\n","    xforms.stride = 300 # no overlap of the sliding windows\n","    global log_info\n","    log_info = \"Generated by leotta_2021_load_data.ipynb\\n\"\n","    today = date.today()\n","    log_info += today.strftime(\"%B %d, %Y\") + \"\\n\"\n","    log_info += \"sub dict = \" + str(subj_alloc_dict) + \"\\n\"\n","    # Iterate through the IR1s in the dictionary, determine train-vs-test\n","    # then convert to IR2.  Much of this code was pulled from xform get_ir3_from_dict\n","    # which was used by TWristAR and heavily modified here.  This seems pretty \n","    # close to being a generic version that could be put into transforms.\n","    label_map = label_map_leotta\n","    ir1_dict = get_leotta_ir1_dict()\n","\n","    # Empty lists - it is better to make lists versus appending in the loop\n","    x_train_list, y_train_list, sub_train_list, ss_times_train_list = ([] for i in range(4))\n","    x_valid_list, y_valid_list, sub_valid_list, ss_times_valid_list = ([] for i in range(4))\n","    x_test_list, y_test_list, sub_test_list, ss_times_test_list = ([] for i in range(4))\n","    # iterate through the IR1 dataframes in the dictionary, process and allocate\n","    # to the train/valid/test lists.\n","    for df_name, df in ir1_dict.items():\n","        if verbose:\n","            print(\"leotta_2021_load_dataset is processing\",df_name)\n","        df = xforms.assign_ints_ir1_labels(df,label_mapping_dict=label_map)\n","        df = xforms.drop_ir1_columns(df, drop_col_list = leotta_comp_accel)\n","        if (df['sub'].nunique() != 1):\n","            print(\"WARNING: IR1\", df_name, \"contains multiple subjects\")\n","        sub_num = df['sub'].mode()[0] # since only one column it is a series not df\n","        x_temp, y_temp, sub_temp, ss_times_temp, ch_list_temp = xforms.get_ir2_from_ir1(df)\n","        if sub_num in subj_alloc_dict['train_subj']:\n","            if verbose:\n","                print('Allocating Subject',sub_num, 'to train')\n","            x_train_list.append(x_temp)\n","            y_train_list.append(y_temp)\n","            sub_train_list.append(sub_temp)\n","            ss_times_train_list.append(ss_times_temp)\n","        elif sub_num in subj_alloc_dict['valid_subj']:\n","            if verbose:\n","                print('Allocating Subject',sub_num, 'to valid')\n","            x_valid_list.append(x_temp)\n","            y_valid_list.append(y_temp)\n","            sub_valid_list.append(sub_temp)\n","            ss_times_valid_list.append(ss_times_temp)\n","        elif sub_num in subj_alloc_dict['test_subj']:\n","            if verbose:\n","                print('Allocating Subject',sub_num, 'to test')\n","            x_test_list.append(x_temp)\n","            y_test_list.append(y_temp)\n","            sub_test_list.append(sub_temp)\n","            ss_times_test_list.append(ss_times_temp)\n","        else:\n","            print('WARNING: Subject',sub_num,'not found in subj_alloc_dict, discarding')\n","\n","    # https://stackoverflow.com/questions/27516849/how-to-convert-list-of-numpy-arrays-into-single-numpy-array\n","    x_train = np.vstack(x_train_list)\n","    y_train = np.vstack(y_train_list)\n","    sub_train = np.vstack(sub_train_list)\n","    ss_times_train = np.vstack(ss_times_train_list)\n","    x_train, y_train, sub_train, ss_times_train = xforms.unify_ir2_labels(x_train, y_train, sub_train, ss_times_train, method = 'drop')\n","    #print(utils.tabulate_numpy_arrays({'x_train':x_train, 'y_train':y_train, 'sub_train':sub_train, 'ss_times_train': ss_times_train}))\n","\n","    x_valid = np.vstack(x_valid_list)\n","    y_valid = np.vstack(y_valid_list)\n","    sub_valid = np.vstack(sub_valid_list)\n","    ss_times_valid = np.vstack(ss_times_valid_list)\n","    x_valid, y_valid, sub_valid, ss_times_valid = xforms.unify_ir2_labels(x_valid, y_valid, sub_valid, ss_times_valid, method = 'drop')\n","    #print(utils.tabulate_numpy_arrays({'x_valid':x_valid, 'y_valid':y_valid, 'sub_valid':sub_valid, 'ss_times_valid': ss_times_valid}))\n","\n","    x_test = np.vstack(x_test_list)\n","    y_test = np.vstack(y_test_list)\n","    sub_test = np.vstack(sub_test_list)\n","    ss_times_test = np.vstack(ss_times_test_list)\n","    x_test, y_test, sub_test, ss_times_test = xforms.unify_ir2_labels(x_test, y_test, sub_test, ss_times_test, method = 'mode')\n","    #print(utils.tabulate_numpy_arrays({'x_test':x_test, 'y_test':y_test, 'sub_test':sub_test, 'ss_times_test': ss_times_test}))\n","\n","    if (one_hot_encode):\n","        y_train = one_hot_by_label_dict(y = y_train, label_map_in = label_map)\n","        y_valid = one_hot_by_label_dict(y = y_valid, label_map_in = label_map)\n","        y_test = one_hot_by_label_dict(y = y_test, label_map_in = label_map)\n","\n","        x_train, y_train, sub_train, ss_times_train = drop_ir2_one_hot_column(x_train, y_train, sub_train, ss_times_train, y_col_index = 0)\n","        x_valid, y_valid, sub_valid, ss_times_valid = drop_ir2_one_hot_column(x_valid, y_valid, sub_valid, ss_times_valid, y_col_index = 0)\n","\n","    \n","    if (incl_val_group):\n","        return x_train, y_train, x_valid, y_valid, x_test, y_test\n","    else:\n","        return np.vstack([x_train,x_valid]), np.vstack([y_train,y_valid]), x_test, y_test"],"metadata":{"id":"b97K7cNITnxY","executionInfo":{"status":"ok","timestamp":1684859003517,"user_tz":300,"elapsed":202,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"MaT1dfqavvtk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684859103499,"user_tz":300,"elapsed":99989,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}},"outputId":"ab4ec7f5-5273-40a3-c986-4fee49621c30"},"source":["if __name__ == \"__main__\":\n","    print(\"Downloading and processing Leotta 2021 dataset\")\n","    print(\"Building dictionary of IR1 dataframes\")\n","    ir1_dict = get_leotta_ir1_dict()\n","    print('IR1 dataframes:',ir1_dict.keys())\n","    # for df_name, df in ir1_dict.items():\n","    #     print(df.head())\n","    #     break # just want one\n","    \n","    x_train, y_train, x_test, y_test = leotta_2021_load_dataset()\n","    print(\"\\nreturned arrays without validation group:\")\n","    print(\"x_train shape \",x_train.shape,\" y_train shape \", y_train.shape)\n","    print(\"x_test shape  \",x_test.shape,\" y_test shape  \",y_test.shape)\n","\n","    x_train, y_train, x_validation, y_validation, x_test, y_test = leotta_2021_load_dataset(incl_val_group=True)\n","    print(\"\\nreturned arrays with validation group:\")\n","    print(\"x_train shape \",x_train.shape,\" y_train shape \", y_train.shape)\n","    print(\"x_validation shape \",x_validation.shape,\" y_validation shape \", y_validation.shape)\n","    print(\"x_test shape  \",x_test.shape,\" y_test shape  \",y_test.shape)\n","\n","    x_train, y_train, x_validation, y_validation, x_test, y_test = leotta_2021_load_dataset(incl_val_group=True, one_hot_encode = True)\n","    print(\"\\nreturned arrays with validation group and one-hot encoded:\")\n","    print(\"x_train shape \",x_train.shape,\" y_train shape \", y_train.shape)\n","    print(\"x_validation shape \",x_validation.shape,\" y_validation shape \", y_validation.shape)\n","    print(\"x_test shape  \",x_test.shape,\" y_test shape  \",y_test.shape)\n","    print(10*'-', \"Contents of log_info\", 10*'-')\n","    print(log_info)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading and processing Leotta 2021 dataset\n","Building dictionary of IR1 dataframes\n","Unzipping Leotta 2021 dataset into ./dataset\n","Using source file ./ADL_Leotta_2021.zip\n","IR1 dataframes: dict_keys(['Leotta_Sub1', 'Leotta_Sub2', 'Leotta_Sub3', 'Leotta_Sub4', 'Leotta_Sub5', 'Leotta_Sub6', 'Leotta_Sub7', 'Leotta_Sub8'])\n","Using existing Leotta archive in ./dataset\n","\n","returned arrays without validation group:\n","x_train shape  (5488, 300, 3)  y_train shape  (5488, 1)\n","x_test shape   (1987, 300, 3)  y_test shape   (1987, 1)\n","Using existing Leotta archive in ./dataset\n","\n","returned arrays with validation group:\n","x_train shape  (3733, 300, 3)  y_train shape  (3733, 1)\n","x_validation shape  (1755, 300, 3)  y_validation shape  (1755, 1)\n","x_test shape   (1987, 300, 3)  y_test shape   (1987, 1)\n","Using existing Leotta archive in ./dataset\n","\n","returned arrays with validation group and one-hot encoded:\n","x_train shape  (2391, 300, 3)  y_train shape  (2391, 18)\n","x_validation shape  (1167, 300, 3)  y_validation shape  (1167, 18)\n","x_test shape   (1987, 300, 3)  y_test shape   (1987, 18)\n","---------- Contents of log_info ----------\n","Generated by leotta_2021_load_data.ipynb\n","May 23, 2023\n","sub dict = {'train_subj': [1, 2, 7, 8], 'valid_subj': [3, 6], 'test_subj': [4, 5]}\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]}]}]}