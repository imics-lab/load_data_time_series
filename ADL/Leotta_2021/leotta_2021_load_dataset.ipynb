{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1YgsIveo9CPZT2dqdTMvZTafUd-yVqmhS","authorship_tag":"ABX9TyNypRH+qAW8xpJ8d9nxaay+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Khc4g511HMYk"},"source":["#leotta_2021_load_dataset.ipynb\n","This data set loader uses the leotta_2021_get_X_y_sub.py file generated by downloading the python version of the same name Jupyter notebook.\n","\n","It will perform a train/test (and optional validation) split and one-hot encode the activity labels.   Returns x/y_train and x/y_test numpy arrays that may be fed directly into a neural net model.\n","\n","Example usage:\n","\n","    x_train, y_train, x_test, y_test = leotta_2021_load_dataset()\n","  \n","\n","Developed and tested using colab.research.google.com\n","IMPORTANT a high RAM runtime is required. Select runtime > change type > shape = high RAM  \n","To save as .py version use File > Download .py\n","\n","Author:  [Lee B. Hinkle](https://userweb.cs.txstate.edu/~lbh31/), [IMICS Lab](https://imics.wp.txstate.edu/), Texas State University, 2023\n","\n","<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.\n","\n","TODOs:\n","* Integration of get_X_y_sub code and conversion to the more standard format in progress.\n"]},{"cell_type":"code","metadata":{"id":"q6H67o-YARCx","executionInfo":{"status":"ok","timestamp":1684357015409,"user_tz":300,"elapsed":150,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["import os\n","import shutil #https://docs.python.org/3/library/shutil.html\n","from shutil import unpack_archive # to unzip\n","import urllib.request # to get files from web w/o !wget\n","import requests #for downloading zip file\n","import pandas as pd\n","import numpy as np\n","# from tabulate import tabulate # for verbose tables, showing data\n","import matplotlib.pyplot as plt\n","# from tensorflow.keras.utils import to_categorical # for one-hot encoding\n","# from sklearn.model_selection import train_test_split\n","# from sklearn.preprocessing import LabelEncoder\n","# from sklearn.preprocessing import OneHotEncoder"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["# Load shared transform (xforms) functions and utils from IMICS Public Repo\n","\n"],"metadata":{"id":"GZ3Jm4r354nl"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"FagULQSH-69Z","executionInfo":{"status":"ok","timestamp":1684356301706,"user_tz":300,"elapsed":4,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"outputs":[],"source":["def get_web_file(fname, url):\n","    \"\"\"checks for local file, if none downloads from URL.    \n","    :return: nothing\"\"\"\n","    if (os.path.exists(fname)):\n","        print (\"Local\",fname, \"found, skipping download\")\n","    else:\n","        print(\"Downloading\",fname, \"from\", url)\n","        urllib.request.urlretrieve(url, filename=fname)"]},{"cell_type":"code","source":["try:\n","    import load_data_transforms as xforms\n","except:\n","    get_web_file(fname = 'load_data_transforms.py', url = 'https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/load_data_transforms.py')\n","    import load_data_transforms as xforms\n","\n","try:\n","    import load_data_utils as utils  \n","except:\n","    get_web_file(fname = 'load_data_utils.py', url = 'https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/load_data_utils.py')\n","    import load_data_utils as utils"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t7qLPe1l4h2V","executionInfo":{"status":"ok","timestamp":1684356307999,"user_tz":300,"elapsed":6297,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}},"outputId":"937e5600-cbaa-4101-d8c7-1f39b18ca75f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading load_data_transforms.py from https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/load_data_transforms.py\n","Downloading load_data_utils.py from https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/load_data_utils.py\n"]}]},{"cell_type":"markdown","source":["# Global and Dataset Parameters"],"metadata":{"id":"iDsgBVo_BFkc"}},{"cell_type":"code","metadata":{"id":"1LkvTO5hujPH","executionInfo":{"status":"ok","timestamp":1684356308000,"user_tz":300,"elapsed":19,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["# environment and execution parameters\n","my_dir = '.' # replace with absolute path if desired\n","dataset_dir = os.path.join(my_dir,'dataset') # temp dir for processing\n","\n","interactive = True # for exploring data and functions interactively\n","verbose = True\n","\n","log_info = \"\" # a global to append dataset processing info\n","\n","# dataset parameters\n","all_channel_list = ['accel_x', 'accel_y', 'accel_z','accel_ttl','bvp','eda','p_temp']\n","# frequency = 32 - unlike some of the other loaders this is hardcoded due to\n","# the unique sample freqencies that differ between the individual e4 sensors\n","xforms.time_steps = 96 # three seconds at 32Hz\n","xforms.stride = 32 # one second step for each sliding window\n","# The label_map_<dataset> contains a mapping from strings to ints for all\n","# possible labels in the entire dataset.   This allows for predictable conversion\n","# regardless of the slices.\n","subj_alloc_dict = dict (train_subj = [1,2,7,8], valid_subj = [3,6], test_subj = [4,5])\n","label_map_leotta = {\"label\":     {'OTHER':0,'RELAX':1,'KEYBOARD_WRITING':2,\n","                                  'LAPTOP':3,'HANDWRITING':4,'HANDWASHING':5,\n","                                  'FACEWASHING':6,'TEETHBRUSH':7,'SWEEPING':8,\n","                                  'VACUUMING':9,'EATING':10,'DUSTING':11,\n","                                  'RUBBING':12,'DOWNSTAIRS':13,'WALKING':14,\n","                                  'WALKING_FAST':15,'UPSTAIRS_FAST':16,\n","                                  'UPSTAIRS':17}} # from README.txt"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"_arxQU-n6nKK","executionInfo":{"status":"ok","timestamp":1684356308000,"user_tz":300,"elapsed":18,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["interactive = False # runs when saved as .py, skip cell if developing/debugging"],"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Please go to https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/G23QTS\n","# to read through the terms and also find the proper citations if you use this dataset.\n","# This is also where you can download directly if the link below fails.\n","!gdown \"1P5PIYeYvbfL4kQj-P2sm-JquUIddAxec&confirm=t\" # ADL_Leotta_2021.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CgZzIUHoS_BB","executionInfo":{"status":"ok","timestamp":1684356310858,"user_tz":300,"elapsed":2875,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}},"outputId":"63132846-eb2a-4f8f-af30-119ebd7490f2"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1P5PIYeYvbfL4kQj-P2sm-JquUIddAxec&confirm=t\n","To: /content/ADL_Leotta_2021.zip\n","100% 258M/258M [00:01<00:00, 135MB/s]\n"]}]},{"cell_type":"markdown","source":["The !gdown above works in colab but not as .py.   Need to see if an alternate method is possible."],"metadata":{"id":"yE84tsx-hZHC"}},{"cell_type":"code","source":["# import gdown\n","# url = 'https://drive.google.com/file/d/1P5PIYeYvbfL4kQj-P2sm-JquUIddAxec/view?usp=share_link' \n","# out_path = 'ADL_Leotta_2021.zip'\n","# gdown.download(url, out_path, quiet=False)"],"metadata":{"id":"W1i0zdPSghpX"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9oab3XMPgL8Z","executionInfo":{"status":"ok","timestamp":1684356310859,"user_tz":300,"elapsed":5,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def unzip_leotta():\n","    \"\"\"check for local copy, if none unzips the dataset structure in working_dir\"\"\"\n","    if (os.path.isdir(dataset_dir)):\n","        print(\"Using existing Leotta archive in\", dataset_dir)\n","        return\n","    else:\n","        print(\"Unzipping Leotta 2021 dataset into\", dataset_dir)\n","        zip_ffname = os.path.join(my_dir, 'ADL_Leotta_2021.zip')\n","        if (os.path.exists(zip_ffname)):\n","            print(\"Using source file\", zip_ffname)\n","            shutil.unpack_archive(zip_ffname,dataset_dir,'zip')\n","        else:\n","            print(\"ERROR: \", zip_ffname, \" not found\")\n","            print(\"Go to https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/G23QTS\")\n","            print(\"Click access dataset on right, accept terms, download zip and place in current directory\")\n","            print(\"with filename ADL_Leotta_2021.zip, should be a 246.2MB zip file\")\n","            return\n","if interactive:\n","    unzip_leotta()"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"ttKS9Ox6JSed","executionInfo":{"status":"ok","timestamp":1684356311636,"user_tz":300,"elapsed":13,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def df_from_csv (\n","    sub_num, # 1 - 8\n","    sensor_loc): # ankle, hip, wrist\n","    \"\"\"reads Leotta_2021 csv, returns df with accel x/y/z/ttl, label, sub_num\n","    args:\n","        sub_num (int) subject number from 1 to 8\n","        sensor_loc (string) sensor location ankle, hip, or wrist\n","    returns:\n","        An IR1 format dataframe, note labels are int encoded as in the raw dataset\"\"\"\n","    fnameX = sensor_loc + '_X_0' + str(sub_num) +  '.csv'\n","    fnamey = sensor_loc + '_Y_0' + str(sub_num) +  '.csv'\n","    ffnameX = os.path.join(dataset_dir, sensor_loc, fnameX)\n","    ffnamey = os.path.join(dataset_dir, sensor_loc, fnamey)\n","    if verbose:\n","        print ('df_from_csv processing: ', ffnameX, ffnamey)\n","    df = pd.read_csv(ffnameX)\n","    if (sensor_loc == 'wrist'): # Centrepoint device has different header name\n","        df.rename(columns={'Timestamp UTC': 'Timestamp'}, inplace=True)\n","    # the imported Timestamp is an object - need to convert to DateTime\n","    # in order to set the index to DateTime format.  Enables resampling etc.\n","    # Leaving these here - helpful to debug if leveraging this code!\n","        #print(\"*** Start ***\")\n","        #print(type(df.index))\n","        #print(df.info(verbose=True))  \n","    df['Timestamp'] = pd.to_datetime(df['Timestamp']) \n","    df.set_index('Timestamp', drop = True, inplace = True)\n","    if (sensor_loc != 'wrist'): # Centrepoint doesn't have non-accel columnns\n","        df = df.drop(['Temperature','Gyroscope X','Gyroscope Y','Gyroscope Z',\n","                      'Magnetometer X','Magnetometer Y','Magnetometer Z'], axis=1)\n","    df_sqd = df.pow(2)[['Accelerometer X','Accelerometer Y','Accelerometer Z']] #square each accel\n","    df_sum = df_sqd.sum(axis=1) #add sum of squares, new 1 col df\n","    df.loc[:,'accel_ttl'] = df_sum.pow(0.5)-1  # sqrt and remove 1g due to gravity\n","    del df_sqd, df_sum\n","    df.columns = [sensor_loc + '_accel_x', sensor_loc + '_accel_y', sensor_loc + '_accel_z', sensor_loc + '_accel_ttl']\n","    # tighten up the column types for space savings, probably should be function in utils or xforms\n","    # change to 32-bit, credit/ref https://stackoverflow.com/questions/69188132/how-to-convert-all-float64-columns-to-float32-in-pandas\n","    # Select columns with 'float64' dtype  \n","    float64_cols = list(df.select_dtypes(include='float64'))\n","    # The same code again calling the columns\n","    df[float64_cols] = df[float64_cols].astype('float32')\n","    # add activity numbers - number of rows are the same in this dataset\n","    # Why doesn't this work? df['label'] = pd.read_csv(ffnamey, dtype='Int64')\n","    dfy = pd.read_csv(ffnamey)\n","    df['label']=dfy['label'].to_numpy() # this works, above doesn't?\n","    df['label'] = df['label'].astype(np.int8) # change from float to int\n","    del dfy\n","    # add column with subject number\n","    df['sub'] = np.int8(sub_num)\n","    return df\n","if interactive:\n","    snum = 1\n","    df_ankle = df_from_csv(sub_num = snum, sensor_loc = 'ankle')\n","    df_hip = df_from_csv(sub_num = snum, sensor_loc = 'hip')\n","    df_wrist = df_from_csv(sub_num = snum, sensor_loc = 'wrist')\n","    display(df_ankle.info())\n","    display(df_hip.info())\n","    display(df_wrist.info())\n","    display(df_ankle.head())\n","    display(df_hip.head())\n","    display(df_wrist.head())\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"ikZjlyBtk2fj","executionInfo":{"status":"ok","timestamp":1684356311637,"user_tz":300,"elapsed":12,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def df_from_one_sub (sub_num): # 1 - 8\n","    \"\"\"reads 3 csv files for a single subject, combines an returns a single dataframe\"\"\"\n","    my_sub_num = sub_num # not sure necessary but easier to follow...\n","    df_ankle = df_from_csv(sub_num = my_sub_num, sensor_loc = 'ankle')\n","    df_hip = df_from_csv(sub_num = my_sub_num, sensor_loc = 'hip')\n","    #wrist is a bit more complicated since the sample rate is different\n","    df_wrist = df_from_csv(sub_num = my_sub_num, sensor_loc = 'wrist')\n","    df_wrist = xforms.to_fixed_ir1_timedelta(df_wrist,new_time_step='10ms')\n","\n","    if ((df_ankle['label'].equals(df_hip['label']))\n","            and (df_ankle['sub'].equals(df_hip['sub']))\n","            and (df_ankle['label'].equals(df_wrist['label']))\n","            and (df_ankle['sub'].equals(df_wrist['sub']))) :\n","        if verbose:\n","            print('confirmed label and sub match - dropping from ankle and hip')\n","        df_ankle.drop(['label','sub'], axis=1, inplace=True)\n","        df_hip.drop(['label','sub'], axis=1, inplace=True)\n","    else:\n","        print('Error:  label and sub do not match, cannot combine dataframes')\n","        print('label match = ',df_ankle['label'].equals(df_hip['label']))\n","        print('sub match = ',df_ankle['sub'].equals(df_hip['sub']))\n","    df_temp = df_ankle.join(df_hip)\n","    df_final = df_temp.join(df_wrist)\n","    del df_temp\n","    df_final = xforms.convert_ir1_labels_to_strings(df = df_final, label_map = label_map_leotta)\n","    return df_final\n","if interactive:\n","    df_temp = df_from_one_sub (sub_num = 8)\n","    print(type(df_temp.index)) # should be DateTimeIndex\n","    print(df_temp.info(verbose=True))\n","    display(df_temp.head()) "],"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def get_leotta_ir1_dict():\n","    \"\"\"reads the Leotta dataset and converts each \"session file\" to an IR1\n","    dataframe.  The goal here is to capture and convert all raw data into\n","    a 2D dataframe of rows = datetime index of each sample, columns = {channels,\n","    label(s), subject_num}.  Additional methods may be used to drop channels,\n","    and convert the string labels to mapped ints prior to switch to ndarrays.\n","    Args:\n","        none \n","    Returns: a dict containing key = df_name and item = IR1 dataframes.\"\"\"\n","    unzip_leotta()\n","    ir1_df_dict = dict() # an empty dictionary\n","    for i in range(1,9):\n","        ir1_name = \"Leotta_Sub\" + str(i)\n","        print('Processing subject number', i, \"into\", ir1_name)\n","        df_temp = df_from_one_sub (sub_num = i)\n","        ir1_df_dict[ir1_name]=df_temp # key is root name in the file\n","    return ir1_df_dict\n","if interactive:\n","    verbose = False\n","    ir1_dict = get_leotta_ir1_dict()\n","    print('IR1 dataframes:',ir1_dict.keys())\n","    for df_name, df in ir1_dict.items():\n","        display(df.head())\n","        break # just want one\n","    verbose = True"],"metadata":{"id":"p_EmZhbavtzi","executionInfo":{"status":"ok","timestamp":1684356311638,"user_tz":300,"elapsed":13,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# Start of original loader code"],"metadata":{"id":"XLDCXLnCUpRn"}},{"cell_type":"code","metadata":{"id":"trfLorthy59i","executionInfo":{"status":"ok","timestamp":1684356311640,"user_tz":300,"elapsed":14,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def leotta_2021_load_dataset(\n","    verbose = True,\n","    incl_xyz_accel = False, # include component accel_x/y/z in ____X data\n","    incl_rms_accel = True, # add rms value (total accel) of accel_x/y/z in ____X data\n","    incl_val_group = False, # split train into train and validate\n","\n","    one_hot_encode = True # make y into multi-column one-hot, one for each activity\n","    ):\n","    \"\"\"calls e4_get_X_y_sub and processes the returned arrays by separating\n","    into _train, _validate, and _test arrays for X and y based on split_sub\n","    dictionary.\"\"\"\n","    orig_zipfile = '/content/drive/My Drive/Datasets/ADL_Leotta_2021.zip'\n","    X, y, sub, xys_info = get_X_y_sub(orig_zipfile=orig_zipfile)\n","    log_info = 'Processing'+str(orig_zipfile)\n","    #remove component accel if needed\n","    if (not incl_xyz_accel):\n","        print(\"Removing component accel\")\n","        X = np.delete(X, [0,1,2,4,5,6,8,9,10], 2)\n","    if (not incl_rms_accel):\n","        print(\"Removing total accel\")\n","        X = np.delete(X, [3,7,11], 2)  \n","    #One-Hot-Encode y...there must be a better way when starting with strings\n","    #https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n","\n","    if (one_hot_encode):\n","        # integer encode\n","        y_vector = np.ravel(y) #encoder won't take column vector\n","        le = LabelEncoder()\n","        integer_encoded = le.fit_transform(y_vector) #convert from string to int\n","        name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n","        print(\"One-hot-encoding: category names -> int -> one-hot\")\n","        print(name_mapping) # seems risky as interim step before one-hot\n","        log_info += \"One Hot:\" + str(name_mapping) +\"\\n\\n\"\n","        onehot_encoder = OneHotEncoder(sparse=False)\n","        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n","        onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n","        print(\"One-hot-encoding\",onehot_encoder.categories_)\n","        y=onehot_encoded\n","        #return X,y\n","    # split by subject number pass in dictionary\n","    sub_num = np.ravel(sub[ : , 0] ) # convert shape to (1047,)\n","    if (not incl_val_group):\n","        train_index = np.nonzero(np.isin(sub_num, split_subj['train_subj'] + \n","                                        split_subj['validation_subj']))\n","        x_train = X[train_index]\n","        y_train = y[train_index]\n","    else:\n","        train_index = np.nonzero(np.isin(sub_num, split_subj['train_subj']))\n","        x_train = X[train_index]\n","        y_train = y[train_index]\n","\n","        validation_index = np.nonzero(np.isin(sub_num, split_subj['validation_subj']))\n","        x_validation = X[validation_index]\n","        y_validation = y[validation_index]\n","\n","    test_index = np.nonzero(np.isin(sub_num, split_subj['test_subj']))\n","    x_test = X[test_index]\n","    y_test = y[test_index]\n","    if (incl_val_group):\n","        return x_train, y_train, x_validation, y_validation, x_test, y_test\n","    else:\n","        return x_train, y_train, x_test, y_test\n","\n","\n","        if(verbose):\n","            headers = (\"Reshaped data\",\"shape\", \"object type\", \"data type\")\n","            mydata = [(\"x_train:\", x_train.shape, type(x_train), x_train.dtype),\n","                    (\"y_train:\", y_train.shape ,type(y_train), y_train.dtype),\n","                    (\"x_test:\", x_test.shape, type(x_test), x_test.dtype),\n","                    (\"y_test:\", y_test.shape ,type(y_test), y_test.dtype)]\n","            print(tabulate(mydata, headers=headers))\n","\n","        return x_train, y_train, x_test, y_test"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"MaT1dfqavvtk","colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"status":"ok","timestamp":1684356342271,"user_tz":300,"elapsed":30645,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}},"outputId":"680ce204-d169-4da2-fffe-b3107df39b6c"},"source":["if __name__ == \"__main__\":\n","    verbose = False\n","    ir1_dict = get_leotta_ir1_dict()\n","    print('IR1 dataframes:',ir1_dict.keys())\n","    for df_name, df in ir1_dict.items():\n","        print(df.head())\n","        break # just want one\n","    verbose = True\n","    # print(\"Downloading and processing Leotta 2021 dataset\")\n","    # x_train, y_train, x_test, y_test = leotta_2021_load_dataset()\n","    # print(\"\\nreturned arrays without validation group:\")\n","    # print(\"x_train shape \",x_train.shape,\" y_train shape \", y_train.shape)\n","    # print(\"x_test shape  \",x_test.shape,\" y_test shape  \",y_test.shape)\n","\n","    # x_train, y_train, x_validation, y_validation, x_test, y_test = leotta_2021_load_dataset(incl_val_group=True)\n","    # print(\"\\nreturned arrays with validation group:\")\n","    # print(\"x_train shape \",x_train.shape,\" y_train shape \", y_train.shape)\n","    # print(\"x_validation shape \",x_validation.shape,\" y_validation shape \", y_validation.shape)\n","    # print(\"x_test shape  \",x_test.shape,\" y_test shape  \",y_test.shape)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Unzipping Leotta 2021 dataset into ./dataset\n","Using source file ./ADL_Leotta_2021.zip\n","Processing subject number 1 into Leotta_Sub1\n","Processing subject number 2 into Leotta_Sub2\n","Processing subject number 3 into Leotta_Sub3\n","Processing subject number 4 into Leotta_Sub4\n","Processing subject number 5 into Leotta_Sub5\n","Processing subject number 6 into Leotta_Sub6\n","Processing subject number 7 into Leotta_Sub7\n","Processing subject number 8 into Leotta_Sub8\n","IR1 dataframes: dict_keys(['Leotta_Sub1', 'Leotta_Sub2', 'Leotta_Sub3', 'Leotta_Sub4', 'Leotta_Sub5', 'Leotta_Sub6', 'Leotta_Sub7', 'Leotta_Sub8'])\n"]},{"output_type":"display_data","data":{"text/plain":["                         ankle_accel_x  ankle_accel_y  ankle_accel_z  \\\n","Timestamp                                                              \n","2020-07-30 11:21:00.000      -0.035156      -1.027832       0.029785   \n","2020-07-30 11:21:00.010      -0.022949      -1.023926       0.033203   \n","2020-07-30 11:21:00.020      -0.010254      -1.028320       0.038574   \n","2020-07-30 11:21:00.030      -0.000977      -1.026855       0.048340   \n","2020-07-30 11:21:00.040       0.003906      -1.031250       0.038574   \n","\n","                         ankle_accel_ttl  hip_accel_x  hip_accel_y  \\\n","Timestamp                                                            \n","2020-07-30 11:21:00.000         0.028864    -0.583984    -0.764160   \n","2020-07-30 11:21:00.010         0.024721    -0.579102    -0.756348   \n","2020-07-30 11:21:00.020         0.029094    -0.578613    -0.758301   \n","2020-07-30 11:21:00.030         0.027993    -0.578125    -0.756836   \n","2020-07-30 11:21:00.040         0.031979    -0.575195    -0.755371   \n","\n","                         hip_accel_z  hip_accel_ttl  wrist_accel_x  \\\n","Timestamp                                                            \n","2020-07-30 11:21:00.000     0.374023       0.031926       0.475777   \n","2020-07-30 11:21:00.010     0.374512       0.023563       0.481189   \n","2020-07-30 11:21:00.020     0.367676       0.022252       0.490935   \n","2020-07-30 11:21:00.030     0.374023       0.023192       0.498793   \n","2020-07-30 11:21:00.040     0.372559       0.019919       0.514537   \n","\n","                         wrist_accel_y  wrist_accel_z  wrist_accel_ttl  label  \\\n","Timestamp                                                                       \n","2020-07-30 11:21:00.000       0.029808       0.934387         0.049075  OTHER   \n","2020-07-30 11:21:00.010       0.013713       0.918378         0.036949  OTHER   \n","2020-07-30 11:21:00.020       0.032158       0.904531         0.029686  OTHER   \n","2020-07-30 11:21:00.030       0.031644       0.909606         0.037908  OTHER   \n","2020-07-30 11:21:00.040       0.030619       0.906663         0.042950  OTHER   \n","\n","                         sub  \n","Timestamp                     \n","2020-07-30 11:21:00.000    1  \n","2020-07-30 11:21:00.010    1  \n","2020-07-30 11:21:00.020    1  \n","2020-07-30 11:21:00.030    1  \n","2020-07-30 11:21:00.040    1  "],"text/html":["\n","  <div id=\"df-db095848-474d-461a-94b2-7467fda660be\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ankle_accel_x</th>\n","      <th>ankle_accel_y</th>\n","      <th>ankle_accel_z</th>\n","      <th>ankle_accel_ttl</th>\n","      <th>hip_accel_x</th>\n","      <th>hip_accel_y</th>\n","      <th>hip_accel_z</th>\n","      <th>hip_accel_ttl</th>\n","      <th>wrist_accel_x</th>\n","      <th>wrist_accel_y</th>\n","      <th>wrist_accel_z</th>\n","      <th>wrist_accel_ttl</th>\n","      <th>label</th>\n","      <th>sub</th>\n","    </tr>\n","    <tr>\n","      <th>Timestamp</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2020-07-30 11:21:00.000</th>\n","      <td>-0.035156</td>\n","      <td>-1.027832</td>\n","      <td>0.029785</td>\n","      <td>0.028864</td>\n","      <td>-0.583984</td>\n","      <td>-0.764160</td>\n","      <td>0.374023</td>\n","      <td>0.031926</td>\n","      <td>0.475777</td>\n","      <td>0.029808</td>\n","      <td>0.934387</td>\n","      <td>0.049075</td>\n","      <td>OTHER</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2020-07-30 11:21:00.010</th>\n","      <td>-0.022949</td>\n","      <td>-1.023926</td>\n","      <td>0.033203</td>\n","      <td>0.024721</td>\n","      <td>-0.579102</td>\n","      <td>-0.756348</td>\n","      <td>0.374512</td>\n","      <td>0.023563</td>\n","      <td>0.481189</td>\n","      <td>0.013713</td>\n","      <td>0.918378</td>\n","      <td>0.036949</td>\n","      <td>OTHER</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2020-07-30 11:21:00.020</th>\n","      <td>-0.010254</td>\n","      <td>-1.028320</td>\n","      <td>0.038574</td>\n","      <td>0.029094</td>\n","      <td>-0.578613</td>\n","      <td>-0.758301</td>\n","      <td>0.367676</td>\n","      <td>0.022252</td>\n","      <td>0.490935</td>\n","      <td>0.032158</td>\n","      <td>0.904531</td>\n","      <td>0.029686</td>\n","      <td>OTHER</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2020-07-30 11:21:00.030</th>\n","      <td>-0.000977</td>\n","      <td>-1.026855</td>\n","      <td>0.048340</td>\n","      <td>0.027993</td>\n","      <td>-0.578125</td>\n","      <td>-0.756836</td>\n","      <td>0.374023</td>\n","      <td>0.023192</td>\n","      <td>0.498793</td>\n","      <td>0.031644</td>\n","      <td>0.909606</td>\n","      <td>0.037908</td>\n","      <td>OTHER</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2020-07-30 11:21:00.040</th>\n","      <td>0.003906</td>\n","      <td>-1.031250</td>\n","      <td>0.038574</td>\n","      <td>0.031979</td>\n","      <td>-0.575195</td>\n","      <td>-0.755371</td>\n","      <td>0.372559</td>\n","      <td>0.019919</td>\n","      <td>0.514537</td>\n","      <td>0.030619</td>\n","      <td>0.906663</td>\n","      <td>0.042950</td>\n","      <td>OTHER</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-db095848-474d-461a-94b2-7467fda660be')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-db095848-474d-461a-94b2-7467fda660be button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-db095848-474d-461a-94b2-7467fda660be');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}]}]}