{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"leotta_2021_load_dataset.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1YgsIveo9CPZT2dqdTMvZTafUd-yVqmhS","authorship_tag":"ABX9TyOm/KbN9fc8bXiq4o6TjLca"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Khc4g511HMYk"},"source":["#leotta_2021_load_dataset.ipynb\n","This data set loader uses the leotta_2021_get_X_y_sub.py file generated by downloading the python version of the same name Jupyter notebook.\n","\n","It will perform a train/test (and optional validation) split and one-hot encode the activity labels.   Returns x/y_train and x/y_test numpy arrays that may be fed directly into a neural net model.\n","\n","Example usage:\n","\n","    x_train, y_train, x_test, y_test = leotta_2021_load_dataset()\n","  \n","\n","Developed and tested using colab.research.google.com\n","IMPORTANT a high RAM runtime is required. Select runtime > change type > shape = high RAM  \n","To save as .py version use File > Download .py\n","\n","Author:  [Lee B. Hinkle](https://userweb.cs.txstate.edu/~lbh31/), [IMICS Lab](https://imics.wp.txstate.edu/), Texas State University, 2021\n","\n","<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.\n","\n","TODOs:\n","* Very early version derived from e4_load_data.\n","* Removing component accel is very order specific - numpy column names?\n"]},{"cell_type":"code","metadata":{"id":"NmKBvlsatEdF","executionInfo":{"status":"ok","timestamp":1628103575984,"user_tz":300,"elapsed":83,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["#mount google drive in colab session\n","#enter path to where the git repo was cloned\n","my_path = '/content/drive/My Drive/Colab Notebooks/imics_lab_repositories/load_data_time_series_dev'"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"q6H67o-YARCx","executionInfo":{"status":"ok","timestamp":1628103564754,"user_tz":300,"elapsed":3789,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["import os\n","import shutil #https://docs.python.org/3/library/shutil.html\n","from shutil import unpack_archive # to unzip\n","import requests #for downloading zip file\n","import numpy as np\n","from tabulate import tabulate # for verbose tables, showing data\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.utils import to_categorical # for one-hot encoding\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import OneHotEncoder"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"IZYH9-5wuINO","executionInfo":{"status":"ok","timestamp":1628103580243,"user_tz":300,"elapsed":1305,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["# use get_x_y_sub to get partially processed numpy arrays\n","full_filename = my_path+os.path.join('/ADL/Leotta_2021/'+'leotta_2021_get_x_y_sub.py')\n","shutil.copy(full_filename,'leotta_2021_get_x_y_sub.py')\n","from leotta_2021_get_x_y_sub import get_X_y_sub"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"trfLorthy59i","executionInfo":{"status":"ok","timestamp":1628105956873,"user_tz":300,"elapsed":79,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["def leotta_2021_load_dataset(\n","    verbose = True,\n","    incl_xyz_accel = False, # include component accel_x/y/z in ____X data\n","    incl_rms_accel = True, # add rms value (total accel) of accel_x/y/z in ____X data\n","    incl_val_group = False, # split train into train and validate\n","    split_subj = dict\n","                (train_subj = [1,2,7,8],\n","                validation_subj = [3,6],\n","                test_subj = [4,5]),\n","    one_hot_encode = True # make y into multi-column one-hot, one for each activity\n","    ):\n","    \"\"\"calls e4_get_X_y_sub and processes the returned arrays by separating\n","    into _train, _validate, and _test arrays for X and y based on split_sub\n","    dictionary.\"\"\"\n","    orig_zipfile = '/content/drive/My Drive/Datasets/ADL_Leotta_2021.zip'\n","    X, y, sub, xys_info = get_X_y_sub(orig_zipfile=orig_zipfile)\n","    log_info = 'Processing'+str(orig_zipfile)\n","    #remove component accel if needed\n","    if (not incl_xyz_accel):\n","        print(\"Removing component accel\")\n","        X = np.delete(X, [0,1,2,4,5,6,8,9,10], 2)\n","    if (not incl_rms_accel):\n","        print(\"Removing total accel\")\n","        X = np.delete(X, [3,7,11], 2)  \n","    #One-Hot-Encode y...there must be a better way when starting with strings\n","    #https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n","\n","    if (one_hot_encode):\n","        # integer encode\n","        y_vector = np.ravel(y) #encoder won't take column vector\n","        le = LabelEncoder()\n","        integer_encoded = le.fit_transform(y_vector) #convert from string to int\n","        name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n","        print(\"One-hot-encoding: category names -> int -> one-hot\")\n","        print(name_mapping) # seems risky as interim step before one-hot\n","        log_info += \"One Hot:\" + str(name_mapping) +\"\\n\\n\"\n","        onehot_encoder = OneHotEncoder(sparse=False)\n","        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n","        onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n","        print(\"One-hot-encoding\",onehot_encoder.categories_)\n","        y=onehot_encoded\n","        #return X,y\n","    # split by subject number pass in dictionary\n","    sub_num = np.ravel(sub[ : , 0] ) # convert shape to (1047,)\n","    if (not incl_val_group):\n","        train_index = np.nonzero(np.isin(sub_num, split_subj['train_subj'] + \n","                                        split_subj['validation_subj']))\n","        x_train = X[train_index]\n","        y_train = y[train_index]\n","    else:\n","        train_index = np.nonzero(np.isin(sub_num, split_subj['train_subj']))\n","        x_train = X[train_index]\n","        y_train = y[train_index]\n","\n","        validation_index = np.nonzero(np.isin(sub_num, split_subj['validation_subj']))\n","        x_validation = X[validation_index]\n","        y_validation = y[validation_index]\n","\n","    test_index = np.nonzero(np.isin(sub_num, split_subj['test_subj']))\n","    x_test = X[test_index]\n","    y_test = y[test_index]\n","    if (incl_val_group):\n","        return x_train, y_train, x_validation, y_validation, x_test, y_test\n","    else:\n","        return x_train, y_train, x_test, y_test\n","\n","\n","        if(verbose):\n","            headers = (\"Reshaped data\",\"shape\", \"object type\", \"data type\")\n","            mydata = [(\"x_train:\", x_train.shape, type(x_train), x_train.dtype),\n","                    (\"y_train:\", y_train.shape ,type(y_train), y_train.dtype),\n","                    (\"x_test:\", x_test.shape, type(x_test), x_test.dtype),\n","                    (\"y_test:\", y_test.shape ,type(y_test), y_test.dtype)]\n","            print(tabulate(mydata, headers=headers))\n","\n","        return x_train, y_train, x_test, y_test"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"MaT1dfqavvtk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628106313308,"user_tz":300,"elapsed":351092,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}},"outputId":"14543b5d-a5ce-4d88-f5c7-44e673f17c89"},"source":["if __name__ == \"__main__\":\n","    print(\"Downloading and processing Leotta 2021 dataset\")\n","    x_train, y_train, x_test, y_test = leotta_2021_load_dataset()\n","    print(\"\\nreturned arrays without validation group:\")\n","    print(\"x_train shape \",x_train.shape,\" y_train shape \", y_train.shape)\n","    print(\"x_test shape  \",x_test.shape,\" y_test shape  \",y_test.shape)\n","\n","    x_train, y_train, x_validation, y_validation, x_test, y_test = leotta_2021_load_dataset(incl_val_group=True)\n","    print(\"\\nreturned arrays with validation group:\")\n","    print(\"x_train shape \",x_train.shape,\" y_train shape \", y_train.shape)\n","    print(\"x_validation shape \",x_validation.shape,\" y_validation shape \", y_validation.shape)\n","    print(\"x_test shape  \",x_test.shape,\" y_test shape  \",y_test.shape)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Downloading and processing Leotta 2021 dataset\n","Unzipping Leotta 2021 dataset\n","Using source file /content/drive/My Drive/Datasets/ADL_Leotta_2021.zip\n","Processing subject number 1\n","Processing:  /content/dataset/ankle/ankle_X_01.csv /content/dataset/ankle/ankle_Y_01.csv\n","Processing:  /content/dataset/hip/hip_X_01.csv /content/dataset/hip/hip_Y_01.csv\n","Processing:  /content/dataset/wrist/wrist_X_01.csv /content/dataset/wrist/wrist_Y_01.csv\n","Resample: Original/New # rows =  780800 305000\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","Processing subject number 2\n","Processing:  /content/dataset/ankle/ankle_X_02.csv /content/dataset/ankle/ankle_Y_02.csv\n","Processing:  /content/dataset/hip/hip_X_02.csv /content/dataset/hip/hip_Y_02.csv\n","Processing:  /content/dataset/wrist/wrist_X_02.csv /content/dataset/wrist/wrist_Y_02.csv\n","Resample: Original/New # rows =  708608 276800\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","Processing subject number 3\n","Processing:  /content/dataset/ankle/ankle_X_03.csv /content/dataset/ankle/ankle_Y_03.csv\n","Processing:  /content/dataset/hip/hip_X_03.csv /content/dataset/hip/hip_Y_03.csv\n","Processing:  /content/dataset/wrist/wrist_X_03.csv /content/dataset/wrist/wrist_Y_03.csv\n","Resample: Original/New # rows =  670720 262000\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","Processing subject number 4\n","Processing:  /content/dataset/ankle/ankle_X_04.csv /content/dataset/ankle/ankle_Y_04.csv\n","Processing:  /content/dataset/hip/hip_X_04.csv /content/dataset/hip/hip_Y_04.csv\n","Processing:  /content/dataset/wrist/wrist_X_04.csv /content/dataset/wrist/wrist_Y_04.csv\n","Resample: Original/New # rows =  734720 287000\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","Processing subject number 5\n","Processing:  /content/dataset/ankle/ankle_X_05.csv /content/dataset/ankle/ankle_Y_05.csv\n","Processing:  /content/dataset/hip/hip_X_05.csv /content/dataset/hip/hip_Y_05.csv\n","Processing:  /content/dataset/wrist/wrist_X_05.csv /content/dataset/wrist/wrist_Y_05.csv\n","Resample: Original/New # rows =  792320 309500\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","Processing subject number 6\n","Processing:  /content/dataset/ankle/ankle_X_06.csv /content/dataset/ankle/ankle_Y_06.csv\n","Processing:  /content/dataset/hip/hip_X_06.csv /content/dataset/hip/hip_Y_06.csv\n","Processing:  /content/dataset/wrist/wrist_X_06.csv /content/dataset/wrist/wrist_Y_06.csv\n","Resample: Original/New # rows =  720640 281500\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","Processing subject number 7\n","Processing:  /content/dataset/ankle/ankle_X_07.csv /content/dataset/ankle/ankle_Y_07.csv\n","Processing:  /content/dataset/hip/hip_X_07.csv /content/dataset/hip/hip_Y_07.csv\n","Processing:  /content/dataset/wrist/wrist_X_07.csv /content/dataset/wrist/wrist_Y_07.csv\n","Resample: Original/New # rows =  755200 295000\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","Processing subject number 8\n","Processing:  /content/dataset/ankle/ankle_X_08.csv /content/dataset/ankle/ankle_Y_08.csv\n","Processing:  /content/dataset/hip/hip_X_08.csv /content/dataset/hip/hip_Y_08.csv\n","Processing:  /content/dataset/wrist/wrist_X_08.csv /content/dataset/wrist/wrist_Y_08.csv\n","Resample: Original/New # rows =  712704 278400\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","Removing component accel\n","One-hot-encoding: category names -> int -> one-hot\n","{1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16}\n","One-hot-encoding [array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])]\n","\n","returned arrays without validation group:\n","x_train shape  (3558, 300, 3)  y_train shape  (3558, 17)\n","x_test shape   (1187, 300, 3)  y_test shape   (1187, 17)\n","Unzipping Leotta 2021 dataset\n","Using source file /content/drive/My Drive/Datasets/ADL_Leotta_2021.zip\n","Processing subject number 1\n","Processing:  /content/dataset/ankle/ankle_X_01.csv /content/dataset/ankle/ankle_Y_01.csv\n","Processing:  /content/dataset/hip/hip_X_01.csv /content/dataset/hip/hip_Y_01.csv\n","Processing:  /content/dataset/wrist/wrist_X_01.csv /content/dataset/wrist/wrist_Y_01.csv\n","Resample: Original/New # rows =  780800 305000\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","Processing subject number 2\n","Processing:  /content/dataset/ankle/ankle_X_02.csv /content/dataset/ankle/ankle_Y_02.csv\n","Processing:  /content/dataset/hip/hip_X_02.csv /content/dataset/hip/hip_Y_02.csv\n","Processing:  /content/dataset/wrist/wrist_X_02.csv /content/dataset/wrist/wrist_Y_02.csv\n","Resample: Original/New # rows =  708608 276800\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","Processing subject number 3\n","Processing:  /content/dataset/ankle/ankle_X_03.csv /content/dataset/ankle/ankle_Y_03.csv\n","Processing:  /content/dataset/hip/hip_X_03.csv /content/dataset/hip/hip_Y_03.csv\n","Processing:  /content/dataset/wrist/wrist_X_03.csv /content/dataset/wrist/wrist_Y_03.csv\n","Resample: Original/New # rows =  670720 262000\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","Processing subject number 4\n","Processing:  /content/dataset/ankle/ankle_X_04.csv /content/dataset/ankle/ankle_Y_04.csv\n","Processing:  /content/dataset/hip/hip_X_04.csv /content/dataset/hip/hip_Y_04.csv\n","Processing:  /content/dataset/wrist/wrist_X_04.csv /content/dataset/wrist/wrist_Y_04.csv\n","Resample: Original/New # rows =  734720 287000\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","Processing subject number 5\n","Processing:  /content/dataset/ankle/ankle_X_05.csv /content/dataset/ankle/ankle_Y_05.csv\n","Processing:  /content/dataset/hip/hip_X_05.csv /content/dataset/hip/hip_Y_05.csv\n","Processing:  /content/dataset/wrist/wrist_X_05.csv /content/dataset/wrist/wrist_Y_05.csv\n","Resample: Original/New # rows =  792320 309500\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","Processing subject number 6\n","Processing:  /content/dataset/ankle/ankle_X_06.csv /content/dataset/ankle/ankle_Y_06.csv\n","Processing:  /content/dataset/hip/hip_X_06.csv /content/dataset/hip/hip_Y_06.csv\n","Processing:  /content/dataset/wrist/wrist_X_06.csv /content/dataset/wrist/wrist_Y_06.csv\n","Resample: Original/New # rows =  720640 281500\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","Processing subject number 7\n","Processing:  /content/dataset/ankle/ankle_X_07.csv /content/dataset/ankle/ankle_Y_07.csv\n","Processing:  /content/dataset/hip/hip_X_07.csv /content/dataset/hip/hip_Y_07.csv\n","Processing:  /content/dataset/wrist/wrist_X_07.csv /content/dataset/wrist/wrist_Y_07.csv\n","Resample: Original/New # rows =  755200 295000\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","Processing subject number 8\n","Processing:  /content/dataset/ankle/ankle_X_08.csv /content/dataset/ankle/ankle_Y_08.csv\n","Processing:  /content/dataset/hip/hip_X_08.csv /content/dataset/hip/hip_Y_08.csv\n","Processing:  /content/dataset/wrist/wrist_X_08.csv /content/dataset/wrist/wrist_Y_08.csv\n","Resample: Original/New # rows =  712704 278400\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","Removing component accel\n","One-hot-encoding: category names -> int -> one-hot\n","{1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16}\n","One-hot-encoding [array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])]\n","\n","returned arrays with validation group:\n","x_train shape  (2391, 300, 3)  y_train shape  (2391, 17)\n","x_validation shape  (1167, 300, 3)  y_validation shape  (1167, 17)\n","x_test shape   (1187, 300, 3)  y_test shape   (1187, 17)\n"],"name":"stdout"}]}]}