{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Leotta_2021_get_X_y_sub.ipynb","provenance":[{"file_id":"1RkiXI3GhB-rNtyUp_VYw05xiiuD_oDFA","timestamp":1612028534003}],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1S59WDs0nbTeuTGpms2v1rBUM-LYJp8FG","authorship_tag":"ABX9TyMfoSSncQskhlpX8ou0or7J"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Khc4g511HMYk"},"source":["#Leotta_2021_get_X_y_sub.ipynb\n","Loads the dataset from a local zip file and converts the data into numpy arrays of X (data), y(labels), and sub (subject numbers)\n",">X = (samples, time steps per sample, accel_x/y/z/total_accel)  \n",">y = (samples, {0,1,...17}) #activity classification  \n",">s = subject number  \n","\n","This is an intermediate representation that can be used to build the train/validate/test arrays.\n","\n","Some functions are defined, but this is mostly meant to be run in interactive\n","mode with the files saved at the end.\n","\n","The dataset citation and link to the paper and download are available on this site https://sepl.dibris.unige.it/2020-DailyActivityDataset.php\n","\n","<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.\n","\n","[Lee B. Hinkle](https://userweb.cs.txstate.edu/~lbh31/), Texas State University, [IMICS Lab](https://imics.wp.txstate.edu/)  \n","TODO:\n","* This is still work-in-progress\n","* Figure out how to replace int labels with strings - seems like it should be easy, but apparently not.\n","* Reshape from big df to numpy arrays crashes unless run on colab high RAM runtime.   Maybe reduce y and sub types to int8?\n","* Make timesteps and stepsize passed parameters from get_X_y_sub\n"]},{"cell_type":"code","metadata":{"id":"q6H67o-YARCx","executionInfo":{"status":"ok","timestamp":1628102205550,"user_tz":300,"elapsed":2129,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["import os\n","import shutil #https://docs.python.org/3/library/shutil.html\n","from shutil import unpack_archive # to unzip\n","#from shutil import make_archive # to create zip for storage\n","import requests #for downloading zip file\n","from scipy import io #for loadmat, matlab conversion\n","import time\n","import pandas as pd\n","import numpy as np\n","from numpy import savetxt\n","import matplotlib.pyplot as plt # for plotting - pandas uses matplotlib\n","from tabulate import tabulate # for verbose tables\n","from tensorflow.keras.utils import to_categorical # for one-hot encoding\n","import gc #trying to resolve crash on reshape method"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"_arxQU-n6nKK","executionInfo":{"status":"ok","timestamp":1628095810960,"user_tz":300,"elapsed":91,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["interactive = False # enables functions for exploring data and dataframes"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"awuF4ENKphtA","executionInfo":{"status":"ok","timestamp":1628102217497,"user_tz":300,"elapsed":9,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["#Helper functions especially useful in colab\n","from requests import get\n","def what_is_my_name():\n","    \"\"\"returns the name of the running colab ipynb file\"\"\"\n","    #code is readily available on web - not original\n","    my_name = get('http://172.28.0.2:9000/api/sessions').json()[0]['name']\n","    return my_name\n","#credit https://stackoverflow.com/users/4944093/george-petrov for name method\n","def namestr(obj, namespace):\n","    return [name for name in namespace if namespace[name] is obj]\n","def get_shapes(np_arr_list):\n","    \"\"\"Returns text, each line is shape and dtype for numpy array in list\n","       example: print(get_shapes([X_train, X_test, y_train, y_test]))\n","       WARNING: Gets 'list index out of range' if called within method.\"\"\"\n","    return # do nothing until out of range issue fixed\n","    #probably related to this https://stackoverflow.com/questions/592746/how-can-you-print-a-variable-name-in-python\n","    #bonus for LOL comments\n","    shapes = \"\"\n","    print(np_arr_list)\n","    for i in np_arr_list:\n","        print(' i = ', i)\n","        my_name = namestr(i,globals())\n","        print ('my_name = ',my_name)\n","        shapes += (my_name[0] + \" shape is \" + str(i.shape) \\\n","            + \" data type is \" + str(i.dtype) + \"\\n\")\n","    return shapes"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"9oab3XMPgL8Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628102230184,"user_tz":300,"elapsed":9673,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}},"outputId":"821d6d7d-9e25-4970-9464-2ef2e3c515f6"},"source":["def unzip_leotta(\n","    orig_zipfile, #full file name of original dataset zipfile\n","    working_dir = '/content/dataset' # location of unzipped files in colab\n","    ):\n","    \"\"\"check for local copy, if none unzips the dataset structure in working_dir\"\"\"\n","    if (os.path.isdir(working_dir)):\n","        print(\"Using existing archive in colab\")\n","        return\n","    else:\n","        print(\"Unzipping Leotta 2021 dataset\")\n","        if (os.path.exists(orig_zipfile)):\n","            print(\"Using source file\", orig_zipfile)\n","            shutil.unpack_archive(orig_zipfile,'/content/dataset','zip')\n","        else:\n","            print(\"Error: \", orig_zipfile, \" not found, exiting\")\n","            return\n","if interactive:\n","    unzip_leotta(orig_zipfile = '/content/drive/My Drive/Datasets/ADL_Leotta_2021.zip')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Unzipping Leotta 2021 dataset\n","Using source file /content/drive/My Drive/Datasets/ADL_Leotta_2021.zip\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ttKS9Ox6JSed","executionInfo":{"status":"ok","timestamp":1628095815942,"user_tz":300,"elapsed":85,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["def df_from_csv (\n","    sub_num, # 1 - 8\n","    sensor_loc, # ankle, hip, wrist\n","    working_dir = '/content/dataset'): # location of unzipped files in colab \n","    \"\"\"reads csv, returns df with accel x/y/z/ttl, label, sub_num\"\"\"\n","    fnameX = sensor_loc + '_X_0' + str(sub_num) +  '.csv'\n","    fnamey = sensor_loc + '_Y_0' + str(sub_num) +  '.csv'\n","    ffnameX = os.path.join(working_dir, sensor_loc, fnameX)\n","    ffnamey = os.path.join(working_dir, sensor_loc, fnamey)\n","    print ('Processing: ', ffnameX, ffnamey)\n","    df = pd.read_csv(ffnameX)\n","    if (sensor_loc == 'wrist'): # Centrepoint device has different header name\n","        df.rename(columns={'Timestamp UTC': 'Timestamp'}, inplace=True)\n","    # the imported Timestamp is an object - need to convert to DateTime\n","    # in order to set the index to DateTime format.  Enables resampling etc.\n","    # Leaving these here - helpful to debug if leveraging this code!\n","        #print(\"*** Start ***\")\n","        #print(type(df.index))\n","        #print(df.info(verbose=True))  \n","    df['Timestamp'] = pd.to_datetime(df['Timestamp']) \n","    df.set_index('Timestamp', drop = True, inplace = True)\n","    if (sensor_loc != 'wrist'): # Centrepoint doesn't have non-accel columnns\n","        df = df.drop(['Temperature','Gyroscope X','Gyroscope Y','Gyroscope Z',\n","                      'Magnetometer X','Magnetometer Y','Magnetometer Z'], axis=1)\n","    df_sqd = df.pow(2)[['Accelerometer X','Accelerometer Y','Accelerometer Z']] #square each accel\n","    df_sum = df_sqd.sum(axis=1) #add sum of squares, new 1 col df\n","    df.loc[:,'accel_ttl'] = df_sum.pow(0.5)-1  # sqrt and remove 1g due to gravity\n","    del df_sqd, df_sum\n","    df.columns = [sensor_loc + '_accel_x', sensor_loc + '_accel_y', sensor_loc + '_accel_z', sensor_loc + '_accel_ttl']\n","    # add activity numbers - number of rows are the same in this dataset\n","    # Why doesn't this work? df['label'] = pd.read_csv(ffnamey, dtype='Int64')\n","    dfy = pd.read_csv(ffnamey)\n","    df['label']=dfy['label'].to_numpy() # this works, above doesn't?\n","    df['label'] = df['label'].astype(int) # change from float to int\n","    del dfy\n","    # add column with subject number\n","    df['sub'] = sub_num\n","    return df"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"ikZjlyBtk2fj","executionInfo":{"status":"ok","timestamp":1628095817526,"user_tz":300,"elapsed":91,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["def df_from_one_sub (sub_num): # 1 - 8\n","    \"\"\"reads 3 csv files for a single subject, combines an returns a single dataframe\"\"\"\n","    my_sub_num = sub_num # not sure necessary but easier to follow...\n","    df_ankle = df_from_csv(sub_num = my_sub_num, sensor_loc = 'ankle')\n","    df_hip = df_from_csv(sub_num = my_sub_num, sensor_loc = 'hip')\n","    #wrist is a bit more complicated since the sample rate is different\n","    df_wrist = df_from_csv(sub_num = my_sub_num, sensor_loc = 'wrist')\n","    df_wrist = to_fixed_timedelta(df_wrist,new_time_step='10ms')\n","\n","    if ((df_ankle['label'].equals(df_hip['label']))\n","            and (df_ankle['sub'].equals(df_hip['sub']))\n","            and (df_ankle['label'].equals(df_wrist['label']))\n","            and (df_ankle['sub'].equals(df_wrist['sub']))) :\n","        print('confirmed label and sub match - dropping from ankle and hip')\n","        df_ankle.drop(['label','sub'], axis=1, inplace=True)\n","        df_hip.drop(['label','sub'], axis=1, inplace=True)\n","    else:\n","        print('Error:  label and sub do not match, cannot combine dataframes')\n","        print('label match = ',df_ankle['label'].equals(df_hip['label']))\n","        print('sub match = ',df_ankle['sub'].equals(df_hip['sub']))\n","    df_temp = df_ankle.join(df_hip)\n","    df_final = df_temp.join(df_wrist)\n","    del df_temp\n","    return df_final"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"tCxN2oYhzFf2","executionInfo":{"status":"ok","timestamp":1628095818322,"user_tz":300,"elapsed":122,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["def to_fixed_timedelta(df_in, new_time_step='50ms'):\n","    \"\"\"resamples DateTime indexed dataframe to new_time_step.  Will\n","    return NaN per resample method (happens on irregular samples)\"\"\"\n","    #print(\"Resampling at \",new_time_step,\": Original # rows = \",len(df_in.index))\n","    orig_rows = len(df_in.index)\n","    df_out = df_in.resample(new_time_step).mean()\n","    df_out = df_out.interpolate() #linear interpolation for nan\n","    print(\"Resample: Original/New # rows = \",orig_rows,len(df_out.index))\n","    return df_out\n","# method copied from MobiAct_ADL_get_X_y_sub.ipynb, it is named this way\n","# because it can also be used to \"correct\" sample jitter in smartphone data"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"hR1l_YcQQ04p","executionInfo":{"status":"ok","timestamp":1628095819752,"user_tz":300,"elapsed":96,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["# exploratory code to try and figure out why plots seem off\n","# confirmed units in g's https://actigraphcorp.com/support/downloads/#Manuals\n","if interactive:\n","    snum = 1\n","    df_ankle = df_from_csv(sub_num = snum, sensor_loc = 'ankle')\n","    display(df_ankle.head())\n","    df_hip = df_from_csv(sub_num = snum, sensor_loc = 'hip')\n","    display(df_hip.head())\n","    df_wrist = df_from_csv(sub_num = snum, sensor_loc = 'wrist')\n","    display(df_wrist.head())\n","    print('ankle')\n","    print(np.amin(df_ankle, axis=0))\n","    print(np.amax(df_ankle, axis=0))\n","    print('hip')\n","    print(np.amin(df_hip, axis=0))\n","    print(np.amax(df_hip, axis=0))\n","    print('wrist')\n","    print(np.amin(df_wrist, axis=0))\n","    print(np.amax(df_wrist, axis=0))"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"XHwZwufHJtYm","executionInfo":{"status":"ok","timestamp":1628095820797,"user_tz":300,"elapsed":95,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["if interactive:\n","    df_temp = df_from_one_sub (sub_num = 8)\n","    print(type(df_temp.index)) # should be DateTimeIndex\n","    print(df_temp.info(verbose=True))\n","    display(df_temp.head())"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"fgEprJlATzbY","executionInfo":{"status":"ok","timestamp":1628095821606,"user_tz":300,"elapsed":94,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["if interactive:\n","    #df_temp = my_df[my_df['label'] != 0] # drop 'other' labeled activities\n","    fig, axes = plt.subplots(nrows=1,ncols=2,figsize=(12,6))\n","    df_temp['label'].plot(ax = axes[0], subplots=True) \n","    df_temp['sub'].plot(ax = axes[1], subplots=True)\n","    plt.show()"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"gg12hg9N2F4G","executionInfo":{"status":"ok","timestamp":1628095822393,"user_tz":300,"elapsed":90,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["if interactive:\n","    print('Act. # instances (rows)')\n","    print(df_temp['label'].value_counts()) # shows the number of each activity"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"BOmbwLUtWYWk","executionInfo":{"status":"ok","timestamp":1628095823260,"user_tz":300,"elapsed":88,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["if interactive:\n","    # shows stacked acceleration diagrams for given label\n","    df_temp[df_temp['label'] == 1].plot(figsize=(12, 8),subplots = True)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l81EGmz6n5Hm"},"source":["# Dataframe pieces in place, next method takes dataframe and parses to numpy arrays\n","In order to limit memory requirements it is done by subject rather than creating one big dataframe as in other datasets"]},{"cell_type":"code","metadata":{"id":"z2cJI4qntyqE","executionInfo":{"status":"ok","timestamp":1628095826511,"user_tz":300,"elapsed":107,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["def split_df_to_timeslice_nparrays(df, features, time_steps, step):\n","    \"\"\"slice the df into segments of time_steps length and return X, y, sub\n","    ndarrays.  if step = time_steps there is no overlap. Updated from original\n","    in e4_get_X_y_sub to accept list of features.\"\"\"\n","    N_FEATURES = len(features)\n","    segments = []\n","    labels = []\n","    subject = []\n","    for i in range(0, len(df) - time_steps, step):\n","        #df_segX = df[['accel_x', 'accel_y', 'accel_z','accel_ttl']].iloc[i: i + time_steps]\n","        df_segX = df[features].iloc[i: i + time_steps]\n","        df_lbl = df['label'].iloc[i: i + time_steps]\n","        df_sub = df['sub'].iloc[i: i + time_steps]\n","        # Save only if labels are the same for the entire segment and valid\n","        if (df_lbl.value_counts().iloc[0] != time_steps):\n","            #print('Segment starting at',i,'contains multiple labels.  Discarding.')\n","            continue\n","\n","        if 0 in df_lbl.values :\n","            #print('Segment starting at',i,'contains Undefined labels.  Discarding')\n","            continue\n","        # Save only if sub is the same for the entire segment and valid\n","        if (df_sub.value_counts().iloc[0] != time_steps):\n","            #print('Segment starting at',i,'contains multiple subjects.  Discarding.')\n","            continue\n","        segments.append(df_segX.to_numpy())\n","        labels.append(df['label'].iloc[i])\n","        subject.append(df['sub'].iloc[i])\n","        #this still requires high memory instance on colab.\n","        #del [[df_segX,df_lbl,df_sub]]\n","        del df_segX\n","        del df_lbl\n","        del df_sub\n","        gc.collect\n","\n","    # Bring the segments into a better shape, convert to nparrays\n","    reshaped_segments = np.asarray(segments, dtype= np.float32).reshape(-1, time_steps, N_FEATURES)\n","    labels = np.asarray(labels)\n","    subject = np.asarray(subject)\n","    # both labels and sub are row arrays, change to single column arrays\n","    labels = labels[np.newaxis].T\n","    subject = subject[np.newaxis].T\n","    # check for nan - issue with resampled data\n","    bad_data_locations = np.argwhere(np.isnan(reshaped_segments))\n","    np.unique(bad_data_locations[:,0]) #[:,0] accesses just 1st column\n","    if (bad_data_locations.size==0):\n","        print(\"No NaN entries found\")\n","    else:\n","        print(\"Warning: Output arrays contain NaN entries\")\n","        print(\"execute print(X[99]) # to view single sample\")\n","    return reshaped_segments, labels, subject"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"RJtnN1Ss84JJ","executionInfo":{"status":"ok","timestamp":1628095829210,"user_tz":300,"elapsed":91,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["def get_X_y_sub(\n","    # you probably need to change this path to your google drive mount\n","    orig_zipfile = '/content/drive/My Drive/Datasets/ADL_Leotta_2021.zip',\n","    working_dir='/content/temp', # this directory will be created inside colab\n","    time_steps = 0, #TODO - the timesteps do not propagate, set to 300 & 300\n","    step = 0 #if equal to time_steps there will be no overlap of sliding window\n","    ):\n","    \"\"\"processes dataset zip file to extract csv file and convert into X (data),\n","     y (labels), and sub (subject number) ndarrays.\n","     Returns X, y, sub, xys_info (a text file)\n","    \"\"\"\n","    unzip_leotta(orig_zipfile = orig_zipfile, working_dir = working_dir)\n","    xys_info = 'not setup for Leotta dataset'\n","    for i in range(1,9):\n","        print('Processing subject number', i)\n","        df_temp = df_from_one_sub (sub_num = i)\n","        feature_list = list(df_temp.columns)\n","        feature_list.remove('label')\n","        feature_list.remove('sub')\n","        print(\"Using\",len(feature_list),'features',feature_list)\n","        my_X, my_y, my_sub = split_df_to_timeslice_nparrays(df_temp, feature_list, 300, 300)\n","        if i==1:\n","            X = my_X\n","            y = my_y\n","            sub = my_sub\n","        else:\n","            X = np.vstack([X, my_X])\n","            y = np.vstack([y, my_y])\n","            sub = np.vstack([sub, my_sub])\n","        print(get_shapes([X, y, sub]))\n","    return X, y, sub, xys_info"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xmxE5mpcna5B","executionInfo":{"status":"ok","timestamp":1628095953317,"user_tz":300,"elapsed":122494,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}},"outputId":"60fc613f-5251-40fa-e90f-0d2d1f8ef8cf"},"source":["if __name__ == \"__main__\":\n","    print(\"Processing dataset zip files and label csv into X, y, sub ndarrays\")\n","    X, y, sub, xys_info = get_X_y_sub()\n","    print(\"X shape \",X.shape,\"dtype = \",X.dtype)\n","    print(\"y shape \",y.shape,\"dtype = \",y.dtype)\n","    print(\"sub shape \",sub.shape,\"dtype = \",sub.dtype)\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Processing dataset zip files and label csv into X, y, sub ndarrays\n","Unzipping Leotta 2021 dataset\n","Using source file /content/drive/My Drive/Datasets/ADL_Leotta_2021.zip\n","Processing subject number 1\n","Processing:  /content/dataset/ankle/ankle_X_01.csv /content/dataset/ankle/ankle_Y_01.csv\n","Processing:  /content/dataset/hip/hip_X_01.csv /content/dataset/hip/hip_Y_01.csv\n","Processing:  /content/dataset/wrist/wrist_X_01.csv /content/dataset/wrist/wrist_Y_01.csv\n","Resample: Original/New # rows =  780800 305000\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","Processing subject number 2\n","Processing:  /content/dataset/ankle/ankle_X_02.csv /content/dataset/ankle/ankle_Y_02.csv\n","Processing:  /content/dataset/hip/hip_X_02.csv /content/dataset/hip/hip_Y_02.csv\n","Processing:  /content/dataset/wrist/wrist_X_02.csv /content/dataset/wrist/wrist_Y_02.csv\n","Resample: Original/New # rows =  708608 276800\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","Processing subject number 3\n","Processing:  /content/dataset/ankle/ankle_X_03.csv /content/dataset/ankle/ankle_Y_03.csv\n","Processing:  /content/dataset/hip/hip_X_03.csv /content/dataset/hip/hip_Y_03.csv\n","Processing:  /content/dataset/wrist/wrist_X_03.csv /content/dataset/wrist/wrist_Y_03.csv\n","Resample: Original/New # rows =  670720 262000\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","Processing subject number 4\n","Processing:  /content/dataset/ankle/ankle_X_04.csv /content/dataset/ankle/ankle_Y_04.csv\n","Processing:  /content/dataset/hip/hip_X_04.csv /content/dataset/hip/hip_Y_04.csv\n","Processing:  /content/dataset/wrist/wrist_X_04.csv /content/dataset/wrist/wrist_Y_04.csv\n","Resample: Original/New # rows =  734720 287000\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","Processing subject number 5\n","Processing:  /content/dataset/ankle/ankle_X_05.csv /content/dataset/ankle/ankle_Y_05.csv\n","Processing:  /content/dataset/hip/hip_X_05.csv /content/dataset/hip/hip_Y_05.csv\n","Processing:  /content/dataset/wrist/wrist_X_05.csv /content/dataset/wrist/wrist_Y_05.csv\n","Resample: Original/New # rows =  792320 309500\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","Processing subject number 6\n","Processing:  /content/dataset/ankle/ankle_X_06.csv /content/dataset/ankle/ankle_Y_06.csv\n","Processing:  /content/dataset/hip/hip_X_06.csv /content/dataset/hip/hip_Y_06.csv\n","Processing:  /content/dataset/wrist/wrist_X_06.csv /content/dataset/wrist/wrist_Y_06.csv\n","Resample: Original/New # rows =  720640 281500\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","Processing subject number 7\n","Processing:  /content/dataset/ankle/ankle_X_07.csv /content/dataset/ankle/ankle_Y_07.csv\n","Processing:  /content/dataset/hip/hip_X_07.csv /content/dataset/hip/hip_Y_07.csv\n","Processing:  /content/dataset/wrist/wrist_X_07.csv /content/dataset/wrist/wrist_Y_07.csv\n","Resample: Original/New # rows =  755200 295000\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","Processing subject number 8\n","Processing:  /content/dataset/ankle/ankle_X_08.csv /content/dataset/ankle/ankle_Y_08.csv\n","Processing:  /content/dataset/hip/hip_X_08.csv /content/dataset/hip/hip_Y_08.csv\n","Processing:  /content/dataset/wrist/wrist_X_08.csv /content/dataset/wrist/wrist_Y_08.csv\n","Resample: Original/New # rows =  712704 278400\n","confirmed label and sub match - dropping from ankle and hip\n","Using 12 features ['ankle_accel_x', 'ankle_accel_y', 'ankle_accel_z', 'ankle_accel_ttl', 'hip_accel_x', 'hip_accel_y', 'hip_accel_z', 'hip_accel_ttl', 'wrist_accel_x', 'wrist_accel_y', 'wrist_accel_z', 'wrist_accel_ttl']\n","No NaN entries found\n","None\n","X shape  (4745, 300, 12) dtype =  float32\n","y shape  (4745, 1) dtype =  int64\n","sub shape  (4745, 1) dtype =  int64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zOEZbC1HBbZr"},"source":["# Save files to drive"]},{"cell_type":"code","metadata":{"id":"b2S4BThhXuqw","executionInfo":{"status":"ok","timestamp":1628096127411,"user_tz":300,"elapsed":125,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["if False: #change to True to save files\n","    xys_info = 'Early output, needs logging updates'\n","    output_dir = '/content/drive/MyDrive/Processed_Datasets/leotta/original'\n","    if (os.path.isdir(output_dir)):\n","        #quick check for existing files, '.ipynb_checkpoints' file \n","        #makes it more complicated to see if directory is empty\n","        if (not os.path.isfile(output_dir + '/X.npy')):\n","            summary = \"Leotta hand/wrist/ankle data\\n\"\n","            summary += \"Saved to \" + output_dir + \"\\n\"\n","            summary += \"Generated by \" + what_is_my_name() \n","            summary += \" on \" + time.strftime('%b-%d-%Y_%H%M', time.localtime())\n","\n","            info_fname = output_dir +'/'+'info.txt'\n","            full_info = summary + \"\\n\" + xys_info + \"\\n\"\n","            print(full_info)\n","\n","            with open(info_fname, \"w\") as file_object:\n","                file_object.write(full_info)\n","\n","            if True:\n","                np.save(output_dir + '/'+'X.npy',X)\n","                np.save(output_dir + '/'+'y.npy',y)\n","                np.save(output_dir + '/'+'sub.npy',sub)\n","        else:\n","            print(\"Error \"+output_dir+\" contains X.npy, please delete files\")\n","    else:\n","        print(output_dir + \" not found, please create directory\") "],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"DDcRhBO1Af0y","executionInfo":{"status":"ok","timestamp":1628096128935,"user_tz":300,"elapsed":107,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["if interactive:\n","    # This labeling does not work - also not 100% sure strings are better.\n","    ALPHA_LABEL = ['OTHER','RELAX','KEYBOARD_WRITING','LAPTOP','HANDWRITING',\n","                'HANDWASHING','FACEWASHING','TEETHBRUSH','SWEEPING','VACUUMING',\n","                'EATING','DUSTING','RUBBING','DOWNSTAIRS','WALKING',\n","                'WALKING_FAST','UPSTAIRS_FAST','UPSTAIRS'] # from README.txt\n","    for i in range(18):\n","        print(i,ALPHA_LABEL[i])\n","    print(ALPHA_LABEL[2])\n","    print (df_temp.loc[df_temp.index[4000],'label'])\n","    print (ALPHA_LABEL[df_temp.loc[df_temp.index[4000],'label']])\n","    #arrggghhh\n","    #df_temp['alpha_label'] = df_temp.apply(lambda row: ALPHA_LABEL[df_temp.loc[df_temp.index[row],'label']], axis=1)\n","    #df['add'] = df.apply(lambda row : add(row['A'],row['B'], row['C']), axis = 1)\n","    #df_temp['alpha_label'] = df_temp.apply(lambda row : ALPHA_LABEL[row['index']], axis = 1)\n","    #df_temp['alpha_label'] = ALPHA_LABEL[df_temp['label']]\n","    #df_temp.head()"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mWP5BDYRhZd4"},"source":["#More exploratory code for ndarrays - should probably be own notebook"]},{"cell_type":"code","metadata":{"id":"1KlCiUI6NVzU","executionInfo":{"status":"ok","timestamp":1628096132163,"user_tz":300,"elapsed":97,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["if interactive:\n","    #show number of samples per subject\n","    unique_elements, counts_elements = np.unique(sub, return_counts=True)\n","    print(\" subject #\",int(unique_elements[np.argmin(counts_elements)]),\n","        \"has \",np.amin(counts_elements),\" samples (min)\\n\",\n","        \"subject #\",int(unique_elements[np.argmax(counts_elements)]),\n","        \"has \",np.amax(counts_elements),\" samples (max)\\n\")\n","    print(\"Sample count per subject:\")\n","    print(np.asarray((unique_elements, counts_elements)))"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"6zSP9lPX_esj","executionInfo":{"status":"ok","timestamp":1628096133726,"user_tz":300,"elapsed":141,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["def plot_subjects():\n","    uniques, s_num = np.unique(sub, return_inverse=True)\n","    print (uniques)\n","    plt.plot(s_num) \n","    plt.show()\n","if interactive:    \n","    plot_subjects()"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"TbwO1MGAaHRP","executionInfo":{"status":"ok","timestamp":1628096135732,"user_tz":300,"elapsed":107,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["if interactive:\n","    #Find min and max values for consistent plot scales\n","    min_g = np.nanmin(X[::1])\n","    max_g = np.nanmax(X[::1])\n","    print ('min g value is',min_g,'max g value is',max_g)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"OcmjJYNYXJtd","executionInfo":{"status":"ok","timestamp":1628096136951,"user_tz":300,"elapsed":106,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["if interactive:\n","    #Plot a sample\n","    #sample_num = 100 # activity is 3 using laptop - values are very small\n","    sample_num = 500 # activity is 14 walking fast - more typical plot\n","    plt.figure(figsize=(20,5))\n","    plt.ylim([min_g/2, max_g/2])\n","    plt.plot(X[sample_num])\n","    plt.title('sample '+str(sample_num)+' subject '+str(int(sub[sample_num,0]))+' activity '+str(y[sample_num]))\n","    plt.xlabel(\"time step\")\n","    plt.ylabel(\"accel\")\n","    plt.show()\n"],"execution_count":21,"outputs":[]}]}