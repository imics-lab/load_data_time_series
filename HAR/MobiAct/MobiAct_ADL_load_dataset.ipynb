{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MobiAct_ADL_load_dataset.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"19K7MOgfzcpHqrDtY8kqkC4w2gRUNm0mb","authorship_tag":"ABX9TyPgsh+yJfdTtNbRSt9y3ecU"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Khc4g511HMYk"},"source":["#MobiAct_ADL_load_dataset.ipynb. \n","Loads the 6 Activities of Daily living (ADL) from the first release of the MobiAct dataset and converts to numpy arrays while adhering to the general format of the [Keras MNIST load_data function](https://keras.io/api/datasets/mnist/#load_data-function).\n","\n","STD Standing (1x5min)\n","WAL Walking (1x5min)\n","JOG Jogging (3x30sec)\n","JUM Jumping (3x30sec)\n","STU Stairs Up (6x10sec)\n","STN Stairs Down (6x10sec)\n","\n","Only the timestamp 'nanoseconds' and accelerometer data (accel_x/y/z) is imported. Sitting, Fall, gyro, and orientation data is not used.\n","\n","Data is segmented into 500 samples (~5s) using as much of the sample as possible (incomplete segments are discarded)\n","\n","Returns: Tuple of Numpy arrays:   \n","(x_train, y_train),(x_validation, y_validation)\\[optional\\],(x_test, y_test) \n","\n","* x_train\\/validation\\/test: containing float64 with shapes (num_samples, 500, {3,4,1})\n","* y_train\\/validation\\/test: containing int8 with shapes (num_samples 0-9)\n","\n","Default train/validation/test split is by subject with a best effort to keep gender mix and distributed by height among the three categories.\n","Split is 60%/20%/20%\n","\n","Example usage:  \n","x_train, y_train, x_test, y_test = mobiact_adl_load_dataset()\n","\n","Additional References:   \n","For more information on the dataset, please refer to the publication:    Vavoulas, G., Chatzaki, C., Malliotakis, T., Pediaditis, M. and Tsiknakis, M.,(2016) The MobiAct Dataset: Recognition of Activities of Daily Living using Smartphones.,In Proceedings of the International Conference on Information and Communication Technologies for Ageing Well and e-Health  â€“ Volume 1: ICT4AWE, (ICT4AGEINGWELL 2016) ISBN 978-989-758-180-9, pages 143-151, Rome, Italy. DOI: 10.5220/0005792401430151\n","\n","[tensorflow git repo with more datasets](https://github.com/tensorflow/datasets) which also links to another colab [Keras MNIST Example](https://colab.research.google.com/github/tensorflow/datasets/blob/master/docs/keras_example.ipynb) with more options.\n","\n","Developed and tested using colab.research.google.com  \n","To save as .py version use File > Download .py\n","\n","Author:  Lee B. Hinkle, IMICS Lab, Texas State University, 2021\n","\n","<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.\n","\n","TODOs:\n","* Figure out how to download source directly from https://drive.google.com/file/d/0B5VcW5yHhWhibWxGRTZDd0dGY2s/edit. this example seems newer than version used for UniMiB https://keras.io/examples/timeseries/timeseries_weather_forecasting/\n","* Implement one-hot encoding using libraries and also non-one-hot encode case\n","* Need to implement incl_xyz_accel and incl_rms_accel\n"]},{"cell_type":"code","metadata":{"id":"q6H67o-YARCx","executionInfo":{"status":"ok","timestamp":1624476679146,"user_tz":300,"elapsed":154,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["import os\n","import shutil #https://docs.python.org/3/library/shutil.html\n","from shutil import unpack_archive # to unzip\n","#from shutil import make_archive # to create zip for storage\n","import requests #for downloading zip file\n","import glob # to generate lists of files in directory - unix style pathnames\n","#from scipy import io #for loadmat, matlab conversion\n","import pandas as pd\n","import numpy as np\n","#import matplotlib.pyplot as plt # for plotting - pandas uses matplotlib\n","from tabulate import tabulate # for verbose tables\n","from tensorflow.keras.utils import to_categorical # for one-hot encoding"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"G1U-k5KcvL65","executionInfo":{"status":"ok","timestamp":1624476680192,"user_tz":300,"elapsed":107,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["def get_mobiact_fname_mdata(path_in):\n","    \"\"\"returns dataframe with filename and metadata from mobiact directory\n","    args: path_in is location of files e.g. JOG, JUM, etc directories\n","    returns: pandas dataframe with one row for each file.\n","    columns are full filename, activity, subject, group {unassigned, train, validate, test}\"\"\"\n","    df = pd.DataFrame() # new empty dataframe\n","    GRP = ['train','validation','test']\n","    ACT = ['JOG','JUM','STD','STN','STU','WAL']\n","    for i in ACT:    \n","        sub_path = path_in + i + '/'\n","        fname_in = i + '_acc_*.txt'\n","        print(\"Generating filenames \", fname_in,\" from \", sub_path,\" directory\")\n","        # Build list of matching files in each directory, make into dataframe\n","        # extract and add metadata\n","        in_files = glob.glob(os.path.join(sub_path, fname_in)) # generates a list of all matching files\n","        temp = pd.DataFrame(in_files,dtype=\"string\") # easier in pandas, make column of filenames\n","        temp.columns = ['fname'] # name the column\n","        temp['ACT'] = i # add column with activity\n","        temp['SUB'] = temp['fname'].str.extract(r'(\\d+)') # uses pd method and regex get SUB from fname\n","        # both columns are type object, would be better to set but didn't work first time\n","        df = pd.concat([df,temp], ignore_index=True ) # concat needs two df, add regardless of index   \n","    # add column for validation, test groups by subject, others are marked train\n","    df[\"SUB\"] = pd.to_numeric(df[\"SUB\"]) #convert from object to INT32\n","    return df"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"1to1Guz1Vq3U","executionInfo":{"status":"ok","timestamp":1624476681145,"user_tz":300,"elapsed":101,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["def assign_group(df,split_subj):\n","    \"\"\"returns dataframe with filename and metadata from mobiact directory\n","    args: dataframe with mobiact fname, ACT, SUB columns\n","    returns: dataframe with GRP = {train, test, validate} column added\n","    GRP assignment is based on list in split_subj\n","    \"\"\"\n","    df['GRP'] = 'unassigned' # add column\n","    df.loc[df['SUB'].isin(split_subj['train_subj']),'GRP'] = 'train'\n","    df.loc[df['SUB'].isin(split_subj['validation_subj']),'GRP'] = 'validation'\n","    df.loc[df['SUB'].isin(split_subj['test_subj']),'GRP'] = 'test'\n","    return df"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7fEvWjwKp8K","executionInfo":{"status":"ok","timestamp":1624476682103,"user_tz":300,"elapsed":110,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["def read_mobiact_file(full_filename):\n","    \"\"\"returns dataframe from Mobiact txt file accel_xyz data, skips metadata, labels columns\"\"\"\n","    df = pd.read_csv(full_filename,skiprows=16, header=None) #skip 16 lines of metadata\n","    # json better? https://docs.python.org/3/library/json.html#module-json initial tries weren't successful\n","    df.columns = [\"nanoseconds\", \"accel_x\", \"accel_y\", \"accel_z\"]\n","    return df"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"vTk-MhZKZ23l","executionInfo":{"status":"ok","timestamp":1624476683020,"user_tz":300,"elapsed":112,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["def add_total_accel(df_in, delete_xyz=False):\n","    \"\"\"computes rms of accel_x/y/z data, removed 1g component, adds accel_total column\"\"\"\n","    # compute rms accel and remove 1g due to gravity, removes rotational dependency\n","    dfx = df_in.pow(2)[['accel_x','accel_y','accel_z']] #square each accel\n","    df_sum = dfx.sum(axis=1) #add sum of squares, new 1 col df\n","    df_in.loc[:,'accel_total'] = df_sum.pow(0.5)-9.8  # sqrt + remove 1g gravity component\n","    del dfx, df_sum\n","    if delete_xyz:\n","        return df_in.drop(columns = ['accel_x','accel_y','accel_z'])\n","    else: \n","        return df_in"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"xBdeXqq-ldEj","executionInfo":{"status":"ok","timestamp":1624476684042,"user_tz":300,"elapsed":110,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["def to_abs_time(df):\n","    \"\"\"subtracts the value of 1st nanosecond sample from all samples\"\"\"\n","    start_time = df.loc[0,'nanoseconds']\n","    df.loc[:,'nanoseconds'] = df.loc[:,'nanoseconds'] - start_time\n","    return df"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ohf-StQbEBUT","executionInfo":{"status":"ok","timestamp":1624476684895,"user_tz":300,"elapsed":10,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["def get_df_from_file(fname, start_discard=100, end_discard=100):\n","    \"\"\"processes mobiact file fname and returns dataframe of accel values\n","    inputs: fname = full filemane\n","            start_discard = number of initial samples to delete (default = 200)\n","            end_discard = number of final samples to delete (default = 200)\n","    output: pandas dataframe of shape (samples, accel_total)\"\"\"\n","    df = read_mobiact_file(fname)\n","    df = add_total_accel(df,delete_xyz=True)\n","    df = df.drop(columns=['nanoseconds']) # note this loses some time info - datetime better?\n","    df.drop(df.head(start_discard).index,inplace=True) # drop first rows\n","    df.drop(df.tail(end_discard).index,inplace=True) # drop last rows\n","    return df"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"7hR56XLZ20Ie","executionInfo":{"status":"ok","timestamp":1624476686017,"user_tz":300,"elapsed":119,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["def split_df_npX(df, num_samples = 500):\n","    \"\"\"converts dataframe (samples, total_accel) into 3D numpy array of shape\n","    (num_segments, num_samples, total_accel). e.g. a 1800 row dataframe will\n","    result in a (3,500,1) numpy array.   The samples that don't populate an\n","    entire segment are discarded and there is no overlap (sliding window)\n","    input:  dataframe with one files worth of data, index=samples, column=total_accel\n","            num_samples = number samples (rows) in each segment, default = 500\n","    ouput:  numpy array in trainX format shape = (segments, samples/segment, 1)\"\"\"\n","    temp = df.to_numpy() # easier to reshape and final form needed anyway\n","    if (temp.shape[0]//num_samples)==0:\n","        print (\"WARNING:  File contains less than \",num_samples,\"samples and is discarded\")\n","    temp2 = temp[0:num_samples*(temp.shape[0]//num_samples)] # truncate to multiple of num_samples\n","    tempX = temp2.reshape(-1,num_samples,1) # won't work with out truncation\n","    return tempX"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"J37Cv5yN6y4v","executionInfo":{"status":"ok","timestamp":1624476687891,"user_tz":300,"elapsed":237,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}}},"source":["def mobiact_adl_load_dataset(\n","    verbose = True,\n","    #Pass location of the original MobiAct zip file here.\n","    #Easiest way to find this in colab is:\n","    #  -mount your google drive with MobiAct zip file (link is above)\n","    #  -navigate to the file using File menu to left\n","    #  -right click on file and select 'copy path', paste in next line\n","    orig_zipfile = '/content/drive/My Drive/Datasets/MobiAct_Dataset_v1.0.zip',\n","    incl_xyz_accel = False, #include component accel_x/y/z in ____X data\n","    incl_rms_accel = True, #add rms value of accel_x/y/z in ____X data\n","    incl_val_group = False, #True => returns x/y_test, x/y_validation, x/y_train\n","                           #False => combine test & validation groups\n","    split_subj = {'train_subj':[2,4,5,9,10,16,18,20,23,24,26,27,28,32,34,35,\n","                                36,38,42,45,46,47,48,49,50,51,52,53,54,57],\n","                   'validation_subj':[3,6,8,11,12,22,37,40,43,56],\n","                   'test_subj':[7,19,21,25,29,33,39,41,44,55]},\n","    one_hot_encode = True\n","    ):\n","    #unzip original dataset from google drive map into colab session\n","    if (not os.path.isdir('/content/MobiAct_Dataset')):\n","        print(\"Unzipping MobiAct Dataset\")\n","        shutil.unpack_archive(orig_zipfile,'/content','zip')\n","    else:\n","        print(\"Using existing archive in colab\")\n","    df_flist = get_mobiact_fname_mdata('/content/MobiAct_Dataset/')\n","    df_flist = assign_group(df_flist,split_subj)\n","    #Note:  STU, STN files don't contain 900 samples so 200 discard start/finish + 500 time step doesn't work\n","    # Create zero'd np arrays otherwise accumulates when run more than once\n","    # should be a better way to do this\n","    trainX = np.zeros(shape=(1,500,1)) #otherwise accumulates when run more than once\n","    trainy = np.zeros(shape=(1,6))\n","    validationX = np.zeros(shape=(1,500,1))\n","    validationy = np.zeros(shape=(1,6))\n","    testX = np.zeros(shape=(1,500,1))\n","    testy = np.zeros(shape=(1,6))\n","    for i in df_flist.index:\n","        if ((df_flist['GRP'][i])=='train'):\n","            #print (\"processing train file\", df_flist['fname'][i])\n","            df = get_df_from_file(df_flist['fname'][i])\n","            tempX = split_df_npX(df)\n","            trainX = np.vstack([trainX, tempX])\n","            tempy = np.zeros(shape=(tempX.shape[0],6)) # 6 is number of ACT\n","            #one hot encoding of ACT\n","            #Class indices are  {'JOG': 0, 'JUM': 1, 'STD': 2, 'STN': 3, 'STU': 4, 'WAL': 5}\n","            #to match the ones generated by Keras image import\n","            if ((df_flist['ACT'][i])=='JOG'): tempy[:,0]=1\n","            if ((df_flist['ACT'][i])=='JUM'): tempy[:,1]=1\n","            if ((df_flist['ACT'][i])=='STD'): tempy[:,2]=1\n","            if ((df_flist['ACT'][i])=='STN'): tempy[:,3]=1\n","            if ((df_flist['ACT'][i])=='STU'): tempy[:,4]=1\n","            if ((df_flist['ACT'][i])=='WAL'): tempy[:,5]=1\n","            trainy = np.vstack([trainy, tempy])\n","        if ((df_flist['GRP'][i])=='validation'):\n","            #print (\"processing test file\", df_flist['fname'][i])\n","            df = get_df_from_file(df_flist['fname'][i])\n","            tempX = split_df_npX(df)\n","            validationX = np.vstack([validationX, tempX])\n","            tempy = np.zeros(shape=(tempX.shape[0],6)) # 6 is number of ACT\n","            #one hot encoding of ACT\n","            if ((df_flist['ACT'][i])=='JOG'): tempy[:,0]=1\n","            if ((df_flist['ACT'][i])=='JUM'): tempy[:,1]=1\n","            if ((df_flist['ACT'][i])=='STD'): tempy[:,2]=1\n","            if ((df_flist['ACT'][i])=='STN'): tempy[:,3]=1\n","            if ((df_flist['ACT'][i])=='STU'): tempy[:,4]=1\n","            if ((df_flist['ACT'][i])=='WAL'): tempy[:,5]=1\n","            validationy = np.vstack([validationy, tempy])\n","        if ((df_flist['GRP'][i])=='test'):\n","            #print (\"processing test file\", df_flist['fname'][i])\n","            df = get_df_from_file(df_flist['fname'][i])\n","            tempX = split_df_npX(df)\n","            testX = np.vstack([testX, tempX])\n","            tempy = np.zeros(shape=(tempX.shape[0],6)) # 6 is number of ACT\n","            #one hot encoding of ACT\n","            if ((df_flist['ACT'][i])=='JOG'): tempy[:,0]=1\n","            if ((df_flist['ACT'][i])=='JUM'): tempy[:,1]=1\n","            if ((df_flist['ACT'][i])=='STD'): tempy[:,2]=1\n","            if ((df_flist['ACT'][i])=='STN'): tempy[:,3]=1\n","            if ((df_flist['ACT'][i])=='STU'): tempy[:,4]=1\n","            if ((df_flist['ACT'][i])=='WAL'): tempy[:,5]=1\n","            testy = np.vstack([testy, tempy])\n","    #delete first row placeholders\n","    trainX = np.delete(trainX, (0), axis=0) \n","    trainy = np.delete(trainy, (0), axis=0)\n","    validationX = np.delete(validationX, (0), axis=0) \n","    validationy = np.delete(validationy, (0), axis=0)\n","    testX = np.delete(testX, (0), axis=0)\n","    testy = np.delete(testy, (0), axis=0)\n","    if (incl_val_group):\n","        return trainX, trainy, validationX, validationy, testX, testy\n","    else:\n","        return np.concatenate((trainX, validationX), axis=0),\\\n","            np.concatenate((trainy, validationy), axis=0),\\\n","            testX, testy\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"MaT1dfqavvtk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624476756857,"user_tz":300,"elapsed":28832,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}},"outputId":"02a82556-9333-45be-8b0b-fc8a76e25e8d"},"source":["if __name__ == \"__main__\":\n","    print(\"Downloading and processing MobiAct dataset, ADL Portion\")\n","    x_train, y_train, x_test, y_test = mobiact_adl_load_dataset()\n","    print(\"\\nMobiAct ADL returned arrays:\")\n","    print(\"x_train shape \",x_train.shape,\" y_train shape \", y_train.shape)\n","    print(\"x_test shape  \",x_test.shape,\" y_test shape  \",y_test.shape)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Downloading and processing MobiAct dataset, ADL Portion\n","Unzipping MobiAct Dataset\n","Generating filenames  JOG_acc_*.txt  from  /content/MobiAct_Dataset/JOG/  directory\n","Generating filenames  JUM_acc_*.txt  from  /content/MobiAct_Dataset/JUM/  directory\n","Generating filenames  STD_acc_*.txt  from  /content/MobiAct_Dataset/STD/  directory\n","Generating filenames  STN_acc_*.txt  from  /content/MobiAct_Dataset/STN/  directory\n","Generating filenames  STU_acc_*.txt  from  /content/MobiAct_Dataset/STU/  directory\n","Generating filenames  WAL_acc_*.txt  from  /content/MobiAct_Dataset/WAL/  directory\n","\n","MobiAct ADL returned arrays:\n","x_train shape  (5587, 500, 1)  y_train shape  (5587, 6)\n","x_test shape   (1395, 500, 1)  y_test shape   (1395, 6)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bavHcdnEa1N4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618523660266,"user_tz":300,"elapsed":13231,"user":{"displayName":"Lee Hinkle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgewSVTK-UUEEP0ihHQARBRqOb4YrK-IiepxHiI=s64","userId":"00071704663307985880"}},"outputId":"2ba4e0dc-66ef-468e-a14e-89883cdc2617"},"source":["#change to True for example/testing including\n","if (False):\n","    x_train, y_train, x_validation, y_validation, x_test, y_test = mobiact_adl_load_dataset(incl_val_group=True)\n","    print(\"x/y_train shape \",x_train.shape,y_train.shape)\n","    print(\"x/y_validation shape \",x_validation.shape,y_validation.shape)\n","    print(\"x/y_test shape  \",x_test.shape,y_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using existing archive in colab\n","Generating filenames  JOG_acc_*.txt  from  /content/MobiAct_Dataset/JOG/  directory\n","Generating filenames  JUM_acc_*.txt  from  /content/MobiAct_Dataset/JUM/  directory\n","Generating filenames  STD_acc_*.txt  from  /content/MobiAct_Dataset/STD/  directory\n","Generating filenames  STN_acc_*.txt  from  /content/MobiAct_Dataset/STN/  directory\n","Generating filenames  STU_acc_*.txt  from  /content/MobiAct_Dataset/STU/  directory\n","Generating filenames  WAL_acc_*.txt  from  /content/MobiAct_Dataset/WAL/  directory\n","x/y_train shape  (4190, 500, 1) (4190, 6)\n","x/y_validation shape  (1397, 500, 1) (1397, 6)\n","x/y_test shape   (1395, 500, 1) (1395, 6)\n"],"name":"stdout"}]}]}