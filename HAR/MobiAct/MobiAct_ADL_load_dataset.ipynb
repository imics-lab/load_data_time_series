{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPaCHNWz2YcYZXdenS/OSJS"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Khc4g511HMYk"},"source":["#MobiAct_ADL_load_dataset.ipynb. \n","Loads the 6 Activities of Daily living (ADL) from the first release of the MobiAct dataset and converts to numpy arrays while adhering to the general format of the [Keras MNIST load_data function](https://keras.io/api/datasets/mnist/#load_data-function).\n","\n","STD Standing (1x5min)\n","WAL Walking (1x5min)\n","JOG Jogging (3x30sec)\n","JUM Jumping (3x30sec)\n","STU Stairs Up (6x10sec)\n","STN Stairs Down (6x10sec)\n","\n","Only the timestamp 'nanoseconds' and accelerometer data (accel_x/y/z) is imported. Sitting, Fall, gyro, and orientation data is not used.\n","\n","Data is segmented into 500 samples (~5s) using as much of the sample as possible (incomplete segments are discarded)\n","\n","Returns: Tuple of Numpy arrays:   \n","(x_train, y_train),(x_valid, y_valid)\\[optional\\],(x_test, y_test) \n","\n","* x_train\\/valid\\/test: containing float64 with shapes (num_samples, 500, {3,4,1})\n","* y_train\\/valid\\/test: containing int8 with shapes (num_samples 0-9)\n","\n","Default train/valid/test split is by subject with a best effort to keep gender mix and distributed by height among the three categories.\n","Split is 60%/20%/20%\n","\n","Example usage:  \n","x_train, y_train, x_test, y_test = mobiact_adl_load_dataset()\n","\n","Additional References:   \n","For more information on the dataset, please refer to the publication:    Vavoulas, G., Chatzaki, C., Malliotakis, T., Pediaditis, M. and Tsiknakis, M.,(2016) The MobiAct Dataset: Recognition of Activities of Daily Living using Smartphones.,In Proceedings of the International Conference on Information and Communication Technologies for Ageing Well and e-Health  â€“ Volume 1: ICT4AWE, (ICT4AGEINGWELL 2016) ISBN 978-989-758-180-9, pages 143-151, Rome, Italy. DOI: 10.5220/0005792401430151\n","\n","[tensorflow git repo with more datasets](https://github.com/tensorflow/datasets) which also links to another colab [Keras MNIST Example](https://colab.research.google.com/github/tensorflow/datasets/blob/master/docs/keras_example.ipynb) with more options.\n","\n","Developed and tested using colab.research.google.com  \n","To save as .py version use File > Download .py\n","\n","Author:  Lee B. Hinkle, IMICS Lab, Texas State University, 2021\n","\n","<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.\n","\n","TODOs:\n","* Still can't figure out how to download source directly from https://drive.google.com/file/d/0B5VcW5yHhWhibWxGRTZDd0dGY2s/edit.\n","* Implement one-hot encoding using libraries and also non-one-hot encode case with int\n","* Change dtypes to take up less space\n","* verbose needs to be global and cleaned up\n","* add gyro channels and implement list based channel selection versus bools"]},{"cell_type":"code","metadata":{"id":"q6H67o-YARCx","executionInfo":{"status":"ok","timestamp":1672511828791,"user_tz":360,"elapsed":88,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["import os\n","import shutil #https://docs.python.org/3/library/shutil.html\n","from shutil import unpack_archive # to unzip\n","#from shutil import make_archive # to create zip for storage\n","import requests #for downloading zip file\n","import glob # to generate lists of files in directory - unix style pathnames\n","#from scipy import io #for loadmat, matlab conversion\n","import pandas as pd\n","import numpy as np\n","#import matplotlib.pyplot as plt # for plotting - pandas uses matplotlib\n","from tabulate import tabulate # for verbose tables\n","from tensorflow.keras.utils import to_categorical # for one-hot encoding"],"execution_count":79,"outputs":[]},{"cell_type":"code","source":["interactive = True # run example calls for each function as Jupyter Notebook"],"metadata":{"id":"knzV6vmCGXFd","executionInfo":{"status":"ok","timestamp":1672511828877,"user_tz":360,"elapsed":5,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["interactive = False # skip to run as interactive, executes main as .py"],"metadata":{"id":"sIf35GqJKIuP","executionInfo":{"status":"ok","timestamp":1672511828878,"user_tz":360,"elapsed":5,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"execution_count":81,"outputs":[]},{"cell_type":"code","source":["# Trying to figure out how to download automatically to eliminate drive mount\n","# gdown doesn't work even with confirm=t option - could be google\n","# limiting downloads from scripts (the link works fine when signed in)\n","# https://github.com/wkentaro/gdown/issues/43\n","# tried AnirudhJayant06 colab suggestion of\n","# !pip install --upgrade --no-cache-dir gdown\n","# !gdown \"0B5VcW5yHhWhibWxGRTZDd0dGY2s&confirm=t\" # Download MobiAct zipfile\n","# still no luck and search for alternative locations came up empty."],"metadata":{"id":"hN5MyVnv3Lcy","executionInfo":{"status":"ok","timestamp":1672511829024,"user_tz":360,"elapsed":151,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"execution_count":82,"outputs":[]},{"cell_type":"code","source":["def get_mobiact():\n","    zip_fname = 'MobiAct_Dataset_v1.0.zip'\n","    if (not os.path.isfile(zip_fname)):\n","        print(zip_fname, 'not found.\\nPlease go to',\\\n","        'https://drive.google.com/uc?id=0B5VcW5yHhWhibWxGRTZDd0dGY2s')\n","        print('to download, then copy the file to current working directory')\n","        print('upload to colab takes ~5min and gives \"not a zipfile\" error until finished')\n","        quit() # to exit when run as python script\n","    elif (not os.path.isdir('MobiAct_Dataset')):\n","        print(\"Unzipping MobiAct Dataset\")\n","        shutil.unpack_archive(zip_fname,'.','zip')\n","    else:\n","        print(\"Using existing archive\")\n","if interactive:\n","    get_mobiact()"],"metadata":{"id":"J7noEIt75rPy","executionInfo":{"status":"ok","timestamp":1672511829024,"user_tz":360,"elapsed":5,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","metadata":{"id":"G1U-k5KcvL65","executionInfo":{"status":"ok","timestamp":1672511829025,"user_tz":360,"elapsed":6,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def get_mobiact_fname_mdata(path_in):\n","    \"\"\"returns dataframe with filename and metadata from mobiact directory\n","    args: path_in is location of files e.g. JOG, JUM, etc directories\n","    returns: pandas dataframe with one row for each file.\n","    columns are full filename, activity, subject, group {unassigned, train, validate, test}\"\"\"\n","    df = pd.DataFrame() # new empty dataframe\n","    GRP = ['train','valid','test']\n","    ACT = ['JOG','JUM','STD','STN','STU','WAL']\n","    for i in ACT:    \n","        sub_path = path_in + i + '/'\n","        fname_in = i + '_acc_*.txt'\n","        print(\"Generating filenames \", fname_in,\" from \", sub_path,\" directory\")\n","        # Build list of matching files in each directory, make into dataframe\n","        # extract and add metadata\n","        in_files = glob.glob(os.path.join(sub_path, fname_in)) # generates a list of all matching files\n","        temp = pd.DataFrame(in_files,dtype=\"string\") # easier in pandas, make column of filenames\n","        temp.columns = ['fname'] # name the column\n","        temp['ACT'] = i # add column with activity\n","        temp['SUB'] = temp['fname'].str.extract(r'(\\d+)') # uses pd method and regex get SUB from fname\n","        # both columns are type object, would be better to set but didn't work first time\n","        df = pd.concat([df,temp], ignore_index=True ) # concat needs two df, add regardless of index   \n","    # add column for valid, test groups by subject, others are marked train\n","    df[\"SUB\"] = pd.to_numeric(df[\"SUB\"]) #convert from object to INT32\n","    return df\n","if interactive:\n","    df_flist = get_mobiact_fname_mdata('MobiAct_Dataset/')\n","    display(df_flist.head())"],"execution_count":84,"outputs":[]},{"cell_type":"code","metadata":{"id":"1to1Guz1Vq3U","executionInfo":{"status":"ok","timestamp":1672511829026,"user_tz":360,"elapsed":6,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def assign_group(df,split_subj):\n","    \"\"\"returns dataframe with filename and metadata from mobiact directory\n","    args: dataframe with mobiact fname, ACT, SUB columns\n","    returns: dataframe with GRP = {train, test, validate} column added\n","    GRP assignment is based on list in split_subj\n","    \"\"\"\n","    df['GRP'] = 'unassigned' # add column\n","    df.loc[df['SUB'].isin(split_subj['train_subj']),'GRP'] = 'train'\n","    df.loc[df['SUB'].isin(split_subj['valid_subj']),'GRP'] = 'valid'\n","    df.loc[df['SUB'].isin(split_subj['test_subj']),'GRP'] = 'test'\n","    return df\n","if interactive:\n","    split_subj = {'train_subj':[2,4,5,9,10,16,18,20,23,24,26,27,28,32,34,35,\n","                            36,38,42,45,46,47,48,49,50,51,52,53,54,57],\n","                'valid_subj':[3,6,8,11,12,22,37,40,43,56],\n","                'test_subj':[7,19,21,25,29,33,39,41,44,55]}\n","    df_flist = assign_group(df_flist, split_subj)\n","    display(df_flist.head())\n","    print('Use iloc to access a single element')\n","    print(df_flist['fname'].iloc[0])\n"],"execution_count":85,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7fEvWjwKp8K","executionInfo":{"status":"ok","timestamp":1672511829026,"user_tz":360,"elapsed":6,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def read_mobiact_file(full_filename):\n","    \"\"\"returns dataframe from Mobiact txt file accel_xyz data, skips metadata, labels columns\"\"\"\n","    df = pd.read_csv(full_filename,skiprows=16, header=None) #skip 16 lines of metadata\n","    # json better? https://docs.python.org/3/library/json.html#module-json initial tries weren't successful\n","    df.columns = [\"nanoseconds\", \"accel_x\", \"accel_y\", \"accel_z\"]\n","    return df\n","if interactive:\n","    my_df = read_mobiact_file(df_flist['fname'].iloc[0]) # iloc value is row\n","    display(my_df.head())"],"execution_count":86,"outputs":[]},{"cell_type":"code","metadata":{"id":"vTk-MhZKZ23l","executionInfo":{"status":"ok","timestamp":1672511829149,"user_tz":360,"elapsed":129,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def add_total_accel(df_in):\n","    \"\"\"computes rms of accel_x/y/z data, removed 1g component, adds accel_total column\"\"\"\n","    # compute rms accel and remove 1g due to gravity, removes rotational dependency\n","    dfx = df_in.pow(2)[['accel_x','accel_y','accel_z']] #square each accel\n","    df_sum = dfx.sum(axis=1) #add sum of squares, new 1 col df\n","    df_in.loc[:,'accel_total'] = df_sum.pow(0.5)-9.8  # sqrt + remove 1g gravity component\n","    del dfx, df_sum\n","    return df_in\n","if interactive:\n","    my_df =  add_total_accel(my_df)\n","    display(my_df.head())"],"execution_count":87,"outputs":[]},{"cell_type":"code","metadata":{"id":"xBdeXqq-ldEj","executionInfo":{"status":"ok","timestamp":1672511829150,"user_tz":360,"elapsed":7,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def to_abs_time(df):\n","    \"\"\"subtracts the value of 1st nanosecond sample from all samples\"\"\"\n","    start_time = df.loc[0,'nanoseconds']\n","    df.loc[:,'nanoseconds'] = df.loc[:,'nanoseconds'] - start_time\n","    return df\n","if interactive:\n","    my_df =  to_abs_time(my_df)\n","    display(my_df.head())"],"execution_count":88,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ohf-StQbEBUT","executionInfo":{"status":"ok","timestamp":1672511829151,"user_tz":360,"elapsed":8,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def get_df_from_file(fname, start_discard=100, end_discard=100):\n","    \"\"\"processes mobiact file fname and returns dataframe of accel values\n","    inputs: fname = full filemane\n","            start_discard = number of initial samples to delete (default = 200)\n","            end_discard = number of final samples to delete (default = 200)\n","    output: pandas dataframe of shape (samples, accel_total)\"\"\"\n","    df = read_mobiact_file(fname)\n","    df = add_total_accel(df)\n","    df = df.drop(columns=['nanoseconds']) # note this loses some time info - datetime better?\n","    df.drop(df.head(start_discard).index,inplace=True) # drop first rows\n","    df.drop(df.tail(end_discard).index,inplace=True) # drop last rows\n","    return df\n","if interactive:\n","    my_df = get_df_from_file(df_flist['fname'].iloc[0])\n","    display(my_df.head())"],"execution_count":89,"outputs":[]},{"cell_type":"code","metadata":{"id":"7hR56XLZ20Ie","executionInfo":{"status":"ok","timestamp":1672511829152,"user_tz":360,"elapsed":9,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def split_df_npX(df, num_samples = 500):\n","    \"\"\"converts dataframe (samples, total_accel) into 3D numpy array of shape\n","    (num_segments, num_samples, total_accel). e.g. a 1800 row dataframe will\n","    result in a (3,500,1) numpy array.   The samples that don't populate an\n","    entire segment are discarded and there is no overlap (sliding window)\n","    input:  dataframe with one files worth of data, index=samples, column=total_accel\n","            num_samples = number samples (rows) in each segment, default = 500\n","    ouput:  numpy array in trainX format shape = (segments, samples/segment, 1)\"\"\"\n","    temp = df.to_numpy() # easier to reshape and final form needed anyway\n","    if (temp.shape[0]//num_samples)==0:\n","        print (\"WARNING:  File contains less than \",num_samples,\"samples and is discarded\")\n","    temp2 = temp[0:num_samples*(temp.shape[0]//num_samples)] # truncate to multiple of num_samples\n","    # df.shape[1] is the number of columns, which should be preserved as the third dimension\n","    tempX = temp2.reshape(-1,num_samples,df.shape[1]) # won't work with out truncation\n","    return tempX\n","if interactive:\n","    np_temp = split_df_npX(my_df)\n","    print(\"Array shape\",np_temp.shape)\n","    with np.printoptions(precision=3, suppress=True): # make output shorter\n","        print(np_temp[:5]) # first 5 entries"],"execution_count":90,"outputs":[]},{"cell_type":"code","metadata":{"id":"J37Cv5yN6y4v","executionInfo":{"status":"ok","timestamp":1672511829256,"user_tz":360,"elapsed":112,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def mobiact_adl_load_dataset(\n","    verbose = True,\n","    #Pass location of the original MobiAct zip file here.\n","    #Easiest way to find this in colab is:\n","    #  -mount your google drive with MobiAct zip file (link is above)\n","    #  -navigate to the file using File menu to left\n","    #  -right click on file and select 'copy path', paste in next line\n","    # orig_zipfile = '/content/drive/My Drive/Datasets/MobiAct_Dataset_v1.0.zip',\n","    incl_xyz_accel = False, #include component accel_x/y/z in ____X data\n","    incl_ttl_accel = True, #add rms value of accel_x/y/z in ____X data\n","    incl_val_group = False, #True => returns x/y_test, x/y_valid, x/y_train\n","                           #False => combine test & valid groups\n","    split_subj = {'train_subj':[2,4,5,9,10,16,18,20,23,24,26,27,28,32,34,35,\n","                                36,38,42,45,46,47,48,49,50,51,52,53,54,57],\n","                   'valid_subj':[3,6,8,11,12,22,37,40,43,56],\n","                   'test_subj':[7,19,21,25,29,33,39,41,44,55]},\n","    one_hot_encode = True\n","    ):\n","\n","    \"\"\"Loads and processes the MobiAct dataset accel channels for ADL activities.\n","\n","    Args:\n","        verbose (bool): controls level of output (default is True)\n","        incl_xyz_accel (bool): returns the component acceleration (default is False)\n","        incl_ttl_accel (bool): returns the magnitude of accel vector minus 1g\n","        incl_val_group (bool): return includes x_valid and y_valid arrays (default is False)\n","        split_sub (dict): subject numbers assigned to train, valid, test groups\n","        one_hot_encode (bool): return one-hot-encoded y arrays\n","\n","    Returns:\n","        trainX, trainy, validX, validy, testX, testy np arrays\n","    \"\"\"\n","\n","    get_mobiact()\n","    df_flist = get_mobiact_fname_mdata('MobiAct_Dataset/')\n","    df_flist = assign_group(df_flist,split_subj)\n","    #Note:  STU, STN files don't contain 900 samples so 200 discard start/finish + 500 time step doesn't work\n","    # Create zero'd np arrays otherwise accumulates when run more than once\n","    # should be a better way to do this\n","    # FIXME:  Third dimension must follow accel return values - in all three lines\n","    trainX = np.zeros(shape=(1,500,4)) #otherwise accumulates when run more than once\n","    trainy = np.zeros(shape=(1,6))\n","    validX = np.zeros(shape=(1,500,4))\n","    validy = np.zeros(shape=(1,6))\n","    testX = np.zeros(shape=(1,500,4))\n","    testy = np.zeros(shape=(1,6))\n","    for i in df_flist.index:\n","        if ((df_flist['GRP'][i])=='train'):\n","            #print (\"processing train file\", df_flist['fname'][i])\n","            df = get_df_from_file(df_flist['fname'][i])\n","            tempX = split_df_npX(df)\n","            trainX = np.vstack([trainX, tempX])\n","            tempy = np.zeros(shape=(tempX.shape[0],6)) # 6 is number of ACT\n","            #one hot encoding of ACT\n","            #Class indices are  {'JOG': 0, 'JUM': 1, 'STD': 2, 'STN': 3, 'STU': 4, 'WAL': 5}\n","            #to match the ones generated by Keras image import\n","            if ((df_flist['ACT'][i])=='JOG'): tempy[:,0]=1\n","            if ((df_flist['ACT'][i])=='JUM'): tempy[:,1]=1\n","            if ((df_flist['ACT'][i])=='STD'): tempy[:,2]=1\n","            if ((df_flist['ACT'][i])=='STN'): tempy[:,3]=1\n","            if ((df_flist['ACT'][i])=='STU'): tempy[:,4]=1\n","            if ((df_flist['ACT'][i])=='WAL'): tempy[:,5]=1\n","            trainy = np.vstack([trainy, tempy])\n","        if ((df_flist['GRP'][i])=='valid'):\n","            #print (\"processing test file\", df_flist['fname'][i])\n","            df = get_df_from_file(df_flist['fname'][i])\n","            tempX = split_df_npX(df)\n","            validX = np.vstack([validX, tempX])\n","            tempy = np.zeros(shape=(tempX.shape[0],6)) # 6 is number of ACT\n","            #one hot encoding of ACT\n","            if ((df_flist['ACT'][i])=='JOG'): tempy[:,0]=1\n","            if ((df_flist['ACT'][i])=='JUM'): tempy[:,1]=1\n","            if ((df_flist['ACT'][i])=='STD'): tempy[:,2]=1\n","            if ((df_flist['ACT'][i])=='STN'): tempy[:,3]=1\n","            if ((df_flist['ACT'][i])=='STU'): tempy[:,4]=1\n","            if ((df_flist['ACT'][i])=='WAL'): tempy[:,5]=1\n","            validy = np.vstack([validy, tempy])\n","        if ((df_flist['GRP'][i])=='test'):\n","            #print (\"processing test file\", df_flist['fname'][i])\n","            df = get_df_from_file(df_flist['fname'][i])\n","            tempX = split_df_npX(df)\n","            testX = np.vstack([testX, tempX])\n","            tempy = np.zeros(shape=(tempX.shape[0],6)) # 6 is number of ACT\n","            #one hot encoding of ACT\n","            if ((df_flist['ACT'][i])=='JOG'): tempy[:,0]=1\n","            if ((df_flist['ACT'][i])=='JUM'): tempy[:,1]=1\n","            if ((df_flist['ACT'][i])=='STD'): tempy[:,2]=1\n","            if ((df_flist['ACT'][i])=='STN'): tempy[:,3]=1\n","            if ((df_flist['ACT'][i])=='STU'): tempy[:,4]=1\n","            if ((df_flist['ACT'][i])=='WAL'): tempy[:,5]=1\n","            testy = np.vstack([testy, tempy])\n","    # delete first row placeholders\n","    trainX = np.delete(trainX, (0), axis=0) \n","    trainy = np.delete(trainy, (0), axis=0)\n","    validX = np.delete(validX, (0), axis=0) \n","    validy = np.delete(validy, (0), axis=0)\n","    testX = np.delete(testX, (0), axis=0)\n","    testy = np.delete(testy, (0), axis=0)\n","\n","    # delete channels using numpy.delete(arr, obj, axis=None)\n","    # this should be replaced with channel list, also returns empty if both False\n","    if (incl_ttl_accel == False):\n","        trainX = np.delete(trainX, 3, axis = 2)\n","        validX = np.delete(validX, 3, axis = 2)\n","        testX = np.delete(testX, 3, axis = 2)\n","    if (incl_xyz_accel == False):\n","        trainX = np.delete(trainX, [0,1,2], axis = 2)\n","        validX = np.delete(validX, [0,1,2], axis = 2)\n","        testX = np.delete(testX, [0,1,2], axis = 2)\n","    if (incl_val_group):\n","        return trainX, trainy, validX, validy, testX, testy\n","    else:\n","        return np.concatenate((trainX, validX), axis=0),\\\n","            np.concatenate((trainy, validy), axis=0),\\\n","            testX, testy\n"],"execution_count":91,"outputs":[]},{"cell_type":"code","metadata":{"id":"MaT1dfqavvtk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672511863053,"user_tz":360,"elapsed":33799,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}},"outputId":"1fca1719-f4f3-4a92-f21b-bfb3efd86480"},"source":["if __name__ == \"__main__\":\n","    print(\"Downloading and processing MobiAct dataset, ADL Portion\")\n","    x_train, y_train, x_test, y_test = mobiact_adl_load_dataset()\n","    print(\"\\nMobiAct ADL returned arrays with default settings:\")\n","    print(\"x_train shape \",x_train.shape,\" y_train shape \", y_train.shape)\n","    print(\"x_test shape  \",x_test.shape,\" y_test shape  \",y_test.shape)\n","    print(80*'-')\n","    x_train, y_train, x_valid, y_valid, x_test, y_test = mobiact_adl_load_dataset(incl_xyz_accel = True, incl_val_group= True)\n","    print(\"\\nMobiAct ADL returned arrays, incl_xyz_accel and incl_val_grp are True:\")\n","    print(\"x_train shape \",x_train.shape,\" y_train shape \", y_train.shape)\n","    print(\"x_valid shape  \",x_valid.shape,\" y_valid shape  \",y_valid.shape)\n","    print(\"x_test shape  \",x_test.shape,\" y_test shape  \",y_test.shape)"],"execution_count":92,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading and processing MobiAct dataset, ADL Portion\n","Using existing archive\n","Generating filenames  JOG_acc_*.txt  from  MobiAct_Dataset/JOG/  directory\n","Generating filenames  JUM_acc_*.txt  from  MobiAct_Dataset/JUM/  directory\n","Generating filenames  STD_acc_*.txt  from  MobiAct_Dataset/STD/  directory\n","Generating filenames  STN_acc_*.txt  from  MobiAct_Dataset/STN/  directory\n","Generating filenames  STU_acc_*.txt  from  MobiAct_Dataset/STU/  directory\n","Generating filenames  WAL_acc_*.txt  from  MobiAct_Dataset/WAL/  directory\n","\n","MobiAct ADL returned arrays with default settings:\n","x_train shape  (5587, 500, 1)  y_train shape  (5587, 6)\n","x_test shape   (1395, 500, 1)  y_test shape   (1395, 6)\n","--------------------------------------------------------------------------------\n","Using existing archive\n","Generating filenames  JOG_acc_*.txt  from  MobiAct_Dataset/JOG/  directory\n","Generating filenames  JUM_acc_*.txt  from  MobiAct_Dataset/JUM/  directory\n","Generating filenames  STD_acc_*.txt  from  MobiAct_Dataset/STD/  directory\n","Generating filenames  STN_acc_*.txt  from  MobiAct_Dataset/STN/  directory\n","Generating filenames  STU_acc_*.txt  from  MobiAct_Dataset/STU/  directory\n","Generating filenames  WAL_acc_*.txt  from  MobiAct_Dataset/WAL/  directory\n","\n","MobiAct ADL returned arrays, incl_xyz_accel and incl_val_grp are True:\n","x_train shape  (4190, 500, 4)  y_train shape  (4190, 6)\n","x_valid shape   (1397, 500, 4)  y_valid shape   (1397, 6)\n","x_test shape   (1395, 500, 4)  y_test shape   (1395, 6)\n"]}]},{"cell_type":"code","metadata":{"id":"b2S4BThhXuqw","executionInfo":{"status":"ok","timestamp":1672511863054,"user_tz":360,"elapsed":25,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["\n","if False: # mount drive and change to true to save files interactively\n","    import time\n","    #output_dir = '/content/drive/MyDrive/Processed_Datasets/MobiAct/All_Accel_Numpys'\n","    output_dir = '.' # if running locally or you want to download files manually\n","    if (os.path.isdir(output_dir)):\n","        #quick check for existing files, '.ipynb_checkpoints' file \n","        #makes it more complicated to see if directory is empty\n","        if (not os.path.isfile(output_dir + '/x_train.npy')):\n","            summary = \"MobiAct data\\n\"\n","            summary += \"Saved to \" + output_dir + \"\\n\"\n","            summary += \"Generated by MobiAct_ADL_load_dataset.ipynb\\n\"\n","            summary += \" on \" + time.strftime('%b-%d-%Y_%H%M', time.localtime())+'\\n'\n","            summary += \"this version for ???? work\\n\"\n","            info_fname = output_dir +'/'+'README.txt'\n","            full_info = summary\n","            print(full_info)\n","\n","            with open(info_fname, \"w\") as file_object:\n","                file_object.write(full_info)\n","\n","            if True:\n","                np.save(output_dir + '/'+'x_train.npy',x_train)\n","                np.save(output_dir + '/'+'y_train.npy',y_train)\n","                np.save(output_dir + '/'+'x_valid.npy',x_valid)\n","                np.save(output_dir + '/'+'y_valid.npy',y_valid)\n","                np.save(output_dir + '/'+'x_test.npy',x_test)\n","                np.save(output_dir + '/'+'y_test.npy',y_test)\n","\n","        else:\n","            print(\"Error \"+output_dir+\" contains x_train.npy, please delete files\")\n","    else:\n","        print(output_dir + \" not found, please create directory\") "],"execution_count":93,"outputs":[]}]}