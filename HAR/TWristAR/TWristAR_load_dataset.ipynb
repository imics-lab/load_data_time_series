{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1WbviRoNfMEZwPiA0Jm0FruV9l8tODu_e","timestamp":1656703965031},{"file_id":"1RkiXI3GhB-rNtyUp_VYw05xiiuD_oDFA","timestamp":1612028534003}],"authorship_tag":"ABX9TyOwgVhj+nYEub0sszjuzJvG"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Khc4g511HMYk"},"source":["#TWristAR_load_dataset.ipynb\n","Loads the raw e4 signals and .csv label files from the [Zenodo repository](https://zenodo.org/record/5911808) and returns train and test X/y numpy arrays.\n","\n","The basic flow is:\n","* Download and unzip the dataset if not already present\n","* Convert each recording *session* into Intermediate Representation 1 (IR1) format - a datetime indexed pandas dataframe with columns for each channel plus the label and subject number.\n","* Transform the IR1 into IR2 - a set of three numpy arrays containing sliding window samples\n","   * X = (samples, time steps per sample, channels)  \n","   * y =  (samples, label) # activity classification  \n","   * s =  (samples, subject) # subject number\n","* Clean and further transforms the IR2 arrays as needed - note the transforms that can be applied here are train vs test dependent.   For example, the IR2 arrays in the training set may be rebalanced, but those in the test set should not.\n","* Concatenate the processed IR2 arrays into the final returned train/validate/test arrays.\n","\n","TWRistAR is small and easily downloadable so there is no option to used saved Intermediate Representations here as there is in some of the loaders for larger datasets.\n","\n","Set interactive to true to run the Jupyter Notebook version.  Note most of the calls are setup to test the functions, not process the entire dataset, to do that set interactive to false and run all so that main executes.   This notebook can be saved and run as a python file as well.\n","\n","This video describes the code https://mediaflo.txstate.edu/Watch/e4_data_processing. (updates have been made since this was made)\n","\n","\n","Acknowledgement to and a good example of the WISDM format being pre-processed is https://towardsdatascience.com/human-activity-recognition-har-tutorial-with-keras-and-core-ml-part-1-8c05e365dfa0  by Nils Ackermann.  \n","\n","\n","<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.\n","\n","[Lee B. Hinkle](https://userweb.cs.txstate.edu/~lbh31/), Texas State University, [IMICS Lab](https://imics.wp.txstate.edu/)  \n","TODO:\n","* The IR3 transform could be further optimized by taking a dictionary of IR1 dataframes as was done in the later datasets.\n","* log_info needs to be updated to dictionary format so we can read things like the channel names automatically.\n","* Time is off by 6 hrs due to time zone issues - adjusted in Excel/csv but would be good to show it in the correct time zone.\n","* Need to incorporate session numbers or just use the alternate .csv files where validation was 'fake' subs 11 and 22 which were just a few of the sessions from subjects 1 and 2.  This was done in the Semi-Supervised version of the loader for WISHWell but not integrated back into this version.\n"]},{"cell_type":"markdown","source":["# Import Libraries and Common Load Dataset Code (from IMICS public repo)"],"metadata":{"id":"ghyvAqipvbvs"}},{"cell_type":"code","metadata":{"id":"q6H67o-YARCx","executionInfo":{"status":"ok","timestamp":1677623424811,"user_tz":360,"elapsed":12852,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["import os\n","import shutil #https://docs.python.org/3/library/shutil.html\n","from shutil import unpack_archive # to unzip\n","import time\n","import pandas as pd\n","import numpy as np\n","from numpy import savetxt\n","from tabulate import tabulate # for verbose tables, showing data\n","from tensorflow.keras.utils import to_categorical # for one-hot encoding\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import OneHotEncoder\n","from time import gmtime, strftime, localtime #for displaying Linux UTC timestamps in hh:mm:ss\n","from datetime import datetime, date\n","import urllib.request # to get files from web w/o !wget"],"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":495,"status":"ok","timestamp":1677623425302,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"},"user_tz":360},"outputId":"6ce0bdec-b0b7-42c4-df4c-9d371cabc60e","id":"gajdw42Dt_yO"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading load_data_utils.py from IMICS git repo\n","Downloading load_data_transforms.py from IMICS git repo\n"]}],"source":["def get_py_file(fname, url):\n","    \"\"\"checks for local file, if none downloads from URL.    \n","    :return: nothing\"\"\"\n","    #fname = 'load_data_utils.py'\n","    #ffname = os.path.join(my_dir,fname)\n","    if (os.path.exists(fname)):\n","        print (\"Local\",fname, \"found, skipping download\")\n","    else:\n","        print(\"Downloading\",fname, \"from IMICS git repo\")\n","        urllib.request.urlretrieve(url, filename=fname)\n","\n","get_py_file(fname = 'load_data_utils.py', url = 'https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/load_data_utils.py')\n","get_py_file(fname = 'load_data_transforms.py', url = 'https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/load_data_transforms.py')"]},{"cell_type":"code","source":["# common transforms and utils used by the individual loaders\n","import load_data_transforms as xform\n","import load_data_utils as utils"],"metadata":{"id":"_Ny0oT3ecvZP","executionInfo":{"status":"ok","timestamp":1677623425303,"user_tz":360,"elapsed":4,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Global Parameters"],"metadata":{"id":"iDsgBVo_BFkc"}},{"cell_type":"code","metadata":{"id":"1LkvTO5hujPH","executionInfo":{"status":"ok","timestamp":1677623425664,"user_tz":360,"elapsed":364,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["# environment and execution parameters\n","my_dir = '.' # replace with absolute path if desired\n","dataset_dir = my_dir # TWristAR zip file contains TWristAR directory\n","working_dir = os.path.join(my_dir,'TWristAR_temp') # temp dir for processing\n","\n","if not os.path.exists(working_dir):\n","    os.makedirs(working_dir)\n","interactive = True # for exploring data and functions interactively\n","verbose = True\n","\n","# dataset parameters\n","# frequency = 32 - note this is hardcoded due to the unique sample freqencies\n","# that differ between the individual e4 sensors\n","xform.time_steps = 96 # three seconds at 32Hz\n","xform.stride = 32 # one second step for each sliding window\n","# The label_map_<dataset> contains a mapping from strings to ints for all\n","# possible labels in the entire dataset.   This allows for predictable conversion\n","# regardless of the slices.  I'm using 99 for 'unknown' which will be dropped\n","# to avoid the confusion of shifing by 1 place, zero indexed etc.\n","label_map_twristar = {\"label\":     {\"Downstairs\": 0, \"Jogging\": 1, \"Sitting\": 2,\n","                                \"Standing\": 3, \"Upstairs\": 4, \"Walking\": 5,\n","                                \"Undefined\": 99}}"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"_arxQU-n6nKK","executionInfo":{"status":"ok","timestamp":1677623425664,"user_tz":360,"elapsed":5,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["interactive = False # don't run if interactive, automatically runs for .py version\n","verbose = False # to limit the called functions output"],"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def get_TWristAR():\n","    \"\"\"checks for local zipfile, if none downloads from zenodo repository\n","    after download will unzip the dataset into TWristAR directory.\n","    Assumes a global my_dir has been defined (default is my_dir = \".\")\n","    :return: nothing\"\"\"\n","    zip_ffname = os.path.join(my_dir,'TWristAR.zip')\n","    if (os.path.exists(zip_ffname)):\n","        if verbose:\n","            print (\"Local TWristAR.zip found, skipping download\")\n","    else:\n","        print(\"Downloading TWristAR from Zenodo\")\n","        urllib.request.urlretrieve(\"https://zenodo.org/record/5911808/files/TWristAR.zip\", filename=\"TWristAR.zip\")\n","    if (os.path.isdir(os.path.join(dataset_dir,'TWristAR'))):\n","        if verbose:\n","            print(\"Found existing TWristAR directory, skipping unzip\")\n","        return\n","    else:\n","        print(\"Unzipping TWristAR file in\", dataset_dir, \"directory\")\n","        if (os.path.exists(zip_ffname)):\n","            shutil.unpack_archive(zip_ffname,dataset_dir,'zip')\n","        else:\n","            print(\"Error: \", zip_ffname, \" not found, exiting\")\n","            return\n","if interactive:\n","    get_TWristAR()"],"metadata":{"id":"tlKEXl-BSrLO","executionInfo":{"status":"ok","timestamp":1677623425665,"user_tz":360,"elapsed":6,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"9oab3XMPgL8Z","executionInfo":{"status":"ok","timestamp":1677623426369,"user_tz":360,"elapsed":709,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def unzip_e4_file(zip_ffname):\n","    \"\"\"checks for local copy, if none unzips the e4 zipfile in working_dir\n","    Note:  the files themselves do not contain subject info and there are\n","    multiple files e.g. ACC.csv, BVP,csv etc, in each zipfile.\n","    It is very important to further process the files with <fname>_labels.csv\n","    :param zip_ffname: the path and filename of the zip file\n","    :param working_dir: local (colab) directory where csv files will be placed\n","    :return: nothing\"\"\"\n","    if not os.path.exists(working_dir):\n","        print(\"Error working directory\", working_dir, \"not found, unzip_e4_file exiting\")\n","        return\n","    if (os.path.exists(zip_ffname)):\n","        if verbose:\n","            print(\"Unzipping\",zip_ffname, \"in\", working_dir)\n","        shutil.unpack_archive(zip_ffname,working_dir,'zip')\n","    else:\n","        print(\"Error: \", zip_ffname, \" not found, exiting\")\n","        return\n","if interactive:\n","    zip_ffname = os.path.join(my_dir,'TWristAR','sub1/1574621345_A01F11.zip')\n","    unzip_e4_file(zip_ffname)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"65c2wpiOwVmg","executionInfo":{"status":"ok","timestamp":1677623426370,"user_tz":360,"elapsed":10,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def df_from_e4_csv (ffname,col_labels):\n","    \"\"\"\"reads e4 ACC, BVP, EDA, or TEMP(erature) csv files, uses start time and\n","    sample rate to create time indexed pandas dataframe with columns.  \n","    Note the other e4 files have different format and must be read seperately. \n","    :param ffname:  full filename e.g./content/temp/ACC.csv\n","    :col_labels: list of colums in csv - varies by type ['accel_x','accel_y...]\n","    :returns df: time indexed dataframe\"\"\"\n","\n","    df = pd.read_csv(ffname, header=None)\n","    start_time = df.iloc[0,0].astype('int64') # first line in e4 csv\n","    sample_freq = df.iloc[1,0].astype('int64') # second line in e4 csv\n","    df = df.drop(df.index[[0,1]]) # drop 1st two rows, index is now off by 2\n","    # Convert the index to datetime to allow for pandas resampling\n","    # The start_time from the e4 csv file is forced to int64 which represents the\n","    # number of nanoseconds since January 1, 1970, 00:00:00 (UTC)\n","    # This is tricky - if float representation the join function may not work\n","    # properly later since the indexes must match exactly.\n","    # UTC_time is computed for each row, then made into required datetime format\n","    # which is a int64 that pandas will accept as an index\n","    df['UTC_time'] = (df.index-2)/sample_freq + start_time\n","    end_time = df['UTC_time'].iloc[-1]\n","    if verbose:\n","        print(ffname, \"Sample frequency = \", sample_freq, \" Hz\")\n","        #show time in day month format, assumes same timezone\n","        print(\"File start time = \", strftime(\"%a, %d %b %Y %H:%M:%S\", localtime(start_time)))  \n","        print(\"File end time   = \",strftime(\"%a, %d %b %Y %H:%M:%S\", localtime(end_time)))\n","    df['datetime'] = pd.to_datetime(df['UTC_time'], unit='s')\n","    df.set_index('datetime',inplace=True)\n","    df = df.drop('UTC_time', axis=1)\n","    df.columns = col_labels\n","    return df\n","if interactive:\n","    # Note: IBI.csv is the inter-beat interval, a calculated value with a \n","    # different format.  HR.csv is also calculated from BVP but format is same.\n","    ffname = working_dir + '/ACC.csv'\n","    col_labels = ['accel_x', 'accel_y', 'accel_z']\n","    ir1_acc_df = df_from_e4_csv(ffname, col_labels)\n","    print(\"ACC dataframe shape\", ir1_acc_df.shape)\n","    display(ir1_acc_df.head())\n","\n","    ffname = working_dir + '/BVP.csv'\n","    col_labels = ['bvp']\n","    ir1_bvp_df = df_from_e4_csv(ffname, col_labels)\n","    print(\"BVP dataframe shape\", ir1_bvp_df.shape)\n","    display(ir1_bvp_df.head())\n","\n","    ffname = working_dir + '/EDA.csv'\n","    col_labels = ['eda']\n","    ir1_eda_df = df_from_e4_csv(ffname, col_labels)\n","    print(\"EDA dataframe shape\", ir1_eda_df.shape)\n","    display(ir1_eda_df.head())\n","\n","    ffname = working_dir + '/TEMP.csv'\n","    col_labels = ['p_temp']\n","    ir1_temp_df = df_from_e4_csv(ffname, col_labels)\n","    print(\"Temp dataframe shape\", ir1_temp_df.shape)\n","    display(ir1_temp_df.head())"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ozk2mZVYB-Q","executionInfo":{"status":"ok","timestamp":1677623426370,"user_tz":360,"elapsed":9,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def process_e4_accel(df):\n","    \"\"\"converts component accel into g and adds accel_ttl column\n","    per info.txt range is [-2g, 2g] and unit in this file is 1/64g.\n","    This method is e4 specific due to the way the accelerations is recorded\n","    \"\"\"\n","    df['accel_x'] = df['accel_x']/64\n","    df['accel_y'] = df['accel_y']/64\n","    df['accel_z'] = df['accel_z']/64\n","    df_sqd = df.pow(2)[['accel_x', 'accel_y', 'accel_z']] #square each accel\n","    df_sum = df_sqd.sum(axis=1) #add sum of squares, new 1 col df\n","    df.loc[:,'accel_ttl'] = df_sum.pow(0.5)-1  # sqrt and remove 1g due to gravity\n","    del df_sqd, df_sum\n","    return df\n","if interactive:\n","    ir1_acc_df = process_e4_accel(ir1_acc_df)\n","    display(ir1_acc_df.head())"],"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def get_ir1_from_e4_dir():\n","    \"\"\"processes the four e4 sensor files in global working directory into a \n","    single IR1 datetime indexed dataframe. Labeled columns are channels\"\"\"\n","    # Note: IBI.csv is the inter-beat interval, a calculated value with a \n","    # different format.  HR.csv is also calculated from BVP but format is same.\n","    # TODO:  Should check directory for all four files and uniform start/stop\n","    # times.\n","    # TODO: Might be better to use a different interpolation.  See\n","    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html\n","    ffname = working_dir + '/ACC.csv'\n","    col_labels = ['accel_x', 'accel_y', 'accel_z']\n","    ir1_acc_df = df_from_e4_csv(ffname, col_labels)\n","    ir1_acc_df = process_e4_accel(ir1_acc_df)\n","\n","    ffname = working_dir + '/BVP.csv'\n","    col_labels = ['bvp']\n","    ir1_bvp_df = df_from_e4_csv(ffname, col_labels)\n","\n","    ffname = working_dir + '/EDA.csv'\n","    col_labels = ['eda']\n","    ir1_eda_df = df_from_e4_csv(ffname, col_labels)\n","\n","    ffname = working_dir + '/TEMP.csv'\n","    col_labels = ['p_temp']\n","    ir1_ptemp_df = df_from_e4_csv(ffname, col_labels)\n","\n","    ir1_df = ir1_acc_df.join(ir1_bvp_df, how=\"inner\") # this drops bvp to 32Hz\n","    ir1_df = ir1_df.join(ir1_eda_df, how=\"outer\") # stays at 32Hz, eda fill NaN\n","    ir1_df = ir1_df.join(ir1_ptemp_df, how=\"outer\") # stays at 32Hz, p_temp fill NaN\n","    ir1_df = ir1_df.interpolate() # default is linear interpolation\n","    ir1_df = ir1_df.astype('float32') # no need for 64 precision with these sensors\n","    if verbose:\n","        print(\"IR1 full dataframe shape\",ir1_df.shape)\n","        #print(ir1_df.head(10))\n","    return ir1_df\n","if interactive:\n","    ir1_df = get_ir1_from_e4_dir()\n","    display(ir1_df.head(10))\n","    ir1_df.iloc[499:1999].plot(subplots=True, figsize=(20, 10)) # plot a few seconds"],"metadata":{"id":"Ax17Aew_zTUu","executionInfo":{"status":"ok","timestamp":1677623426371,"user_tz":360,"elapsed":10,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"YZNmr-yPWcck","executionInfo":{"status":"ok","timestamp":1677623426372,"user_tz":360,"elapsed":11,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def show_e4_tag_time(tag_ffname):\n","    \"\"\"utility prints time marks from e4 tags.csv to help with video sync \n","    and labeling.   When this is run in colab it seems to be GMT regardless\n","    of timezone settings.\"\n","    :param tag_ffname: file to be processed e.g. /content/temp/tags.csv'\n","    :return: nothing\"\"\"\n","    df_temp = pd.read_csv(tag_ffname, header=None)\n","    df_temp.columns = ['UTC_time']\n","    print (\"    UTC_time          Local Time\")\n","    for index, row in df_temp.iterrows():\n","        print(index, row['UTC_time'],\n","            strftime(\"%a, %d %b %Y %H:%M:%S\", localtime(row['UTC_time'])))\n","# https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior\n","# link to string formats for date and time\n","if interactive:\n","    print('Tag info (button presses) from tags.csv')\n","    tag_ffname = working_dir + '/tags.csv'\n","    show_e4_tag_time(tag_ffname)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"ttKS9Ox6JSed","executionInfo":{"status":"ok","timestamp":1677623426372,"user_tz":360,"elapsed":10,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def label_df_from_csv (df, labels_ffname):\n","    \"\"\"adds class label and subject number to the dataframe based on the\n","    contents of a .csv file containing time and label info.\n","    Example csv format (see e4_time_sync.xlsx to help build csv from video)\n","        start,finish,label,sub\n","        2019:11:24 18:49:51,2019:11:24 18:50:18,Upstairs,1\n","        2019:11:24 18:50:18,2019:11:24 18:50:45,Downstairs,1\n","    :param df : time indexed dataframe from df_from_e4_csv method\n","    :labels_ffname : csv file with metadata\n","    :return : a dataframe with label and subject columns added\"\"\"\n","    df_labels = pd.read_csv(labels_ffname)\n","    df_labels['start'] =  pd.to_datetime(df_labels['start'], format='%Y:%m:%d %H:%M:%S')\n","    df_labels['finish'] =  pd.to_datetime(df_labels['finish'], format='%Y:%m:%d %H:%M:%S')\n","    # quick check to make sure all subjects are the same - only 1 sub per csv\n","    if (not (df_labels['sub'].eq(df_labels['sub'].iloc[0]).all())):\n","        print('Warning: Multiple subjects detected in csv, unusual for e4 data.')\n","    df['label']='Undefined' # add column with safe value for labels\n","    df['sub'] = np.NaN\n","    for index, row in df_labels.iterrows():\n","        #print(row['start'], row['finish'],row['label'])\n","        df.loc[row['start']:row['finish'],'label'] = row['label']\n","        df.loc[row['start']:row['finish'],'sub'] = row['sub']\n","    return df\n","if interactive:\n","    labels_ffname = os.path.splitext(zip_ffname)[0] + '_labels.csv'\n","    print(\"Adding label and sub info from \",labels_ffname)\n","    ir1_df = label_df_from_csv(ir1_df, labels_ffname)\n","    display(ir1_df[5000:5005]) # head is meaningless since start is undefined\n","    #ir1_df['label'].value_counts()\n","    print (\"Label Counts - # samples before sliding window\")\n","    print (ir1_df['label'].value_counts())"],"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def get_twristar_ir1_dict():\n","    \"\"\"reads the TWRistAR dataset and converts each \"session file\" to an IR1\n","    dataframe.\n","    Returns: a dict containing key = df_name and item = IR1 dataframes.\"\"\"\n","    # A few notes - TWRristAR (or more specifically e4 wristband datafiles)\n","    # require a lot of processing, if trying to leverage for more traditional\n","    # .csv file format see Gesture Phase version.\n","    fn_list = ['sub1/1574621345_A01F11.zip',\n","                'sub1/1574622389_A01F11.zip',\n","                'sub1/1574624998_A01F11.zip',\n","                'sub2/1633107019_A01F11.zip',\n","                'sub2/1633108344_A01F11.zip',\n","                'sub2/1633109744_A01F11.zip',\n","                'sub3/1633704587_A01F11.zip',\n","                'sub3/1633705664_A01F11.zip',\n","                'sub3/1633711821_A01F11.zip']\n","    get_TWristAR()\n","    ir1_df_dict = dict() # an empty dictionary\n","    for item in fn_list:\n","        zip_ffname = os.path.join(my_dir,'TWristAR',item)\n","        if verbose:\n","            print('Processing ', zip_ffname)\n","        if not os.path.exists(working_dir):\n","            os.makedirs(working_dir)\n","        unzip_e4_file(zip_ffname)\n","        df = get_ir1_from_e4_dir()\n","        if verbose:\n","            print('Tag info (button presses) from tags.csv')\n","            tag_ffname = working_dir + '/tags.csv'\n","            show_e4_tag_time(tag_ffname)\n","        # Generate associated csv filename, forces the long numbered filenames to match\n","        labels_ffname = os.path.splitext(zip_ffname)[0] + '_labels.csv'\n","        df = label_df_from_csv (df, labels_ffname)\n","        df['label'].value_counts()\n","        if verbose:\n","            print (\"Label Counts - # samples before sliding window\\n\",ir1_df['label'].value_counts())\n","        # tighten up the column types for space savings.\n","        # change to 32-bit, credit/ref https://stackoverflow.com/questions/69188132/how-to-convert-all-float64-columns-to-float32-in-pandas\n","        # Select columns with 'float64' dtype  \n","        float64_cols = list(df.select_dtypes(include='float64'))\n","        # The same code again calling the columns\n","        df[float64_cols] = df[float64_cols].astype('float32')\n","        # Seems better to explicitly type the other columns vs object.\n","        df['label']=df['label'].astype('category')\n","        df['sub']=df['sub'].astype('category') # this is before convert to int\n","\n","        root_fname = (item.split('/')[1].split('.')[0]) # between / and .\n","        ir1_df_dict[root_fname]=df # key is root name in the file\n","    return ir1_df_dict\n","if interactive:\n","    ir1_dict = get_twristar_ir1_dict()\n","    print(ir1_dict.keys())"],"metadata":{"id":"p_EmZhbavtzi","executionInfo":{"status":"ok","timestamp":1677623426373,"user_tz":360,"elapsed":10,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["# The dataset specific code to generate the dictionary of IR1 dataframes is complete.  Now use Shared Transforms to generate the final output arrays."],"metadata":{"id":"k4RnDlHwnYzz"}},{"cell_type":"code","metadata":{"id":"trfLorthy59i","executionInfo":{"status":"ok","timestamp":1677623426373,"user_tz":360,"elapsed":9,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def twristar_load_dataset(\n","    incl_val_group = False, # split train into train and validate\n","    split_subj = dict\n","                (train_subj = [1,2],\n","                validation_subj = [],\n","                test_subj = [3]),\n","    keep_channel_list = ['accel_ttl'],\n","    one_hot_encode = True, # make y into multi-column one-hot, one for each activity\n","    return_info_dict = False, # return dict of meta info along with ndarrays\n","    suppress_warn = False # special case for stratified warning\n","    ):\n","    \"\"\"Downloads the TWristAR dataset from Zenodo, processes the data, and\n","    returns arrays by separating into _train, _validate, and _test arrays for\n","    X and y based on split_sub dictionary.\"\"\"\n","    log_info = \"Generated by TWristAR_load_data.ipynb\\n\"\n","    today = date.today()\n","    log_info += today.strftime(\"%B %d, %Y\") + \"\\n\"\n","    log_info += \"sub dict = \" + str(split_subj) + \"\\n\"\n","    ir1_dict = get_twristar_ir1_dict()\n","    X, y, sub, ss_times, xys_info = xform.get_ir3_from_dict(ir1_dict, label_map = label_map_twristar, num_channels = 7) \n","    # Drop unwanted channels from X\n","    # TODO:  Channel list should be automatically pulled from info dict.\n","    all_channel_list = ['accel_x', 'accel_y', 'accel_z','accel_ttl','bvp','eda','p_temp']\n","    log_info += \"Keeping channels\" + str(keep_channel_list) + \"\\n\"\n","    X = xform.limit_channel_ir3(X, all_channel_list = all_channel_list, keep_channel_list = keep_channel_list)\n","    # write initial array info to log_info\n","    log_info += \"Initial Arrays\\n\"\n","    log_info += utils.tabulate_numpy_arrays({'X':X,'y':y,'sub':sub,'ss_times':ss_times})+'\\n'\n","    \n","    #One-Hot-Encode y...there must be a better way when starting with strings\n","    #https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n","    # Need to look at https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n","\n","    if (one_hot_encode):\n","        # integer encode\n","        y_vector = np.ravel(y) #encoder won't take column vector\n","        le = LabelEncoder()\n","        integer_encoded = le.fit_transform(y_vector) #convert from string to int\n","        name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n","        if (verbose):\n","            print(\"One-hot-encoding: category names -> int -> one-hot \\n\")\n","            print(name_mapping) # seems risky as interim step before one-hot\n","        log_info += \"One Hot:\" + str(name_mapping) +\"\\n\\n\"\n","        onehot_encoder = OneHotEncoder(sparse=False)\n","        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n","        onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n","        y=onehot_encoded.astype('uint8')\n","        #return X,y\n","    # split by subject number pass in dictionary\n","    sub_num = np.ravel(sub[ : , 0] ) # convert shape to (1047,)\n","    # this code is different from typical due to limited subjects,\n","    # all not test subjects data is placed into train which is then \n","    # split using stratification - validation group is not sub independent\n","    train_index = np.nonzero(np.isin(sub_num, split_subj['train_subj'] + \n","                                        split_subj['validation_subj']))\n","    x_train = X[train_index]\n","    y_train = y[train_index]\n","    if (incl_val_group):\n","        if not suppress_warn:\n","            print(\"Warning: Due to limited subjects the validation group is a stratified\")\n","            print(\"90/10 split of the training group.  It is not subject independent.\")\n","        # split training into training + validate using stratify - note that the\n","        # validation set is not subject independent (hard to achieve with limited\n","        # subjects).   The test set however is subject independent and as a result\n","        # will have much lower accuracy.  Another option is to tag a few of the\n","        # activities for inclusion in validation.  See\n","        # https://github.com/imics-lab/Semi-Supervised-HAR-e4-Wristband\n","        # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n","        x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=0.10, random_state=42, stratify=y_train)\n","\n","    test_index = np.nonzero(np.isin(sub_num, split_subj['test_subj']))\n","    x_test = X[test_index]\n","    y_test = y[test_index]\n","\n","    headers = (\"Returned Array\",\"shape\", \"object type\", \"data type\")\n","    mydata = [(\"x_train:\", x_train.shape, type(x_train), x_train.dtype),\n","                    (\"y_train:\", y_train.shape ,type(y_train), y_train.dtype)]\n","    if (incl_val_group):\n","        mydata += [(\"x_validation:\", x_validation.shape, type(x_validation), x_validation.dtype),\n","                        (\"y_validation:\", y_validation.shape ,type(y_validation), y_validation.dtype)]\n","    mydata += [(\"x_test:\", x_test.shape, type(x_test), x_test.dtype),\n","                    (\"y_test:\", y_test.shape ,type(y_test), y_test.dtype)]\n","    if (verbose):\n","        print(tabulate(mydata, headers=headers))\n","    log_info += tabulate(mydata, headers=headers)\n","    if (incl_val_group):\n","        if (return_info_dict):\n","            return x_train, y_train, x_validation, y_validation, x_test, y_test, log_info\n","        else:\n","            return x_train, y_train, x_validation, y_validation, x_test, y_test\n","    else:\n","        if (return_info_dict):\n","            return x_train, y_train, x_test, y_test, log_info\n","        else:\n","            return x_train, y_train, x_test, y_test\n"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# Main is setup to be a demo and bit of unit test."],"metadata":{"id":"tncwIiZaB3j3"}},{"cell_type":"code","metadata":{"id":"MaT1dfqavvtk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677623442305,"user_tz":360,"elapsed":15941,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}},"outputId":"56ae6e89-290b-480e-f3f5-9baa8470ef6e"},"source":["if __name__ == \"__main__\":\n","    verbose = False\n","    print(\"Get TWristAR using defaults - simple and easy!\")\n","    x_train, y_train, x_test, y_test \\\n","                             = twristar_load_dataset()\n","    print(utils.tabulate_numpy_arrays({'x_train': x_train, 'y_train': y_train,\n","                                   'x_test': x_test, 'y_test': y_test}))\n","    print ('\\n','-'*72)\n","\n","    print(\"Get TWristAR with validation group, info file, and four channels\\n\")\n","    x_train, y_train, x_valid, y_valid, x_test, y_test, log_info \\\n","                             = twristar_load_dataset(\n","                                 incl_val_group = True,\n","                                 keep_channel_list = ['accel_ttl','bvp',\n","                                                      'eda', 'p_temp'],\n","                                 return_info_dict = True)\n","    print(utils.tabulate_numpy_arrays({'x_train': x_train, 'y_train': y_train,\n","                                       'x_valid': x_valid, 'y_valid': y_valid,\n","                                   'x_test': x_test, 'y_test': y_test}))\n","\n","    print(\"\\n----------- Contents of returned log_info ---------------\")\n","    print(log_info)\n","    print(\"\\n------------- End of returned log_info -----------------\")\n","    print(\"Get TWristAR with validation group, no warn, and bvp only\\n\")\n","    x_train, y_train, x_valid, y_valid, x_test, y_test \\\n","                             = twristar_load_dataset(\n","                                 incl_val_group = True,\n","                                 keep_channel_list = ['bvp'],\n","                                 return_info_dict = False,\n","                                 suppress_warn = True)\n","    print(\"This is a no output config - silent execution\")\n","    print(utils.tabulate_numpy_arrays({'x_train': x_train, 'y_train': y_train,\n","                                       'x_valid': x_valid, 'y_valid': y_valid,\n","                                   'x_test': x_test, 'y_test': y_test}))\n","    print ('\\n','-'*72)\n","    print(\"Get TWristAR with validation group, and accel only\\n\")\n","    x_train, y_train, x_valid, y_valid, x_test, y_test, log_accelxyz\\\n","                             = twristar_load_dataset(\n","                                 incl_val_group = True,\n","                                 keep_channel_list = ['accel_x', 'accel_y', 'accel_z', 'accel_ttl'],\n","                                 return_info_dict = True,\n","                                 suppress_warn = True)\n","    print(utils.tabulate_numpy_arrays({'x_train': x_train, 'y_train': y_train,\n","                                       'x_valid': x_valid, 'y_valid': y_valid,\n","                                   'x_test': x_test, 'y_test': y_test}))\n","    print(\"\\n----------- Contents of returned log_info ---------------\")\n","    print(log_accelxyz)\n","    print(\"\\n------------- End of returned log_info -----------------\")"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Get TWristAR using defaults - simple and easy!\n","Downloading TWristAR from Zenodo\n","Unzipping TWristAR file in . directory\n","array    shape          data type\n","-------  -------------  -----------\n","x_train  (2077, 96, 1)  float32\n","y_train  (2077, 6)      uint8\n","x_test   (1091, 96, 1)  float32\n","y_test   (1091, 6)      uint8\n","\n"," ------------------------------------------------------------------------\n","Get TWristAR with validation group, info file, and four channels\n","\n","Warning: Due to limited subjects the validation group is a stratified\n","90/10 split of the training group.  It is not subject independent.\n","array    shape          data type\n","-------  -------------  -----------\n","x_train  (1869, 96, 4)  float32\n","y_train  (1869, 6)      uint8\n","x_valid  (208, 96, 4)   float32\n","y_valid  (208, 6)       uint8\n","x_test   (1091, 96, 4)  float32\n","y_test   (1091, 6)      uint8\n","\n","----------- Contents of returned log_info ---------------\n","Generated by TWristAR_load_data.ipynb\n","February 28, 2023\n","sub dict = {'train_subj': [1, 2], 'validation_subj': [], 'test_subj': [3]}\n","Keeping channels['accel_ttl', 'bvp', 'eda', 'p_temp']\n","Initial Arrays\n","array     shape          data type\n","--------  -------------  --------------\n","X         (3168, 96, 4)  float32\n","y         (3168, 1)      int8\n","sub       (3168, 1)      int16\n","ss_times  (3168, 2)      datetime64[ns]\n","One Hot:{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}\n","\n","Returned Array    shape          object type              data type\n","----------------  -------------  -----------------------  -----------\n","x_train:          (1869, 96, 4)  <class 'numpy.ndarray'>  float32\n","y_train:          (1869, 6)      <class 'numpy.ndarray'>  uint8\n","x_validation:     (208, 96, 4)   <class 'numpy.ndarray'>  float32\n","y_validation:     (208, 6)       <class 'numpy.ndarray'>  uint8\n","x_test:           (1091, 96, 4)  <class 'numpy.ndarray'>  float32\n","y_test:           (1091, 6)      <class 'numpy.ndarray'>  uint8\n","\n","------------- End of returned log_info -----------------\n","Get TWristAR with validation group, no warn, and bvp only\n","\n","This is a no output config - silent execution\n","array    shape          data type\n","-------  -------------  -----------\n","x_train  (1869, 96, 1)  float32\n","y_train  (1869, 6)      uint8\n","x_valid  (208, 96, 1)   float32\n","y_valid  (208, 6)       uint8\n","x_test   (1091, 96, 1)  float32\n","y_test   (1091, 6)      uint8\n","\n"," ------------------------------------------------------------------------\n","Get TWristAR with validation group, and accel only\n","\n","array    shape          data type\n","-------  -------------  -----------\n","x_train  (1869, 96, 4)  float32\n","y_train  (1869, 6)      uint8\n","x_valid  (208, 96, 4)   float32\n","y_valid  (208, 6)       uint8\n","x_test   (1091, 96, 4)  float32\n","y_test   (1091, 6)      uint8\n","\n","----------- Contents of returned log_info ---------------\n","Generated by TWristAR_load_data.ipynb\n","February 28, 2023\n","sub dict = {'train_subj': [1, 2], 'validation_subj': [], 'test_subj': [3]}\n","Keeping channels['accel_x', 'accel_y', 'accel_z', 'accel_ttl']\n","Initial Arrays\n","array     shape          data type\n","--------  -------------  --------------\n","X         (3168, 96, 4)  float32\n","y         (3168, 1)      int8\n","sub       (3168, 1)      int16\n","ss_times  (3168, 2)      datetime64[ns]\n","One Hot:{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}\n","\n","Returned Array    shape          object type              data type\n","----------------  -------------  -----------------------  -----------\n","x_train:          (1869, 96, 4)  <class 'numpy.ndarray'>  float32\n","y_train:          (1869, 6)      <class 'numpy.ndarray'>  uint8\n","x_validation:     (208, 96, 4)   <class 'numpy.ndarray'>  float32\n","y_validation:     (208, 6)       <class 'numpy.ndarray'>  uint8\n","x_test:           (1091, 96, 4)  <class 'numpy.ndarray'>  float32\n","y_test:           (1091, 6)      <class 'numpy.ndarray'>  uint8\n","\n","------------- End of returned log_info -----------------\n"]}]}]}