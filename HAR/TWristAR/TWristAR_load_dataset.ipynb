{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1WbviRoNfMEZwPiA0Jm0FruV9l8tODu_e","timestamp":1656703965031},{"file_id":"1RkiXI3GhB-rNtyUp_VYw05xiiuD_oDFA","timestamp":1612028534003}],"authorship_tag":"ABX9TyN5VnB+o8ft63hFMycAsMCs"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Khc4g511HMYk"},"source":["#TWristAR_load_dataset.ipynb\n","Loads the raw e4 signals and .csv label files from the [Zenodo repository](https://zenodo.org/record/5911808) and returns train and test X/y numpy arrays.\n","\n","The basic flow is:\n","* Download and unzip the dataset if not already present\n","* Convert each recording *session* into Intermediate Representation 1 (IR1) format - a datetime indexed pandas dataframe with columns for each channel plus the label and subject number.\n","* Transform the IR1 into IR2 - a set of three numpy arrays containing sliding window samples\n","   * X = (samples, time steps per sample, channels)  \n","   * y =  (samples, label) # activity classification  \n","   * s =  (samples, subject) # subject number\n","* Clean and further transforms the IR2 arrays as needed - note the transforms that can be applied here are train vs test dependent.   For example, the IR2 arrays in the training set may be rebalanced, but those in the test set should not.\n","* Concatenate the processed IR2 arrays into the final returned train/validate/test arrays.\n","\n","TWRistAR is small and easily downloadable so there is no option to used saved Intermediate Representations here as there is in some of the loaders for larger datasets.\n","\n","Set interactive to true to run the Jupyter Notebook version.  Note most of the calls are setup to test the functions, not process the entire dataset, to do that set interactive to false and run all so that main executes.   This notebook can be saved and run as a python file as well.\n","\n","This video describes the code https://mediaflo.txstate.edu/Watch/e4_data_processing. (updates have been made since this was made)\n","\n","\n","Acknowledgement to and a good example of the WISDM format being pre-processed is https://towardsdatascience.com/human-activity-recognition-har-tutorial-with-keras-and-core-ml-part-1-8c05e365dfa0  by Nils Ackermann.  \n","\n","\n","<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.\n","\n","[Lee B. Hinkle](https://userweb.cs.txstate.edu/~lbh31/), Texas State University, [IMICS Lab](https://imics.wp.txstate.edu/)  \n","TODO:\n","* The IR3 transform could be further optimized by taking a dictionary of IR1 dataframes as was done in the later datasets.\n","* log_info needs to be updated to dictionary format so we can read things like the channel names automatically.\n","* Time is off by 6 hrs due to time zone issues - adjusted in Excel/csv but would be good to show it in the correct time zone.\n","* Need to incorporate session numbers or just use the alternate .csv files where validation was 'fake' subs 11 and 22 which were just a few of the sessions from subjects 1 and 2.  This was done in the Semi-Supervised version of the loader for WISHWell but not integrated back into this version.\n"]},{"cell_type":"markdown","source":["# Import Libraries and Common Load Dataset Code (from IMICS public repo)"],"metadata":{"id":"ghyvAqipvbvs"}},{"cell_type":"code","metadata":{"id":"q6H67o-YARCx","executionInfo":{"status":"ok","timestamp":1677620937498,"user_tz":360,"elapsed":13716,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["import os\n","import shutil #https://docs.python.org/3/library/shutil.html\n","from shutil import unpack_archive # to unzip\n","import time\n","import pandas as pd\n","import numpy as np\n","from numpy import savetxt\n","from tabulate import tabulate # for verbose tables, showing data\n","from tensorflow.keras.utils import to_categorical # for one-hot encoding\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import OneHotEncoder\n","from time import gmtime, strftime, localtime #for displaying Linux UTC timestamps in hh:mm:ss\n","from datetime import datetime, date\n","import urllib.request # to get files from web w/o !wget"],"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":787,"status":"ok","timestamp":1677620940544,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"},"user_tz":360},"outputId":"18ea3169-95d0-45d1-b9c0-9de160f9e15f","id":"gajdw42Dt_yO"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading load_data_utils.py from IMICS git repo\n","Downloading load_data_transforms.py from IMICS git repo\n"]}],"source":["def get_py_file(fname, url):\n","    \"\"\"checks for local file, if none downloads from URL.    \n","    :return: nothing\"\"\"\n","    #fname = 'load_data_utils.py'\n","    #ffname = os.path.join(my_dir,fname)\n","    if (os.path.exists(fname)):\n","        print (\"Local\",fname, \"found, skipping download\")\n","    else:\n","        print(\"Downloading\",fname, \"from IMICS git repo\")\n","        urllib.request.urlretrieve(url, filename=fname)\n","\n","get_py_file(fname = 'load_data_utils.py', url = 'https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/load_data_utils.py')\n","get_py_file(fname = 'load_data_transforms.py', url = 'https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/load_data_transforms.py')"]},{"cell_type":"code","source":["# common transforms and utils used by the individual loaders\n","import load_data_transforms as xform\n","import load_data_utils as utils"],"metadata":{"id":"_Ny0oT3ecvZP","executionInfo":{"status":"ok","timestamp":1677620943567,"user_tz":360,"elapsed":148,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Global Parameters"],"metadata":{"id":"iDsgBVo_BFkc"}},{"cell_type":"code","metadata":{"id":"1LkvTO5hujPH","executionInfo":{"status":"ok","timestamp":1677620946560,"user_tz":360,"elapsed":3,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["# environment and execution parameters\n","my_dir = '.' # replace with absolute path if desired\n","dataset_dir = my_dir # TWristAR zip file contains TWristAR directory\n","working_dir = os.path.join(my_dir,'TWristAR_temp') # temp dir for processing\n","\n","if not os.path.exists(working_dir):\n","    os.makedirs(working_dir)\n","interactive = True # for exploring data and functions interactively\n","verbose = True\n","\n","# dataset parameters\n","# frequency = 32 - note this is hardcoded due to the unique sample freqencies\n","# that differ between the individual e4 sensors\n","xform.time_steps = 96 # three seconds at 32Hz\n","xform.stride = 32 # one second step for each sliding window\n","# The label_map_<dataset> contains a mapping from strings to ints for all\n","# possible labels in the entire dataset.   This allows for predictable conversion\n","# regardless of the slices.  I'm using 99 for 'unknown' which will be dropped\n","# to avoid the confusion of shifing by 1 place, zero indexed etc.\n","label_map_twristar = {\"label\":     {\"Downstairs\": 0, \"Jogging\": 1, \"Sitting\": 2,\n","                                \"Standing\": 3, \"Upstairs\": 4, \"Walking\": 5,\n","                                \"Undefined\": 99}}"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"_arxQU-n6nKK","executionInfo":{"status":"ok","timestamp":1677620951241,"user_tz":360,"elapsed":143,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["interactive = False # don't run if interactive, automatically runs for .py version\n","verbose = False # to limit the called functions output"],"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def get_TWristAR():\n","    \"\"\"checks for local zipfile, if none downloads from zenodo repository\n","    after download will unzip the dataset into TWristAR directory.\n","    Assumes a global my_dir has been defined (default is my_dir = \".\")\n","    :return: nothing\"\"\"\n","    zip_ffname = os.path.join(my_dir,'TWristAR.zip')\n","    if (os.path.exists(zip_ffname)):\n","        if verbose:\n","            print (\"Local TWristAR.zip found, skipping download\")\n","    else:\n","        print(\"Downloading TWristAR from Zenodo\")\n","        urllib.request.urlretrieve(\"https://zenodo.org/record/5911808/files/TWristAR.zip\", filename=\"TWristAR.zip\")\n","    if (os.path.isdir(os.path.join(dataset_dir,'TWristAR'))):\n","        if verbose:\n","            print(\"Found existing TWristAR directory, skipping unzip\")\n","        return\n","    else:\n","        print(\"Unzipping TWristAR file in\", dataset_dir, \"directory\")\n","        if (os.path.exists(zip_ffname)):\n","            shutil.unpack_archive(zip_ffname,dataset_dir,'zip')\n","        else:\n","            print(\"Error: \", zip_ffname, \" not found, exiting\")\n","            return\n","if interactive:\n","    get_TWristAR()"],"metadata":{"id":"tlKEXl-BSrLO","executionInfo":{"status":"ok","timestamp":1677620954011,"user_tz":360,"elapsed":146,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"9oab3XMPgL8Z","executionInfo":{"status":"ok","timestamp":1677620957185,"user_tz":360,"elapsed":119,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def unzip_e4_file(zip_ffname):\n","    \"\"\"checks for local copy, if none unzips the e4 zipfile in working_dir\n","    Note:  the files themselves do not contain subject info and there are\n","    multiple files e.g. ACC.csv, BVP,csv etc, in each zipfile.\n","    It is very important to further process the files with <fname>_labels.csv\n","    :param zip_ffname: the path and filename of the zip file\n","    :param working_dir: local (colab) directory where csv files will be placed\n","    :return: nothing\"\"\"\n","    if not os.path.exists(working_dir):\n","        print(\"Error working directory\", working_dir, \"not found, unzip_e4_file exiting\")\n","        return\n","    if (os.path.exists(zip_ffname)):\n","        if verbose:\n","            print(\"Unzipping\",zip_ffname, \"in\", working_dir)\n","        shutil.unpack_archive(zip_ffname,working_dir,'zip')\n","    else:\n","        print(\"Error: \", zip_ffname, \" not found, exiting\")\n","        return\n","if interactive:\n","    zip_ffname = os.path.join(my_dir,'TWristAR','sub1/1574621345_A01F11.zip')\n","    unzip_e4_file(zip_ffname)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"65c2wpiOwVmg","executionInfo":{"status":"ok","timestamp":1677620960307,"user_tz":360,"elapsed":177,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def df_from_e4_csv (ffname,col_labels):\n","    \"\"\"\"reads e4 ACC, BVP, EDA, or TEMP(erature) csv files, uses start time and\n","    sample rate to create time indexed pandas dataframe with columns.  \n","    Note the other e4 files have different format and must be read seperately. \n","    :param ffname:  full filename e.g./content/temp/ACC.csv\n","    :col_labels: list of colums in csv - varies by type ['accel_x','accel_y...]\n","    :returns df: time indexed dataframe\"\"\"\n","\n","    df = pd.read_csv(ffname, header=None)\n","    start_time = df.iloc[0,0].astype('int64') # first line in e4 csv\n","    sample_freq = df.iloc[1,0].astype('int64') # second line in e4 csv\n","    df = df.drop(df.index[[0,1]]) # drop 1st two rows, index is now off by 2\n","    # Convert the index to datetime to allow for pandas resampling\n","    # The start_time from the e4 csv file is forced to int64 which represents the\n","    # number of nanoseconds since January 1, 1970, 00:00:00 (UTC)\n","    # This is tricky - if float representation the join function may not work\n","    # properly later since the indexes must match exactly.\n","    # UTC_time is computed for each row, then made into required datetime format\n","    # which is a int64 that pandas will accept as an index\n","    df['UTC_time'] = (df.index-2)/sample_freq + start_time\n","    end_time = df['UTC_time'].iloc[-1]\n","    if verbose:\n","        print(ffname, \"Sample frequency = \", sample_freq, \" Hz\")\n","        #show time in day month format, assumes same timezone\n","        print(\"File start time = \", strftime(\"%a, %d %b %Y %H:%M:%S\", localtime(start_time)))  \n","        print(\"File end time   = \",strftime(\"%a, %d %b %Y %H:%M:%S\", localtime(end_time)))\n","    df['datetime'] = pd.to_datetime(df['UTC_time'], unit='s')\n","    df.set_index('datetime',inplace=True)\n","    df = df.drop('UTC_time', axis=1)\n","    df.columns = col_labels\n","    return df\n","if interactive:\n","    # Note: IBI.csv is the inter-beat interval, a calculated value with a \n","    # different format.  HR.csv is also calculated from BVP but format is same.\n","    ffname = working_dir + '/ACC.csv'\n","    col_labels = ['accel_x', 'accel_y', 'accel_z']\n","    ir1_acc_df = df_from_e4_csv(ffname, col_labels)\n","    print(\"ACC dataframe shape\", ir1_acc_df.shape)\n","    display(ir1_acc_df.head())\n","\n","    ffname = working_dir + '/BVP.csv'\n","    col_labels = ['bvp']\n","    ir1_bvp_df = df_from_e4_csv(ffname, col_labels)\n","    print(\"BVP dataframe shape\", ir1_bvp_df.shape)\n","    display(ir1_bvp_df.head())\n","\n","    ffname = working_dir + '/EDA.csv'\n","    col_labels = ['eda']\n","    ir1_eda_df = df_from_e4_csv(ffname, col_labels)\n","    print(\"EDA dataframe shape\", ir1_eda_df.shape)\n","    display(ir1_eda_df.head())\n","\n","    ffname = working_dir + '/TEMP.csv'\n","    col_labels = ['p_temp']\n","    ir1_temp_df = df_from_e4_csv(ffname, col_labels)\n","    print(\"Temp dataframe shape\", ir1_temp_df.shape)\n","    display(ir1_temp_df.head())"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ozk2mZVYB-Q","executionInfo":{"status":"ok","timestamp":1677620963644,"user_tz":360,"elapsed":122,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def process_e4_accel(df):\n","    \"\"\"converts component accel into g and adds accel_ttl column\n","    per info.txt range is [-2g, 2g] and unit in this file is 1/64g.\n","    This method is e4 specific due to the way the accelerations is recorded\n","    \"\"\"\n","    df['accel_x'] = df['accel_x']/64\n","    df['accel_y'] = df['accel_y']/64\n","    df['accel_z'] = df['accel_z']/64\n","    df_sqd = df.pow(2)[['accel_x', 'accel_y', 'accel_z']] #square each accel\n","    df_sum = df_sqd.sum(axis=1) #add sum of squares, new 1 col df\n","    df.loc[:,'accel_ttl'] = df_sum.pow(0.5)-1  # sqrt and remove 1g due to gravity\n","    del df_sqd, df_sum\n","    return df\n","if interactive:\n","    ir1_acc_df = process_e4_accel(ir1_acc_df)\n","    display(ir1_acc_df.head())"],"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def get_ir1_from_e4_dir():\n","    \"\"\"processes the four e4 sensor files in global working directory into a \n","    single IR1 datetime indexed dataframe. Labeled columns are channels\"\"\"\n","    # Note: IBI.csv is the inter-beat interval, a calculated value with a \n","    # different format.  HR.csv is also calculated from BVP but format is same.\n","    # TODO:  Should check directory for all four files and uniform start/stop\n","    # times.\n","    # TODO: Might be better to use a different interpolation.  See\n","    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html\n","    ffname = working_dir + '/ACC.csv'\n","    col_labels = ['accel_x', 'accel_y', 'accel_z']\n","    ir1_acc_df = df_from_e4_csv(ffname, col_labels)\n","    ir1_acc_df = process_e4_accel(ir1_acc_df)\n","\n","    ffname = working_dir + '/BVP.csv'\n","    col_labels = ['bvp']\n","    ir1_bvp_df = df_from_e4_csv(ffname, col_labels)\n","\n","    ffname = working_dir + '/EDA.csv'\n","    col_labels = ['eda']\n","    ir1_eda_df = df_from_e4_csv(ffname, col_labels)\n","\n","    ffname = working_dir + '/TEMP.csv'\n","    col_labels = ['p_temp']\n","    ir1_ptemp_df = df_from_e4_csv(ffname, col_labels)\n","\n","    ir1_df = ir1_acc_df.join(ir1_bvp_df, how=\"inner\") # this drops bvp to 32Hz\n","    ir1_df = ir1_df.join(ir1_eda_df, how=\"outer\") # stays at 32Hz, eda fill NaN\n","    ir1_df = ir1_df.join(ir1_ptemp_df, how=\"outer\") # stays at 32Hz, p_temp fill NaN\n","    ir1_df = ir1_df.interpolate() # default is linear interpolation\n","    ir1_df = ir1_df.astype('float32') # no need for 64 precision with these sensors\n","    if verbose:\n","        print(\"IR1 full dataframe shape\",ir1_df.shape)\n","        #print(ir1_df.head(10))\n","    return ir1_df\n","if interactive:\n","    ir1_df = get_ir1_from_e4_dir()\n","    display(ir1_df.head(10))\n","    ir1_df.iloc[499:1999].plot(subplots=True, figsize=(20, 10)) # plot a few seconds"],"metadata":{"id":"Ax17Aew_zTUu","executionInfo":{"status":"ok","timestamp":1677620967340,"user_tz":360,"elapsed":130,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"YZNmr-yPWcck","executionInfo":{"status":"ok","timestamp":1677620971614,"user_tz":360,"elapsed":655,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def show_e4_tag_time(tag_ffname):\n","    \"\"\"utility prints time marks from e4 tags.csv to help with video sync \n","    and labeling.   When this is run in colab it seems to be GMT regardless\n","    of timezone settings.\"\n","    :param tag_ffname: file to be processed e.g. /content/temp/tags.csv'\n","    :return: nothing\"\"\"\n","    df_temp = pd.read_csv(tag_ffname, header=None)\n","    df_temp.columns = ['UTC_time']\n","    print (\"    UTC_time          Local Time\")\n","    for index, row in df_temp.iterrows():\n","        print(index, row['UTC_time'],\n","            strftime(\"%a, %d %b %Y %H:%M:%S\", localtime(row['UTC_time'])))\n","# https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior\n","# link to string formats for date and time\n","if interactive:\n","    print('Tag info (button presses) from tags.csv')\n","    tag_ffname = working_dir + '/tags.csv'\n","    show_e4_tag_time(tag_ffname)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"ttKS9Ox6JSed","executionInfo":{"status":"ok","timestamp":1677620976343,"user_tz":360,"elapsed":130,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def label_df_from_csv (df, labels_ffname):\n","    \"\"\"adds class label and subject number to the dataframe based on the\n","    contents of a .csv file containing time and label info.\n","    Example csv format (see e4_time_sync.xlsx to help build csv from video)\n","        start,finish,label,sub\n","        2019:11:24 18:49:51,2019:11:24 18:50:18,Upstairs,1\n","        2019:11:24 18:50:18,2019:11:24 18:50:45,Downstairs,1\n","    :param df : time indexed dataframe from df_from_e4_csv method\n","    :labels_ffname : csv file with metadata\n","    :return : a dataframe with label and subject columns added\"\"\"\n","    df_labels = pd.read_csv(labels_ffname)\n","    df_labels['start'] =  pd.to_datetime(df_labels['start'], format='%Y:%m:%d %H:%M:%S')\n","    df_labels['finish'] =  pd.to_datetime(df_labels['finish'], format='%Y:%m:%d %H:%M:%S')\n","    # quick check to make sure all subjects are the same - only 1 sub per csv\n","    if (not (df_labels['sub'].eq(df_labels['sub'].iloc[0]).all())):\n","        print('Warning: Multiple subjects detected in csv, unusual for e4 data.')\n","    df['label']='Undefined' # add column with safe value for labels\n","    df['sub'] = np.NaN\n","    for index, row in df_labels.iterrows():\n","        #print(row['start'], row['finish'],row['label'])\n","        df.loc[row['start']:row['finish'],'label'] = row['label']\n","        df.loc[row['start']:row['finish'],'sub'] = row['sub']\n","    return df\n","if interactive:\n","    labels_ffname = os.path.splitext(zip_ffname)[0] + '_labels.csv'\n","    print(\"Adding label and sub info from \",labels_ffname)\n","    ir1_df = label_df_from_csv(ir1_df, labels_ffname)\n","    display(ir1_df[5000:5005]) # head is meaningless since start is undefined\n","    #ir1_df['label'].value_counts()\n","    print (\"Label Counts - # samples before sliding window\")\n","    print (ir1_df['label'].value_counts())"],"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def get_twristar_ir1_dict():\n","    \"\"\"reads the TWRistAR dataset and converts each \"session file\" to an IR1\n","    dataframe.\n","    Returns: a dict containing key = df_name and item = IR1 dataframes.\"\"\"\n","    # A few notes - TWRristAR (or more specifically e4 wristband datafiles)\n","    # require a lot of processing, if trying to leverage for more traditional\n","    # .csv file format see Gesture Phase version.\n","    fn_list = ['sub1/1574621345_A01F11.zip',\n","                'sub1/1574622389_A01F11.zip',\n","                'sub1/1574624998_A01F11.zip',\n","                'sub2/1633107019_A01F11.zip',\n","                'sub2/1633108344_A01F11.zip',\n","                'sub2/1633109744_A01F11.zip',\n","                'sub3/1633704587_A01F11.zip',\n","                'sub3/1633705664_A01F11.zip',\n","                'sub3/1633711821_A01F11.zip']\n","    get_TWristAR()\n","    ir1_df_dict = dict() # an empty dictionary\n","    for item in fn_list:\n","        zip_ffname = os.path.join(my_dir,'TWristAR',item)\n","        if verbose:\n","            print('Processing ', zip_ffname)\n","        if not os.path.exists(working_dir):\n","            os.makedirs(working_dir)\n","        unzip_e4_file(zip_ffname)\n","        df = get_ir1_from_e4_dir()\n","        if verbose:\n","            print('Tag info (button presses) from tags.csv')\n","            tag_ffname = working_dir + '/tags.csv'\n","            show_e4_tag_time(tag_ffname)\n","        # Generate associated csv filename, forces the long numbered filenames to match\n","        labels_ffname = os.path.splitext(zip_ffname)[0] + '_labels.csv'\n","        df = label_df_from_csv (df, labels_ffname)\n","        df['label'].value_counts()\n","        if verbose:\n","            print (\"Label Counts - # samples before sliding window\\n\",ir1_df['label'].value_counts())\n","        # tighten up the column types for space savings.\n","        # change to 32-bit, credit/ref https://stackoverflow.com/questions/69188132/how-to-convert-all-float64-columns-to-float32-in-pandas\n","        # Select columns with 'float64' dtype  \n","        float64_cols = list(df.select_dtypes(include='float64'))\n","        # The same code again calling the columns\n","        df[float64_cols] = df[float64_cols].astype('float32')\n","        # Seems better to explicitly type the other columns vs object.\n","        df['label']=df['label'].astype('category')\n","        df['sub']=df['sub'].astype('category') # this is before convert to int\n","\n","        root_fname = (item.split('/')[1].split('.')[0]) # between / and .\n","        ir1_df_dict[root_fname]=df # key is root name in the file\n","    return ir1_df_dict\n","if interactive:\n","    ir1_dict = get_twristar_ir1_dict()\n","    print(ir1_dict.keys())"],"metadata":{"id":"p_EmZhbavtzi","executionInfo":{"status":"ok","timestamp":1677620989015,"user_tz":360,"elapsed":118,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["# Use Shared Transforms for IR1 to Final Array Output"],"metadata":{"id":"k4RnDlHwnYzz"}},{"cell_type":"code","source":["ir1_dict = get_twristar_ir1_dict()\n","X, y, sub, ss_times, xys_info = xform.get_ir3_from_dict(ir1_dict, label_map = label_map_twristar, num_channels = 7)\n","print(utils.tabulate_numpy_arrays({'X':X,'y':y,'sub':sub,'ss_times':ss_times}))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nuGBqccd-PXw","executionInfo":{"status":"ok","timestamp":1677621225626,"user_tz":360,"elapsed":1282,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}},"outputId":"bb98c0e5-3329-49c7-ee89-2d805564307c"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["array     shape          data type\n","--------  -------------  --------------\n","X         (3168, 96, 7)  float32\n","y         (3168, 1)      int8\n","sub       (3168, 1)      int16\n","ss_times  (3168, 2)      datetime64[ns]\n"]}]},{"cell_type":"code","metadata":{"id":"Q38LmSybsG3r","executionInfo":{"status":"ok","timestamp":1677620463378,"user_tz":360,"elapsed":148,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def get_ir3_from_dict(ir1_dict, label_map, num_channels):\n","    \"\"\"Processes a dictionary and combines the IR1 dataframes into a single\n","    IR3 set of numpy arrays.  Converts string labels to integers based on the\n","    passed label map.\n","    Params:\n","    ir1_dict: dict of IR1 dataframes key = IR1 source filename, item = IR1 df\n","    label_map: dict of labels (one entry per label column, most datasets will\n","         have only one with key = 'label'.  The item is a dict with keys of \n","         all possible strings and item = corresponding int.)\n","    num_channels: the number of data channels (# df columns - #labels - 1 for sub)\n","    Returns:\n","    X - ndarray (float32) of all channels\n","    y - ndarray (int8) of labels, for multi-label datasets # labels = # columns\n","    sub - ndarray (int16) subject number, int16 allows for sub nums > 255\n","    ss_times - ndarray (datetime64), start and stop time for sliding window\n","    xys_info - string, basically an autogenerated readme (needs work)\n","    \"\"\"\n","    # TODO - seems like the number of channels could be calculated automatically\n","    ir3_X = np.zeros(shape=(1,xform.time_steps,num_channels), dtype = 'float32')\n","    ir3_y = np.zeros(shape=(1,1),dtype='int8') # newer int method\n","    #ir3_y = np.full(shape=(1,1), fill_value='n/a',dtype='<U10') # unicode 10 char\n","    ir3_sub = np.zeros(shape=(1,1),dtype='int16') # one subject number per entry\n","    ir3_ss_times = np.zeros(shape=(1,2),dtype='datetime64') # start/stop times of sliding window\n","    for ir1_fname, ir1_df in ir1_dict.items():\n","        if verbose:\n","            print('Processing ', ir1_fname)\n","        ir1_df = xform.assign_ints_ir1_labels(ir1_df, label_mapping_dict = label_map_twristar)\n","        ir2_X, ir2_y, ir2_sub, ir2_ss_time, channel_list = xform.get_ir2_from_ir1(ir1_df)\n","        ir2_X, ir2_y, ir2_sub, ir2_ss_time = xform.clean_ir2(ir2_X, ir2_y, ir2_sub, ir2_ss_time)\n","        ir2_X, ir2_y, ir2_sub, ir2_ss_time = xform.drop_label_ir2_ir3(ir2_X, ir2_y, ir2_sub, ir2_ss_time, 99)\n","        ir3_X = np.vstack([ir3_X, ir2_X])\n","        ir3_y = np.vstack([ir3_y, ir2_y])\n","        ir3_sub = np.vstack([ir3_sub, ir2_sub])\n","        ir3_ss_times = np.vstack([ir3_ss_times, ir2_ss_time])\n","    #delete first row placeholders\n","    X = np.delete(ir3_X, (0), axis=0) \n","    y = np.delete(ir3_y, (0), axis=0) \n","    sub = np.delete(ir3_sub, (0), axis=0)\n","    sub = np.delete(ir3_sub, (0), axis=0)\n","    ss_times = np.delete(ir3_ss_times, (0), axis=0)\n","\n","    xys_info = 'Needs work!\\n'\n","    # xys_info += '\\n'.join([str(elem) for elem in zip_flist]) # conv list to string\n","    # xys_info += '\\nTime steps =' + str(time_steps) + ', Step =' + str(stride) + ', no resample\\n'\n","    # xys_info += 'Final Shapes\\n'\n","    # xys_info += \"X shape \" + str(X.shape) + \" dtype = \" + str(X.dtype) + \"\\n\"\n","    # xys_info += \"y shape \" + str(y.shape) + \" dtype = \" + str(y.dtype) + \"\\n\"\n","    # xys_info += \"sub shape \" + str(sub.shape) + \" dtype = \" + str(sub.dtype) + \"\\n\"\n","    xys_info += \"IR1 Channel names:\" + str(channel_list) + \"\\n\"\n","    # # Get final counts for label ndarray - not quite as easy as pandas df\n","    # xys_info += \"Final Label Counts\\n\"\n","    # unique, counts = np.unique(y, return_counts=True)\n","    # xys_info += str(np.asarray((unique, counts)).T)\n","    # xys_info += \"\\nSamples per Subject\\n\"\n","    # unique, counts = np.unique(sub, return_counts=True)\n","    # xys_info += str(np.asarray((unique, counts)).T)\n","    return X, y, sub, ss_times, xys_info\n","if interactive:\n","    X, y, sub, ss_times, xys_info = get_ir3_from_dict(ir1_dict, label_map = label_map_twristar, num_channels = 7)\n","    headers = (\"array\",\"shape\", \"object type\", \"data type\")\n","    mydata = [(\"X:\", X.shape, type(X), X.dtype),\n","            (\"y:\", y.shape ,type(y), y.dtype),\n","            (\"sub:\", sub.shape, type(sub), sub.dtype),\n","            (\"ss_time:\", ss_times.shape, type(ss_times), ss_times.dtype)]\n","    print(tabulate(mydata, headers=headers))\n","    unique, counts = np.unique(y, return_counts=True)\n","    print('Label Counts:\\n',str(np.asarray((unique, counts)).T))\n","    print(label_map_twristar)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"rJp5v319Ybxa","executionInfo":{"status":"ok","timestamp":1677531753042,"user_tz":360,"elapsed":1324,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1b1c4d98-f556-4b9d-a4c9-d0596c531c47"},"source":["def get_ir3(\n","    working_dir = os.path.join(my_dir,'TWristAR_temp'), # dir will be created\n","    zip_flist = ['sub1/1574621345_A01F11.zip',\n","                'sub1/1574622389_A01F11.zip',\n","                'sub1/1574624998_A01F11.zip',\n","                'sub2/1633107019_A01F11.zip',\n","                'sub2/1633108344_A01F11.zip',\n","                'sub2/1633109744_A01F11.zip',\n","                'sub3/1633704587_A01F11.zip',\n","                'sub3/1633705664_A01F11.zip',\n","                'sub3/1633711821_A01F11.zip'],\n","    ):\n","    \"\"\"NOTE:  This methodology works for TWristAR because it requires little\n","    cleaning/rebalancing.  For other datasets a train/test aware version must\n","    be used since the processing of train and test IR2s may differ.\n","    This is the main function to create the three ndarrays X, y, sub from\n","    the dataset zip file.  It is an updated version of get_xys.  \n","    1) Downloads TWristAR from Zenodo repository,\n","    2) Processes each e4 zip file and associated label csv file into X (data),\n","     y (labels), and sub (subject number) IR2 ndarrays.\n","    3) Stackes the IR2 arrays into IR3 (X, y, sub) arrays\n","    Note:  TWristAR is purposely small, but for other datasets this can take\n","    a long time and it may be best to save these numpy arrays locally.\n","    \"\"\"\n","    get_TWristAR()\n","    # create blank ndarrays to append to\n","    ir3_X = np.zeros(shape=(1,xform.time_steps,7), dtype = 'float32')\n","    ir3_y = np.zeros(shape=(1,1),dtype='int8') # newer int method\n","    # ir3_y = np.full(shape=(1,1), fill_value='n/a',dtype='<U10') # unicode 10 char\n","    ir3_sub = np.zeros(shape=(1,1),dtype=int) # one subject number per entry\n","    for i in zip_flist:\n","        zip_ffname = os.path.join(my_dir,'TWristAR',i)\n","        if verbose:\n","            print('Processing ', zip_ffname)\n","        if not os.path.exists(working_dir):\n","            os.makedirs(working_dir)\n","        unzip_e4_file(zip_ffname)\n","        ir1_df = get_ir1_from_e4_dir()\n","        if verbose:\n","            print('Tag info (button presses) from tags.csv')\n","            tag_ffname = working_dir + '/tags.csv'\n","            show_e4_tag_time(tag_ffname)\n","        # Generate associated csv filename, forces the long numbered filenames to match\n","        labels_ffname = os.path.splitext(zip_ffname)[0] + '_labels.csv'\n","        ir1_df = label_df_from_csv (ir1_df, labels_ffname)\n","        ir1_df['label'].value_counts()\n","        if verbose:\n","            print (\"Label Counts - # samples before sliding window\\n\",ir1_df['label'].value_counts())\n","        ir1_df = xform.assign_ints_ir1_labels(ir1_df, label_mapping_dict = label_map_twristar)\n","        ir2_X, ir2_y, ir2_sub, ir2_ss_time, channel_list = xform.get_ir2_from_ir1(ir1_df)\n","        ir2_X, ir2_y, ir2_sub, ir2_ss_time = xform.clean_ir2(ir2_X, ir2_y, ir2_sub, ir2_ss_time)\n","        ir2_X, ir2_y, ir2_sub, ir2_ss_time = xform.drop_label_ir2_ir3(ir2_X, ir2_y, ir2_sub, ir2_ss_time, 99)\n","        # ir2_X, ir2_y, ir2_sub, channel_list = get_ir2_from_ir1(ir1_df)\n","        # ir2_X, ir2_y, ir2_sub = clean_ir2(ir2_X, ir2_y, ir2_sub)\n","        # ir2_X, ir2_y, ir2_sub = drop_label_ir2_ir3(ir2_X, ir2_y, ir2_sub, 'Undefined')\n","        ir3_X = np.vstack([ir3_X, ir2_X])\n","        ir3_y = np.vstack([ir3_y, ir2_y])\n","        ir3_sub = np.vstack([ir3_sub, ir2_sub])\n","        shutil.rmtree(working_dir)\n","\n","    #delete first row placeholders\n","    X = np.delete(ir3_X, (0), axis=0) \n","    y = np.delete(ir3_y, (0), axis=0) \n","    sub = np.delete(ir3_sub, (0), axis=0)\n","    sub = sub.astype(np.uint16) # convert from float to int\n","\n","    xys_info = 'TWristAR e4 wristband structured 6-activity zip files\\n'\n","    xys_info += '\\n'.join([str(elem) for elem in zip_flist]) # conv list to string\n","    xys_info += '\\nTime steps =' + str(xform.time_steps) + ', Step =' + str(xform.stride) + ', no resample\\n'\n","    xys_info += 'Final Shapes\\n'\n","    xys_info += \"X shape \" + str(X.shape) + \" dtype = \" + str(X.dtype) + \"\\n\"\n","    xys_info += \"y shape \" + str(y.shape) + \" dtype = \" + str(y.dtype) + \"\\n\"\n","    xys_info += \"sub shape \" + str(sub.shape) + \" dtype = \" + str(sub.dtype) + \"\\n\"\n","    xys_info += \"Channel names:\" + str(channel_list) + \"\\n\"\n","    # Get final counts for label ndarray - not quite as easy as pandas df\n","    xys_info += \"Final Label Counts\\n\"\n","    unique, counts = np.unique(y, return_counts=True)\n","    xys_info += str(np.asarray((unique, counts)).T)\n","    xys_info += \"\\nSamples per Subject\\n\"\n","    unique, counts = np.unique(sub, return_counts=True)\n","    xys_info += str(np.asarray((unique, counts)).T)\n","    return X, y, sub, xys_info\n","if interactive:\n","    X, y, sub, xys_info = get_ir3()\n","    headers = (\"array\",\"shape\", \"object type\", \"data type\")\n","    mydata = [(\"X:\", X.shape, type(X), X.dtype),\n","            (\"y:\", y.shape ,type(y), y.dtype),\n","            (\"sub:\", sub.shape, type(sub), sub.dtype)]\n","    print(tabulate(mydata, headers=headers))"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Local TWristAR.zip found, skipping download\n","Found existing TWristAR directory, skipping unzip\n","Processing  ./TWristAR/sub1/1574621345_A01F11.zip\n","Unzipping ./TWristAR/sub1/1574621345_A01F11.zip in ./TWristAR_temp\n","./TWristAR_temp/ACC.csv Sample frequency =  32  Hz\n","File start time =  Sun, 24 Nov 2019 18:49:05\n","File end time   =  Sun, 24 Nov 2019 18:58:11\n","./TWristAR_temp/BVP.csv Sample frequency =  64  Hz\n","File start time =  Sun, 24 Nov 2019 18:49:05\n","File end time   =  Sun, 24 Nov 2019 18:58:11\n","./TWristAR_temp/EDA.csv Sample frequency =  4  Hz\n","File start time =  Sun, 24 Nov 2019 18:49:05\n","File end time   =  Sun, 24 Nov 2019 18:58:10\n","./TWristAR_temp/TEMP.csv Sample frequency =  4  Hz\n","File start time =  Sun, 24 Nov 2019 18:49:05\n","File end time   =  Sun, 24 Nov 2019 18:58:10\n","IR1 full dataframe shape (17501, 7)\n","Tag info (button presses) from tags.csv\n","    UTC_time          Local Time\n","0 1574621375.17 Sun, 24 Nov 2019 18:49:35\n","1 1574621774.22 Sun, 24 Nov 2019 18:56:14\n","Label Counts - # samples before sliding window\n"," Upstairs      6208\n","Downstairs    5889\n","Undefined     5404\n","Name: label, dtype: int64\n","Processing  ./TWristAR/sub1/1574622389_A01F11.zip\n","Unzipping ./TWristAR/sub1/1574622389_A01F11.zip in ./TWristAR_temp\n","./TWristAR_temp/ACC.csv Sample frequency =  32  Hz\n","File start time =  Sun, 24 Nov 2019 19:06:29\n","File end time   =  Sun, 24 Nov 2019 19:15:03\n","./TWristAR_temp/BVP.csv Sample frequency =  64  Hz\n","File start time =  Sun, 24 Nov 2019 19:06:29\n","File end time   =  Sun, 24 Nov 2019 19:15:03\n","./TWristAR_temp/EDA.csv Sample frequency =  4  Hz\n","File start time =  Sun, 24 Nov 2019 19:06:29\n","File end time   =  Sun, 24 Nov 2019 19:15:03\n","./TWristAR_temp/TEMP.csv Sample frequency =  4  Hz\n","File start time =  Sun, 24 Nov 2019 19:06:29\n","File end time   =  Sun, 24 Nov 2019 19:15:02\n","IR1 full dataframe shape (16470, 7)\n","Tag info (button presses) from tags.csv\n","    UTC_time          Local Time\n","0 1574622432.21 Sun, 24 Nov 2019 19:07:12\n","1 1574622822.72 Sun, 24 Nov 2019 19:13:42\n","Label Counts - # samples before sliding window\n"," Walking      5793\n","Jogging      5792\n","Undefined    4885\n","Name: label, dtype: int64\n","Processing  ./TWristAR/sub1/1574624998_A01F11.zip\n","Unzipping ./TWristAR/sub1/1574624998_A01F11.zip in ./TWristAR_temp\n","./TWristAR_temp/ACC.csv Sample frequency =  32  Hz\n","File start time =  Sun, 24 Nov 2019 19:49:58\n","File end time   =  Sun, 24 Nov 2019 19:57:15\n","./TWristAR_temp/BVP.csv Sample frequency =  64  Hz\n","File start time =  Sun, 24 Nov 2019 19:49:58\n","File end time   =  Sun, 24 Nov 2019 19:57:15\n","./TWristAR_temp/EDA.csv Sample frequency =  4  Hz\n","File start time =  Sun, 24 Nov 2019 19:49:58\n","File end time   =  Sun, 24 Nov 2019 19:57:14\n","./TWristAR_temp/TEMP.csv Sample frequency =  4  Hz\n","File start time =  Sun, 24 Nov 2019 19:49:58\n","File end time   =  Sun, 24 Nov 2019 19:57:13\n","IR1 full dataframe shape (13987, 7)\n","Tag info (button presses) from tags.csv\n","    UTC_time          Local Time\n","0 1574625042.71 Sun, 24 Nov 2019 19:50:42\n","1 1574625419.43 Sun, 24 Nov 2019 19:56:59\n","Label Counts - # samples before sliding window\n"," Sitting      5857\n","Standing     5632\n","Undefined    2498\n","Name: label, dtype: int64\n","Processing  ./TWristAR/sub2/1633107019_A01F11.zip\n","Unzipping ./TWristAR/sub2/1633107019_A01F11.zip in ./TWristAR_temp\n","./TWristAR_temp/ACC.csv Sample frequency =  32  Hz\n","File start time =  Fri, 01 Oct 2021 16:50:19\n","File end time   =  Fri, 01 Oct 2021 16:57:47\n","./TWristAR_temp/BVP.csv Sample frequency =  64  Hz\n","File start time =  Fri, 01 Oct 2021 16:50:19\n","File end time   =  Fri, 01 Oct 2021 16:57:47\n","./TWristAR_temp/EDA.csv Sample frequency =  4  Hz\n","File start time =  Fri, 01 Oct 2021 16:50:19\n","File end time   =  Fri, 01 Oct 2021 16:57:47\n","./TWristAR_temp/TEMP.csv Sample frequency =  4  Hz\n","File start time =  Fri, 01 Oct 2021 16:50:19\n","File end time   =  Fri, 01 Oct 2021 16:57:46\n","IR1 full dataframe shape (14358, 7)\n","Tag info (button presses) from tags.csv\n","    UTC_time          Local Time\n","0 1633107053.81 Fri, 01 Oct 2021 16:50:53\n","1 1633107451.3 Fri, 01 Oct 2021 16:57:31\n","Label Counts - # samples before sliding window\n"," Upstairs      6528\n","Downstairs    5153\n","Undefined     2677\n","Name: label, dtype: int64\n","Processing  ./TWristAR/sub2/1633108344_A01F11.zip\n","Unzipping ./TWristAR/sub2/1633108344_A01F11.zip in ./TWristAR_temp\n","./TWristAR_temp/ACC.csv Sample frequency =  32  Hz\n","File start time =  Fri, 01 Oct 2021 17:12:24\n","File end time   =  Fri, 01 Oct 2021 17:19:38\n","./TWristAR_temp/BVP.csv Sample frequency =  64  Hz\n","File start time =  Fri, 01 Oct 2021 17:12:24\n","File end time   =  Fri, 01 Oct 2021 17:19:38\n","./TWristAR_temp/EDA.csv Sample frequency =  4  Hz\n","File start time =  Fri, 01 Oct 2021 17:12:24\n","File end time   =  Fri, 01 Oct 2021 17:19:37\n","./TWristAR_temp/TEMP.csv Sample frequency =  4  Hz\n","File start time =  Fri, 01 Oct 2021 17:12:24\n","File end time   =  Fri, 01 Oct 2021 17:19:37\n","IR1 full dataframe shape (13902, 7)\n","Tag info (button presses) from tags.csv\n","    UTC_time          Local Time\n","0 1633108369.83 Fri, 01 Oct 2021 17:12:49\n","1 1633108772.23 Fri, 01 Oct 2021 17:19:32\n","Label Counts - # samples before sliding window\n"," Walking      5728\n","Jogging      5697\n","Undefined    2477\n","Name: label, dtype: int64\n","Processing  ./TWristAR/sub2/1633109744_A01F11.zip\n","Unzipping ./TWristAR/sub2/1633109744_A01F11.zip in ./TWristAR_temp\n","./TWristAR_temp/ACC.csv Sample frequency =  32  Hz\n","File start time =  Fri, 01 Oct 2021 17:35:44\n","File end time   =  Fri, 01 Oct 2021 17:42:29\n","./TWristAR_temp/BVP.csv Sample frequency =  64  Hz\n","File start time =  Fri, 01 Oct 2021 17:35:44\n","File end time   =  Fri, 01 Oct 2021 17:42:29\n","./TWristAR_temp/EDA.csv Sample frequency =  4  Hz\n","File start time =  Fri, 01 Oct 2021 17:35:44\n","File end time   =  Fri, 01 Oct 2021 17:42:28\n","./TWristAR_temp/TEMP.csv Sample frequency =  4  Hz\n","File start time =  Fri, 01 Oct 2021 17:35:44\n","File end time   =  Fri, 01 Oct 2021 17:42:27\n","IR1 full dataframe shape (12980, 7)\n","Tag info (button presses) from tags.csv\n","    UTC_time          Local Time\n","0 1633109768.92 Fri, 01 Oct 2021 17:36:08\n","1 1633110143.53 Fri, 01 Oct 2021 17:42:23\n","Label Counts - # samples before sliding window\n"," Sitting      5825\n","Standing     5696\n","Undefined    1459\n","Name: label, dtype: int64\n","Processing  ./TWristAR/sub3/1633704587_A01F11.zip\n","Unzipping ./TWristAR/sub3/1633704587_A01F11.zip in ./TWristAR_temp\n","./TWristAR_temp/ACC.csv Sample frequency =  32  Hz\n","File start time =  Fri, 08 Oct 2021 14:49:47\n","File end time   =  Fri, 08 Oct 2021 14:58:52\n","./TWristAR_temp/BVP.csv Sample frequency =  64  Hz\n","File start time =  Fri, 08 Oct 2021 14:49:47\n","File end time   =  Fri, 08 Oct 2021 14:58:52\n","./TWristAR_temp/EDA.csv Sample frequency =  4  Hz\n","File start time =  Fri, 08 Oct 2021 14:49:47\n","File end time   =  Fri, 08 Oct 2021 14:58:51\n","./TWristAR_temp/TEMP.csv Sample frequency =  4  Hz\n","File start time =  Fri, 08 Oct 2021 14:49:47\n","File end time   =  Fri, 08 Oct 2021 14:58:52\n","IR1 full dataframe shape (17464, 7)\n","Tag info (button presses) from tags.csv\n","    UTC_time          Local Time\n","0 1633704630.48 Fri, 08 Oct 2021 14:50:30\n","1 1633705123.58 Fri, 08 Oct 2021 14:58:43\n","Label Counts - # samples before sliding window\n"," Upstairs      6784\n","Downstairs    5985\n","Undefined     4695\n","Name: label, dtype: int64\n","Processing  ./TWristAR/sub3/1633705664_A01F11.zip\n","Unzipping ./TWristAR/sub3/1633705664_A01F11.zip in ./TWristAR_temp\n","./TWristAR_temp/ACC.csv Sample frequency =  32  Hz\n","File start time =  Fri, 08 Oct 2021 15:07:44\n","File end time   =  Fri, 08 Oct 2021 15:18:17\n","./TWristAR_temp/BVP.csv Sample frequency =  64  Hz\n","File start time =  Fri, 08 Oct 2021 15:07:44\n","File end time   =  Fri, 08 Oct 2021 15:18:17\n","./TWristAR_temp/EDA.csv Sample frequency =  4  Hz\n","File start time =  Fri, 08 Oct 2021 15:07:44\n","File end time   =  Fri, 08 Oct 2021 15:18:16\n","./TWristAR_temp/TEMP.csv Sample frequency =  4  Hz\n","File start time =  Fri, 08 Oct 2021 15:07:44\n","File end time   =  Fri, 08 Oct 2021 15:18:17\n","IR1 full dataframe shape (20265, 7)\n","Tag info (button presses) from tags.csv\n","    UTC_time          Local Time\n","0 1633705691.39 Fri, 08 Oct 2021 15:08:11\n","1 1633706289.61 Fri, 08 Oct 2021 15:18:09\n","Label Counts - # samples before sliding window\n"," Undefined    8264\n","Walking      6273\n","Jogging      5728\n","Name: label, dtype: int64\n","Processing  ./TWristAR/sub3/1633711821_A01F11.zip\n","Unzipping ./TWristAR/sub3/1633711821_A01F11.zip in ./TWristAR_temp\n","./TWristAR_temp/ACC.csv Sample frequency =  32  Hz\n","File start time =  Fri, 08 Oct 2021 16:50:21\n","File end time   =  Fri, 08 Oct 2021 16:57:15\n","./TWristAR_temp/BVP.csv Sample frequency =  64  Hz\n","File start time =  Fri, 08 Oct 2021 16:50:21\n","File end time   =  Fri, 08 Oct 2021 16:57:15\n","./TWristAR_temp/EDA.csv Sample frequency =  4  Hz\n","File start time =  Fri, 08 Oct 2021 16:50:21\n","File end time   =  Fri, 08 Oct 2021 16:57:14\n","./TWristAR_temp/TEMP.csv Sample frequency =  4  Hz\n","File start time =  Fri, 08 Oct 2021 16:50:21\n","File end time   =  Fri, 08 Oct 2021 16:57:12\n","IR1 full dataframe shape (13254, 7)\n","Tag info (button presses) from tags.csv\n","    UTC_time          Local Time\n","0 1633711846.47 Fri, 08 Oct 2021 16:50:46\n","1 1633712227.66 Fri, 08 Oct 2021 16:57:07\n","Label Counts - # samples before sliding window\n"," Standing     6144\n","Sitting      5665\n","Undefined    1445\n","Name: label, dtype: int64\n","array    shape          object type              data type\n","-------  -------------  -----------------------  -----------\n","X:       (3168, 96, 7)  <class 'numpy.ndarray'>  float32\n","y:       (3168, 1)      <class 'numpy.ndarray'>  int8\n","sub:     (3168, 1)      <class 'numpy.ndarray'>  uint8\n"]}]},{"cell_type":"code","source":["def limit_channel_ir3(ir3_X, \n","                      all_channel_list = ['accel_x', 'accel_y', 'accel_z', 'accel_ttl', 'bvp', 'eda', 'p_temp'],\n","                      keep_channel_list = [\"accel_ttl\"]):\n","    \"\"\"Pass the full ir3_X array with all channels, the stored all_channel_list\n","    that was extracted from the ir1 dataframe column names, and a \n","    keep_channel_list.  Matching channels will be kept, all others dropped.\n","    This would have been much easier at IR1 but that would precluded channel \n","    experiments and by channel feature representations.\n","    This is really new code, I'm leaving in some commented statements for now\"\"\"\n","    ch_idx = []\n","    # should add check here for channels not in list\n","    for i in keep_channel_list:\n","        ch_idx.append(all_channel_list.index(i)) \n","    if verbose:\n","        print(\"Keeping X columns at index\", ch_idx)\n","    new_X = ir3_X[:,:,ch_idx]\n","    return new_X\n","if interactive:\n","    print(\"all_channel_list\", all_channel_list)\n","    print(\"starting X shape\", my_X.shape)\n","    print(\"first row\", my_X[0,0,:])\n","    my_new_X = limit_channel_ir3(my_X,\n","                                 keep_channel_list = ['accel_ttl','p_temp'])\n","    print(\"ending X shape\", my_new_X.shape)\n","    print(\"first row\", my_new_X[0,0,:])"],"metadata":{"id":"c1WYWW5jzf2-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677531771209,"user_tz":360,"elapsed":107,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}},"outputId":"f56def48-7822-439d-d740-a651682d8d36"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["all_channel_list ['accel_x', 'accel_y', 'accel_z', 'accel_ttl', 'bvp', 'eda', 'p_temp']\n","starting X shape (544, 96, 7)\n","first row [-3.1250000e-02 -4.0625000e-01  8.9062500e-01 -2.0597879e-02\n"," -0.0000000e+00  0.0000000e+00  3.1830000e+01]\n","Keeping X columns at index [3, 6]\n","ending X shape (544, 96, 2)\n","first row [-2.0597879e-02  3.1830000e+01]\n"]}]},{"cell_type":"code","metadata":{"id":"trfLorthy59i","executionInfo":{"status":"ok","timestamp":1677531775583,"user_tz":360,"elapsed":272,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}}},"source":["def twristar_load_dataset(\n","    verbose = False,\n","    use_saved_xysub = False, # get X,y,sub from zip, True = faster to used saved ones\n","    incl_val_group = False, # split train into train and validate\n","    split_subj = dict\n","                (train_subj = [1,2],\n","                validation_subj = [],\n","                test_subj = [3]),\n","    keep_channel_list = ['accel_ttl'],\n","    one_hot_encode = True, # make y into multi-column one-hot, one for each activity\n","    return_info_dict = False, # return dict of meta info along with ndarrays\n","    suppress_warn = False # special case for stratified warning\n","    ):\n","    \"\"\"Downloads the TWristAR dataset from Zenodo, processes the data, and\n","    returns arrays by separating into _train, _validate, and _test arrays for\n","    X and y based on split_sub dictionary.\"\"\"\n","    log_info = \"Generated by TWristAR_load_data.ipynb\\n\"\n","    today = date.today()\n","    log_info += today.strftime(\"%B %d, %Y\") + \"\\n\"\n","    log_info += \"sub dict = \" + str(split_subj) + \"\\n\"\n","    if (not use_saved_xysub):\n","        X, y, sub, xys_info = get_ir3()\n","    else:\n","        # read previously stored X, y, sub arrays instead of creating from zip\n","        # saves time when running multiple train/test split experiments\n","        # e.g. passing different subject dictionaries to this method\n","        # create & save X, y, sub using Leotta_2021_get_X_y_sub.ipynb interactively\n","        # this is not a big deal for TWristAR which is purposely small.\n","        input_dir = '/content/drive/MyDrive/Processed_Datasets/shl/ir3_20hz'\n","        X = np.load(input_dir + '/'+'X.npy')\n","        y = np.load(input_dir + '/'+'y.npy')\n","        sub = np.load(input_dir + '/'+'sub.npy')\n","        log_info += \"X, y, sub loaded from \"+str(input_dir)+\"\\n\"\n","    # Drop unwanted channels from X\n","    log_info += \"Keeping channels\" + str(keep_channel_list) + \"\\n\"\n","    X = limit_channel_ir3(X, keep_channel_list = keep_channel_list)\n","    # write initial array info to log_info\n","    headers = (\"Initial Array\",\"shape\", \"object type\", \"data type\")\n","    mydata = [(\"X\", X.shape, type(X), X.dtype),\n","              (\"y\", y.shape, type(y), y.dtype),\n","              (\"sub\", sub.shape, type(sub), sub.dtype)]\n","    if (verbose):\n","        print(tabulate(mydata, headers=headers))\n","    log_info += tabulate(mydata, headers=headers) + \"\\n\"\n","    \n"," \n","    #One-Hot-Encode y...there must be a better way when starting with strings\n","    #https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n","    # Need to look at https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n","\n","    if (one_hot_encode):\n","        # integer encode\n","        y_vector = np.ravel(y) #encoder won't take column vector\n","        le = LabelEncoder()\n","        integer_encoded = le.fit_transform(y_vector) #convert from string to int\n","        name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n","        if (verbose):\n","            print(\"One-hot-encoding: category names -> int -> one-hot \\n\")\n","            print(name_mapping) # seems risky as interim step before one-hot\n","        log_info += \"One Hot:\" + str(name_mapping) +\"\\n\\n\"\n","        onehot_encoder = OneHotEncoder(sparse=False)\n","        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n","        onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n","        y=onehot_encoded.astype('uint8')\n","        #return X,y\n","    # split by subject number pass in dictionary\n","    sub_num = np.ravel(sub[ : , 0] ) # convert shape to (1047,)\n","    # this code is different from typical due to limited subjects,\n","    # all not test subjects data is placed into train which is then \n","    # split using stratification - validation group is not sub independent\n","    train_index = np.nonzero(np.isin(sub_num, split_subj['train_subj'] + \n","                                        split_subj['validation_subj']))\n","    x_train = X[train_index]\n","    y_train = y[train_index]\n","    if (incl_val_group):\n","        if not suppress_warn:\n","            print(\"Warning: Due to limited subjects the validation group is a stratified\")\n","            print(\"90/10 split of the training group.  It is not subject independent.\")\n","        # split training into training + validate using stratify - note that the\n","        # validation set is not subject independent (hard to achieve with limited\n","        # subjects).   The test set however is subject independent and as a result\n","        # will have much lower accuracy.  Another option is to tag a few of the\n","        # activities for inclusion in validation.  See\n","        # https://github.com/imics-lab/Semi-Supervised-HAR-e4-Wristband\n","        # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n","        x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=0.10, random_state=42, stratify=y_train)\n","\n","    test_index = np.nonzero(np.isin(sub_num, split_subj['test_subj']))\n","    x_test = X[test_index]\n","    y_test = y[test_index]\n","\n","    headers = (\"Returned Array\",\"shape\", \"object type\", \"data type\")\n","    mydata = [(\"x_train:\", x_train.shape, type(x_train), x_train.dtype),\n","                    (\"y_train:\", y_train.shape ,type(y_train), y_train.dtype)]\n","    if (incl_val_group):\n","        mydata += [(\"x_validation:\", x_validation.shape, type(x_validation), x_validation.dtype),\n","                        (\"y_validation:\", y_validation.shape ,type(y_validation), y_validation.dtype)]\n","    mydata += [(\"x_test:\", x_test.shape, type(x_test), x_test.dtype),\n","                    (\"y_test:\", y_test.shape ,type(y_test), y_test.dtype)]\n","    if (verbose):\n","        print(tabulate(mydata, headers=headers))\n","    log_info += tabulate(mydata, headers=headers)\n","    if (incl_val_group):\n","        if (return_info_dict):\n","            return x_train, y_train, x_validation, y_validation, x_test, y_test, log_info\n","        else:\n","            return x_train, y_train, x_validation, y_validation, x_test, y_test\n","    else:\n","        if (return_info_dict):\n","            return x_train, y_train, x_test, y_test, log_info\n","        else:\n","            return x_train, y_train, x_test, y_test\n"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["# Main is setup to be a demo and bit of unit test."],"metadata":{"id":"tncwIiZaB3j3"}},{"cell_type":"code","source":["#print(utils.tabulate_numpy_arrays({'ir2_X': ir2_X, 'ir2_y':ir2_y, 'ir2_sub':ir2_sub,\n","#                            'ir2_ss_time':ir2_ss_time}))"],"metadata":{"id":"OelkhXxAsr5Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MaT1dfqavvtk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677531787193,"user_tz":360,"elapsed":3772,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}},"outputId":"acf3a6e6-88c4-444b-d667-d4ec127f706b"},"source":["if __name__ == \"__main__\":\n","    verbose = False\n","    print(\"Get TWristAR using defaults - simple and easy!\")\n","    x_train, y_train, x_test, y_test \\\n","                             = twristar_load_dataset()\n","    headers = (\"Array\",\"shape\", \"data type\")\n","    mydata = [(\"x_train:\", x_train.shape, x_train.dtype),\n","            (\"y_train:\", y_train.shape, y_train.dtype),\n","            (\"x_test:\", x_test.shape, x_test.dtype),\n","            (\"y_test:\", y_test.shape, y_test.dtype)]\n","    print(\"\\n\",tabulate(mydata, headers=headers))\n","    print ('\\n','-'*72)\n","\n","    print(\"Get TWristAR with validation group, info file, and four channels\\n\")\n","    x_train, y_train, x_valid, y_valid, x_test, y_test, log_info \\\n","                             = twristar_load_dataset(\n","                                 incl_val_group = True,\n","                                 keep_channel_list = ['accel_ttl','bvp',\n","                                                      'eda', 'p_temp'],\n","                                 return_info_dict = True)\n","\n","    headers = (\"Array\",\"shape\", \"data type\")\n","    mydata = [(\"x_train:\", x_train.shape, x_train.dtype),\n","            (\"y_train:\", y_train.shape, y_train.dtype),\n","            (\"x_valid:\", x_valid.shape, x_valid.dtype),\n","            (\"y_valid:\", y_valid.shape, y_valid.dtype),\n","            (\"x_test:\", x_test.shape, x_test.dtype),\n","            (\"y_test:\", y_test.shape, y_test.dtype)]\n","    print(\"\\n\",tabulate(mydata, headers=headers))\n","    print(\"\\n----------- Contents of returned log_info ---------------\")\n","    print(log_info)\n","    print(\"\\n------------- End of returned log_info -----------------\")\n","    print(\"Get TWristAR with validation group, no warn, and bvp only\\n\")\n","    x_train, y_train, x_valid, y_valid, x_test, y_test \\\n","                             = twristar_load_dataset(\n","                                 incl_val_group = True,\n","                                 keep_channel_list = ['bvp'],\n","                                 return_info_dict = False,\n","                                 suppress_warn = True)\n","    print(\"This is a no output config - silent execution\")\n","    headers = (\"Array\",\"shape\", \"data type\")\n","    mydata = [(\"x_train:\", x_train.shape, x_train.dtype),\n","            (\"y_train:\", y_train.shape, y_train.dtype),\n","            (\"x_valid:\", x_valid.shape, x_valid.dtype),\n","            (\"y_valid:\", y_valid.shape, y_valid.dtype),\n","            (\"x_test:\", x_test.shape, x_test.dtype),\n","            (\"y_test:\", y_test.shape, y_test.dtype)]\n","    print(\"\\n\",tabulate(mydata, headers=headers))\n","    print(\"Get TWristAR with validation group, and accel only\\n\")\n","    x_train, y_train, x_valid, y_valid, x_test, y_test, log_accelxyz\\\n","                             = twristar_load_dataset(\n","                                 incl_val_group = True,\n","                                 keep_channel_list = ['accel_x', 'accel_y', 'accel_z', 'accel_ttl'],\n","                                 return_info_dict = True,\n","                                 suppress_warn = True)\n","    headers = (\"Array\",\"shape\", \"data type\")\n","    mydata = [(\"x_train:\", x_train.shape, x_train.dtype),\n","            (\"y_train:\", y_train.shape, y_train.dtype),\n","            (\"x_valid:\", x_valid.shape, x_valid.dtype),\n","            (\"y_valid:\", y_valid.shape, y_valid.dtype),\n","            (\"x_test:\", x_test.shape, x_test.dtype),\n","            (\"y_test:\", y_test.shape, y_test.dtype)]\n","    print(\"\\n\",tabulate(mydata, headers=headers))\n","    print(\"\\n----------- Contents of returned log_info ---------------\")\n","    print(log_accelxyz)\n","    print(\"\\n------------- End of returned log_info -----------------\")"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Get TWristAR using defaults - simple and easy!\n","\n"," Array     shape          data type\n","--------  -------------  -----------\n","x_train:  (2077, 96, 1)  float32\n","y_train:  (2077, 6)      uint8\n","x_test:   (1091, 96, 1)  float32\n","y_test:   (1091, 6)      uint8\n","\n"," ------------------------------------------------------------------------\n","Get TWristAR with validation group, info file, and four channels\n","\n","Warning: Due to limited subjects the validation group is a stratified\n","90/10 split of the training group.  It is not subject independent.\n","\n"," Array     shape          data type\n","--------  -------------  -----------\n","x_train:  (1869, 96, 4)  float32\n","y_train:  (1869, 6)      uint8\n","x_valid:  (208, 96, 4)   float32\n","y_valid:  (208, 6)       uint8\n","x_test:   (1091, 96, 4)  float32\n","y_test:   (1091, 6)      uint8\n","\n","----------- Contents of returned log_info ---------------\n","Generated by TWristAR_load_data.ipynb\n","February 27, 2023\n","sub dict = {'train_subj': [1, 2], 'validation_subj': [], 'test_subj': [3]}\n","Keeping channels['accel_ttl', 'bvp', 'eda', 'p_temp']\n","Initial Array    shape          object type              data type\n","---------------  -------------  -----------------------  -----------\n","X                (3168, 96, 4)  <class 'numpy.ndarray'>  float32\n","y                (3168, 1)      <class 'numpy.ndarray'>  int8\n","sub              (3168, 1)      <class 'numpy.ndarray'>  uint8\n","One Hot:{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}\n","\n","Returned Array    shape          object type              data type\n","----------------  -------------  -----------------------  -----------\n","x_train:          (1869, 96, 4)  <class 'numpy.ndarray'>  float32\n","y_train:          (1869, 6)      <class 'numpy.ndarray'>  uint8\n","x_validation:     (208, 96, 4)   <class 'numpy.ndarray'>  float32\n","y_validation:     (208, 6)       <class 'numpy.ndarray'>  uint8\n","x_test:           (1091, 96, 4)  <class 'numpy.ndarray'>  float32\n","y_test:           (1091, 6)      <class 'numpy.ndarray'>  uint8\n","\n","------------- End of returned log_info -----------------\n","Get TWristAR with validation group, no warn, and bvp only\n","\n","This is a no output config - silent execution\n","\n"," Array     shape          data type\n","--------  -------------  -----------\n","x_train:  (1869, 96, 1)  float32\n","y_train:  (1869, 6)      uint8\n","x_valid:  (208, 96, 1)   float32\n","y_valid:  (208, 6)       uint8\n","x_test:   (1091, 96, 1)  float32\n","y_test:   (1091, 6)      uint8\n","Get TWristAR with validation group, and accel only\n","\n","\n"," Array     shape          data type\n","--------  -------------  -----------\n","x_train:  (1869, 96, 4)  float32\n","y_train:  (1869, 6)      uint8\n","x_valid:  (208, 96, 4)   float32\n","y_valid:  (208, 6)       uint8\n","x_test:   (1091, 96, 4)  float32\n","y_test:   (1091, 6)      uint8\n","\n","----------- Contents of returned log_info ---------------\n","Generated by TWristAR_load_data.ipynb\n","February 27, 2023\n","sub dict = {'train_subj': [1, 2], 'validation_subj': [], 'test_subj': [3]}\n","Keeping channels['accel_x', 'accel_y', 'accel_z', 'accel_ttl']\n","Initial Array    shape          object type              data type\n","---------------  -------------  -----------------------  -----------\n","X                (3168, 96, 4)  <class 'numpy.ndarray'>  float32\n","y                (3168, 1)      <class 'numpy.ndarray'>  int8\n","sub              (3168, 1)      <class 'numpy.ndarray'>  uint8\n","One Hot:{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}\n","\n","Returned Array    shape          object type              data type\n","----------------  -------------  -----------------------  -----------\n","x_train:          (1869, 96, 4)  <class 'numpy.ndarray'>  float32\n","y_train:          (1869, 6)      <class 'numpy.ndarray'>  uint8\n","x_validation:     (208, 96, 4)   <class 'numpy.ndarray'>  float32\n","y_validation:     (208, 6)       <class 'numpy.ndarray'>  uint8\n","x_test:           (1091, 96, 4)  <class 'numpy.ndarray'>  float32\n","y_test:           (1091, 6)      <class 'numpy.ndarray'>  uint8\n","\n","------------- End of returned log_info -----------------\n"]}]}]}