{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1WbviRoNfMEZwPiA0Jm0FruV9l8tODu_e","timestamp":1656703965031},{"file_id":"1RkiXI3GhB-rNtyUp_VYw05xiiuD_oDFA","timestamp":1612028534003}],"authorship_tag":"ABX9TyNNFfS/v+GiaL293FEbdmVr"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Khc4g511HMYk"},"source":["#TWristAR_load_dataset.ipynb\n","Loads the raw e4 signals and .csv label files from the [Zenodo repository](https://zenodo.org/record/5911808) and returns train and test X/y numpy arrays.\n","\n","The basic flow is:\n","* Download and unzip the dataset if not already present\n","* Convert each recording *session* into Intermediate Representation 1 (IR1) format - a datetime indexed pandas dataframe with columns for each channel plus the label and subject number.\n","* Put all IR1 dataframes into a dictionary with key = source filename\n","* Allocate the IR1s into train and test IR2s based on subject dictionary.  A single IR1 will generate multiple sliding window instances.\n","   * X = (instances, time steps per instance, channels)  \n","   * y =  (instances, label) # activity classification  \n","   * s =  (instances, sub) # subject number\n","   * ss_time = (instances, 2) # start and stop time of the window\n","* Clean and further transform the IR2 arrays as needed - note the transforms that can be applied here are train vs test dependent.   For example, the IR2 arrays in the training set may be dropped if multi-class or rebalanced, but those in the test set should not.\n","* Concatenate the processed IR2 arrays into the final returned train/validate/test arrays.\n","\n","TWRistAR is small and easily downloadable so there is no option to used saved Intermediate Representations here as there is in some of the loaders for larger datasets.\n","\n","Set interactive to true to run the Jupyter Notebook version.  Note most of the interactive calls are setup to test the functions, not process the entire dataset, to do that set interactive to false and run all so that main executes.   This notebook can be saved and run as a python file as well.\n","\n","This video describes the code https://mediaflo.txstate.edu/Watch/e4_data_processing. (many updates have been made since this was recorded)\n","\n","\n","Acknowledgement to and a good example of the WISDM format being pre-processed is https://towardsdatascience.com/human-activity-recognition-har-tutorial-with-keras-and-core-ml-part-1-8c05e365dfa0  by Nils Ackermann.  \n","\n","\n","<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.\n","\n","[Lee B. Hinkle](https://userweb.cs.txstate.edu/~lbh31/), Texas State University, [IMICS Lab](https://imics.wp.txstate.edu/)  \n","TODO:\n","* Time is off by 6 hrs due to time zone issues - adjusted in Excel/csv but would be good to show it in the correct time zone.\n","* The train and test groups for scripted activities are handled identically which is probably OK for TWristAR since it is balanced but it would be better to separate the big X, y, sub arrays out before dropping windows etc.\n","* Need to incorporate session numbers or just use the alternate .csv files where validation was 'fake' subs 11 and 22 which were just a few of the sessions from subjects 1 and 2.  This was done in the Semi-Supervised version of the loader for WISHWell but not integrated back into this version.\n"]},{"cell_type":"markdown","source":["# Import Libraries"],"metadata":{"id":"ghyvAqipvbvs"}},{"cell_type":"code","metadata":{"id":"q6H67o-YARCx"},"source":["import os\n","import shutil #https://docs.python.org/3/library/shutil.html\n","from shutil import unpack_archive # to unzip\n","import urllib.request # to get files from web w/o !wget\n","\n","import time\n","from time import gmtime, strftime, localtime #for displaying Linux UTC timestamps in hh:mm:ss\n","from datetime import datetime, date\n","\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FagULQSH-69Z"},"outputs":[],"source":["def get_web_file(fname, url):\n","    \"\"\"checks for local file, if none downloads from URL.    \n","    :return: nothing\"\"\"\n","    if (os.path.exists(fname)):\n","        print (\"Local\",fname, \"found, skipping download\")\n","    else:\n","        print(\"Downloading\",fname, \"from\", url)\n","        urllib.request.urlretrieve(url, filename=fname)"]},{"cell_type":"markdown","source":["# Load shared transform (xforms) functions and utils from IMICS Public Repo\n","\n"],"metadata":{"id":"GZ3Jm4r354nl"}},{"cell_type":"code","source":["try:\n","    import load_data_transforms as xforms\n","except:\n","    get_web_file(fname = 'load_data_transforms.py', url = 'https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/load_data_transforms.py')\n","    import load_data_transforms as xforms\n","\n","try:\n","    import load_data_utils as utils  \n","except:\n","    get_web_file(fname = 'load_data_utils.py', url = 'https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/load_data_utils.py')\n","    import load_data_utils as utils"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t7qLPe1l4h2V","executionInfo":{"status":"ok","timestamp":1684262471316,"user_tz":300,"elapsed":8531,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}},"outputId":"12f8ccb9-f9e0-4798-a1e8-d36e87367af6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading load_data_transforms.py from https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/load_data_transforms.py\n","Downloading load_data_utils.py from https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/load_data_utils.py\n"]}]},{"cell_type":"markdown","source":["# Global and Dataset Parameters"],"metadata":{"id":"iDsgBVo_BFkc"}},{"cell_type":"code","metadata":{"id":"1LkvTO5hujPH"},"source":["# environment and execution parameters\n","my_dir = '.' # replace with absolute path if desired\n","dataset_dir = my_dir # TWristAR zip file contains TWristAR directory\n","working_dir = os.path.join(my_dir,'TWristAR_temp') # temp dir for processing\n","\n","if not os.path.exists(working_dir):\n","    os.makedirs(working_dir)\n","interactive = True # for exploring data and functions interactively\n","verbose = True\n","\n","log_info = \"\" # a global to append dataset processing info\n","\n","# dataset parameters\n","all_channel_list = ['accel_x', 'accel_y', 'accel_z','accel_ttl','bvp','eda','p_temp']\n","# frequency = 32 - unlike some of the other loaders this is hardcoded due to\n","# the unique sample freqencies that differ between the individual e4 sensors\n","\n","# I don't think this gets called when using the loader - moving into the\n","# load data method to be sure.\n","# xforms.time_steps = 96 # three seconds at 32Hz\n","# xforms.stride = 32 # one second step for each sliding window\n","\n","# The label_map_<dataset> contains a mapping from strings to ints for all\n","# possible labels in the entire dataset.   This allows for predictable conversion\n","# regardless of the slices.  I'm using 99 for 'unknown' which will be dropped\n","# to avoid the confusion of shifing by 1 place, zero indexed etc.\n","# Also this label map dict is setup to handle multi-labels but TWRristAR \n","# has only a single activity label.\n","subj_alloc_dict = dict (train_subj = [1,2], valid_subj = [], test_subj = [3])\n","label_map_twristar = {\"label\":     {\"Downstairs\": 0, \"Jogging\": 1, \"Sitting\": 2,\n","                                \"Standing\": 3, \"Upstairs\": 4, \"Walking\": 5,\n","                                \"Undefined\": 99}}\n","scripted = True # TWristAR has two categories of data - scripted activities\n","                # and unscripted, set to false to get the unscripted data.\n","                # See example in bottom of this notebook."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_arxQU-n6nKK"},"source":["interactive = False # don't run if interactive, automatically runs for .py version\n","verbose = False # to limit the called functions output"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_TWristAR():\n","    \"\"\"checks for local zipfile, if none downloads from zenodo repository\n","    after download will unzip the dataset into TWristAR directory.\n","    Assumes a global my_dir has been defined (default is my_dir = \".\")\n","    :return: nothing\"\"\"\n","    zip_ffname = os.path.join(my_dir,'TWristAR.zip')\n","    if (os.path.exists(zip_ffname)):\n","        if verbose:\n","            print (\"Local TWristAR.zip found, skipping download\")\n","    else:\n","        print(\"Downloading TWristAR from Zenodo\")\n","        urllib.request.urlretrieve(\"https://zenodo.org/record/5911808/files/TWristAR.zip\", filename=\"TWristAR.zip\")\n","    if (os.path.isdir(os.path.join(dataset_dir,'TWristAR'))):\n","        if verbose:\n","            print(\"Found existing TWristAR directory, skipping unzip\")\n","        return\n","    else:\n","        print(\"Unzipping TWristAR file in\", dataset_dir, \"directory\")\n","        if (os.path.exists(zip_ffname)):\n","            shutil.unpack_archive(zip_ffname,dataset_dir,'zip')\n","        else:\n","            print(\"Error: \", zip_ffname, \" not found, exiting\")\n","            return\n","if interactive:\n","    get_TWristAR()"],"metadata":{"id":"tlKEXl-BSrLO"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9oab3XMPgL8Z"},"source":["def unzip_e4_file(zip_ffname):\n","    \"\"\"checks for local copy, if none unzips the e4 zipfile in working_dir\n","    Note:  the files themselves do not contain subject info and there are\n","    multiple files e.g. ACC.csv, BVP,csv etc, in each zipfile.\n","    It is very important to further process the files with <fname>_labels.csv\n","    :param zip_ffname: the path and filename of the zip file\n","    :param working_dir: local (colab) directory where csv files will be placed\n","    :return: nothing\"\"\"\n","    if not os.path.exists(working_dir):\n","        print(\"Error working directory\", working_dir, \"not found, unzip_e4_file exiting\")\n","        return\n","    if (os.path.exists(zip_ffname)):\n","        if verbose:\n","            print(\"Unzipping\",zip_ffname, \"in\", working_dir)\n","        shutil.unpack_archive(zip_ffname,working_dir,'zip')\n","    else:\n","        print(\"Error: \", zip_ffname, \" not found, exiting\")\n","        return\n","if interactive:\n","    zip_ffname = os.path.join(my_dir,'TWristAR','sub1/1574621345_A01F11.zip')\n","    unzip_e4_file(zip_ffname)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"65c2wpiOwVmg"},"source":["def df_from_e4_csv (ffname,col_labels):\n","    \"\"\"\"reads e4 ACC, BVP, EDA, or TEMP(erature) csv files, uses start time and\n","    sample rate to create time indexed pandas dataframe with columns.  \n","    Note the other e4 files have different format and must be read seperately. \n","    :param ffname:  full filename e.g./content/temp/ACC.csv\n","    :col_labels: list of colums in csv - varies by type ['accel_x','accel_y...]\n","    :returns df: time indexed dataframe\"\"\"\n","\n","    df = pd.read_csv(ffname, header=None)\n","    start_time = df.iloc[0,0].astype('int64') # first line in e4 csv\n","    sample_freq = df.iloc[1,0].astype('int64') # second line in e4 csv\n","    df = df.drop(df.index[[0,1]]) # drop 1st two rows, index is now off by 2\n","    # Convert the index to datetime to allow for pandas resampling\n","    # The start_time from the e4 csv file is forced to int64 which represents the\n","    # number of nanoseconds since January 1, 1970, 00:00:00 (UTC)\n","    # This is tricky - if float representation the join function may not work\n","    # properly later since the indexes must match exactly.\n","    # UTC_time is computed for each row, then made into required datetime format\n","    # which is a int64 that pandas will accept as an index\n","    df['UTC_time'] = (df.index-2)/sample_freq + start_time\n","    end_time = df['UTC_time'].iloc[-1]\n","    if verbose:\n","        print(ffname, \"Sample frequency = \", sample_freq, \" Hz\")\n","        #show time in day month format, assumes same timezone\n","        print(\"File start time = \", strftime(\"%a, %d %b %Y %H:%M:%S\", localtime(start_time)))  \n","        print(\"File end time   = \",strftime(\"%a, %d %b %Y %H:%M:%S\", localtime(end_time)))\n","    df['datetime'] = pd.to_datetime(df['UTC_time'], unit='s')\n","    df.set_index('datetime',inplace=True)\n","    df = df.drop('UTC_time', axis=1)\n","    df.columns = col_labels\n","    return df\n","if interactive:\n","    # Note: IBI.csv is the inter-beat interval, a calculated value with a \n","    # different format.  HR.csv is also calculated from BVP but format is same.\n","    ffname = working_dir + '/ACC.csv'\n","    col_labels = ['accel_x', 'accel_y', 'accel_z']\n","    ir1_acc_df = df_from_e4_csv(ffname, col_labels)\n","    print(\"ACC dataframe shape\", ir1_acc_df.shape)\n","    display(ir1_acc_df.head())\n","\n","    ffname = working_dir + '/BVP.csv'\n","    col_labels = ['bvp']\n","    ir1_bvp_df = df_from_e4_csv(ffname, col_labels)\n","    print(\"BVP dataframe shape\", ir1_bvp_df.shape)\n","    display(ir1_bvp_df.head())\n","\n","    ffname = working_dir + '/EDA.csv'\n","    col_labels = ['eda']\n","    ir1_eda_df = df_from_e4_csv(ffname, col_labels)\n","    print(\"EDA dataframe shape\", ir1_eda_df.shape)\n","    display(ir1_eda_df.head())\n","\n","    ffname = working_dir + '/TEMP.csv'\n","    col_labels = ['p_temp']\n","    ir1_temp_df = df_from_e4_csv(ffname, col_labels)\n","    print(\"Temp dataframe shape\", ir1_temp_df.shape)\n","    display(ir1_temp_df.head())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ozk2mZVYB-Q"},"source":["def process_e4_accel(df):\n","    \"\"\"converts component accel into g and adds accel_ttl column\n","    per info.txt range is [-2g, 2g] and unit in this file is 1/64g.\n","    This method is e4 specific due to the way the accelerations is recorded\n","    \"\"\"\n","    df['accel_x'] = df['accel_x']/64\n","    df['accel_y'] = df['accel_y']/64\n","    df['accel_z'] = df['accel_z']/64\n","    df_sqd = df.pow(2)[['accel_x', 'accel_y', 'accel_z']] #square each accel\n","    df_sum = df_sqd.sum(axis=1) #add sum of squares, new 1 col df\n","    df.loc[:,'accel_ttl'] = df_sum.pow(0.5)-1  # sqrt and remove 1g due to gravity\n","    del df_sqd, df_sum\n","    return df\n","if interactive:\n","    ir1_acc_df = process_e4_accel(ir1_acc_df)\n","    display(ir1_acc_df.head())"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_ir1_from_e4_dir():\n","    \"\"\"processes the four e4 sensor files in global working directory into a \n","    single IR1 datetime indexed dataframe. Labeled columns are channels\"\"\"\n","    # Note: IBI.csv is the inter-beat interval, a calculated value with a \n","    # different format.  HR.csv is also calculated from BVP but format is same.\n","    # TODO:  Should check directory for all four files and uniform start/stop\n","    # times.\n","    # TODO: Might be better to use a different interpolation.  See\n","    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html\n","    ffname = working_dir + '/ACC.csv'\n","    col_labels = ['accel_x', 'accel_y', 'accel_z']\n","    ir1_acc_df = df_from_e4_csv(ffname, col_labels)\n","    ir1_acc_df = process_e4_accel(ir1_acc_df)\n","\n","    ffname = working_dir + '/BVP.csv'\n","    col_labels = ['bvp']\n","    ir1_bvp_df = df_from_e4_csv(ffname, col_labels)\n","\n","    ffname = working_dir + '/EDA.csv'\n","    col_labels = ['eda']\n","    ir1_eda_df = df_from_e4_csv(ffname, col_labels)\n","\n","    ffname = working_dir + '/TEMP.csv'\n","    col_labels = ['p_temp']\n","    ir1_ptemp_df = df_from_e4_csv(ffname, col_labels)\n","\n","    ir1_df = ir1_acc_df.join(ir1_bvp_df, how=\"inner\") # this drops bvp to 32Hz\n","    ir1_df = ir1_df.join(ir1_eda_df, how=\"outer\") # stays at 32Hz, eda fill NaN\n","    ir1_df = ir1_df.join(ir1_ptemp_df, how=\"outer\") # stays at 32Hz, p_temp fill NaN\n","    ir1_df = ir1_df.interpolate() # default is linear interpolation\n","    ir1_df = ir1_df.astype('float32') # no need for 64 precision with these sensors\n","    if verbose:\n","        print(\"IR1 full dataframe shape\",ir1_df.shape)\n","        #print(ir1_df.head(10))\n","    return ir1_df\n","if interactive:\n","    ir1_df = get_ir1_from_e4_dir()\n","    display(ir1_df.head(10))\n","    ir1_df.iloc[499:1999].plot(subplots=True, figsize=(20, 10)) # plot a few seconds"],"metadata":{"id":"Ax17Aew_zTUu"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YZNmr-yPWcck"},"source":["def show_e4_tag_time(tag_ffname):\n","    \"\"\"utility prints time marks from e4 tags.csv to help with video sync \n","    and labeling.   When this is run in colab it seems to be GMT regardless\n","    of timezone settings.\"\n","    :param tag_ffname: file to be processed e.g. /content/temp/tags.csv'\n","    :return: nothing\"\"\"\n","    df_temp = pd.read_csv(tag_ffname, header=None)\n","    df_temp.columns = ['UTC_time']\n","    print (\"    UTC_time          Local Time\")\n","    for index, row in df_temp.iterrows():\n","        print(index, row['UTC_time'],\n","            strftime(\"%a, %d %b %Y %H:%M:%S\", localtime(row['UTC_time'])))\n","# https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior\n","# link to string formats for date and time\n","if interactive:\n","    print('Tag info (button presses) from tags.csv')\n","    tag_ffname = working_dir + '/tags.csv'\n","    show_e4_tag_time(tag_ffname)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ttKS9Ox6JSed"},"source":["def label_df_from_csv (df, labels_ffname):\n","    \"\"\"adds class label and subject number to the dataframe based on the\n","    contents of a .csv file containing time and label info.\n","    Example csv format (see e4_time_sync.xlsx to help build csv from video)\n","        start,finish,label,sub\n","        2019:11:24 18:49:51,2019:11:24 18:50:18,Upstairs,1\n","        2019:11:24 18:50:18,2019:11:24 18:50:45,Downstairs,1\n","    :param df : time indexed dataframe from df_from_e4_csv method\n","    :labels_ffname : csv file with metadata\n","    :return : a dataframe with label and subject columns added\"\"\"\n","    df_labels = pd.read_csv(labels_ffname)\n","    df_labels['start'] =  pd.to_datetime(df_labels['start'], format='%Y:%m:%d %H:%M:%S')\n","    df_labels['finish'] =  pd.to_datetime(df_labels['finish'], format='%Y:%m:%d %H:%M:%S')\n","    # quick check to make sure all subjects are the same - only 1 sub per csv\n","    if (not (df_labels['sub'].eq(df_labels['sub'].iloc[0]).all())):\n","        print('Warning: Multiple subjects detected in csv, unusual for e4 data.')\n","    df['label']='Undefined' # add column with safe value for labels\n","    df['sub'] = np.NaN\n","    for index, row in df_labels.iterrows():\n","        #print(row['start'], row['finish'],row['label'])\n","        df.loc[row['start']:row['finish'],'label'] = row['label']\n","        df.loc[row['start']:row['finish'],'sub'] = row['sub']\n","    return df\n","if interactive:\n","    labels_ffname = os.path.splitext(zip_ffname)[0] + '_labels.csv'\n","    print(\"Adding label and sub info from \",labels_ffname)\n","    ir1_df = label_df_from_csv(ir1_df, labels_ffname)\n","    display(ir1_df[5000:5005]) # head is meaningless since start is undefined\n","    #ir1_df['label'].value_counts()\n","    print (\"Label Counts - # samples before sliding window\")\n","    print (ir1_df['label'].value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_twristar_ir1_dict():\n","    \"\"\"reads the TWRistAR dataset and converts each \"session file\" to an IR1\n","    dataframe.  The goal here is to capture and convert all raw data into\n","    a 2D dataframe of rows = datetime index of each sample, columns = {channels,\n","    label(s), subject_num}.  Additional methods may be used to drop channels,\n","    and convert the string labels to mapped ints prior to switch to ndarrays.\n","    Args:\n","    none but uses global scripted (boolean):\n","     True (default) returns scripted activity dataframes,\n","     False returns unscripted activity dataframes.\n","    Returns: a dict containing key = df_name and item = IR1 dataframes.\"\"\"\n","    # A few notes - TWRristAR (or more specifically e4 wristband datafiles)\n","    # require a lot of processing, if trying to leverage from a more traditional\n","    # .csv file format see Gesture Phase version.\n","    if scripted:\n","        fn_list = ['sub1/1574621345_A01F11.zip',\n","                    'sub1/1574622389_A01F11.zip',\n","                    'sub1/1574624998_A01F11.zip',\n","                    'sub2/1633107019_A01F11.zip',\n","                    'sub2/1633108344_A01F11.zip',\n","                    'sub2/1633109744_A01F11.zip',\n","                    'sub3/1633704587_A01F11.zip',\n","                    'sub3/1633705664_A01F11.zip',\n","                    'sub3/1633711821_A01F11.zip']\n","    else:\n","        fn_list = ['sub1/1574625540_A01F11.zip',\n","                    'sub2/1633111849_A01F11.zip']\n","    get_TWristAR()\n","    ir1_df_dict = dict() # an empty dictionary\n","    for item in fn_list:\n","        zip_ffname = os.path.join(my_dir,'TWristAR',item)\n","        if verbose:\n","            print('Processing ', zip_ffname)\n","        if not os.path.exists(working_dir):\n","            os.makedirs(working_dir)\n","        unzip_e4_file(zip_ffname)\n","        df = get_ir1_from_e4_dir()\n","        if verbose:\n","            print('Tag info (button presses) from tags.csv')\n","            tag_ffname = working_dir + '/tags.csv'\n","            show_e4_tag_time(tag_ffname)\n","        # Generate associated csv filename, forces the long numbered filenames to match\n","        labels_ffname = os.path.splitext(zip_ffname)[0] + '_labels.csv'\n","        df = label_df_from_csv (df, labels_ffname)\n","        df['label'].value_counts()\n","        if verbose:\n","            print (\"Label Counts - # samples before sliding window\\n\",df['label'].value_counts())\n","        # tighten up the column types for space savings.\n","        # change to 32-bit, credit/ref https://stackoverflow.com/questions/69188132/how-to-convert-all-float64-columns-to-float32-in-pandas\n","        # Select columns with 'float64' dtype  \n","        float64_cols = list(df.select_dtypes(include='float64'))\n","        # The same code again calling the columns\n","        df[float64_cols] = df[float64_cols].astype('float32')\n","        # Seems better to explicitly type the other columns vs object.\n","        df['label']=df['label'].astype('category')\n","        df['sub']=df['sub'].astype('category') # this is before convert to int\n","\n","        root_fname = (item.split('/')[1].split('.')[0]) # between / and .\n","        ir1_df_dict[root_fname]=df # key is root name in the file\n","    return ir1_df_dict\n","if interactive:\n","    verbose = False\n","    ir1_dict = get_twristar_ir1_dict()\n","    print('Scripted IR1 dataframes:',ir1_dict.keys())\n","    for df_name, df in ir1_dict.items():\n","        display(df.head())\n","        break # just want one\n","    scripted = False # get the free-form walk IR1s instead\n","    ir1_dict = get_twristar_ir1_dict()\n","    print('\\nUnscripted IR1 dataframes:',ir1_dict.keys())\n","    for df_name, df in ir1_dict.items():\n","        display(df.head())\n","        break # just want one\n","    scripted = True\n","    verbose = True"],"metadata":{"id":"p_EmZhbavtzi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# The dataset specific code to generate the dictionary of IR1 dataframes is complete.  Now use Shared Transforms to generate the final output arrays."],"metadata":{"id":"k4RnDlHwnYzz"}},{"cell_type":"code","metadata":{"id":"trfLorthy59i"},"source":["def twristar_load_dataset(\n","    incl_val_group = False, # split train into train and validate\n","    keep_channel_list = ['accel_ttl'],\n","    one_hot_encode = False, # make y into multi-column one-hot, one for each activity\n","    suppress_warn = False # special case for stratified warning\n","    ):\n","    \"\"\"Downloads the TWristAR dataset from Zenodo, processes the data, and\n","    returns arrays by separating into _train, _validate, and _test arrays for\n","    X and y based on split_sub dictionary.\"\"\"\n","    xforms.time_steps = 96 # three seconds at 32Hz\n","    xforms.stride = 32 # one second step for each sliding window\n","    global log_info\n","    log_info = \"Generated by TWristAR_load_data.ipynb\\n\"\n","    today = date.today()\n","    log_info += today.strftime(\"%B %d, %Y\") + \"\\n\"\n","    log_info += \"sub dict = \" + str(subj_alloc_dict) + \"\\n\"\n","    if scripted:  # this is a global variable in dataset params at top\n","        label_xform = 'drop' # for scripted activities used to train drop mixed\n","    else:\n","        label_xform = 'mode' # for unscripted assign mode label to every window \n","    ir1_dict = get_twristar_ir1_dict()\n","    X, y, sub, ss_times, xys_info = xforms.get_ir3_from_dict(ir1_dict, \n","                                                            label_map = label_map_twristar,\n","                                                            label_method = label_xform) \n","    # Drop unwanted channels from X\n","    log_info += \"Keeping channels\" + str(keep_channel_list) + \"\\n\"\n","    X = xforms.limit_channel_ir3(X, all_channel_list = all_channel_list, keep_channel_list = keep_channel_list)\n","    # write initial array info to log_info\n","    log_info += \"Initial Arrays\\n\"\n","    log_info += utils.tabulate_numpy_arrays({'X':X,'y':y,'sub':sub,'ss_times':ss_times})+'\\n'\n","\n","    if (one_hot_encode):\n","        # tried to specify list to make sure all possible classes are encoded\n","        # label_list = list(label_map_twristar['label'].keys())\n","        # then pass categories=label_list but this does not work because the\n","        # list is longer than the classes due to the inclusion of undefined.\n","        # Note sparse was changed to sparse_output but that fails on my Mac\n","        enc = OneHotEncoder(categories='auto', sparse=False)\n","        y = enc.fit_transform(y)\n","        y=y.astype('uint8')\n","        # print(enc.categories_)\n","        log_info += \"y has been one hot encoded\" + str(enc.categories_) + '\\n'\n","\n","    sub_num = np.ravel(sub[ : , 0] ) # convert shape to (1047,)\n","    # this code is different from typical due to limited subjects,\n","    # all not test subjects data is placed into train which is then \n","    # split using stratification - validation group is not sub independent\n","    train_index = np.nonzero(np.isin(sub_num, subj_alloc_dict['train_subj'] + \n","                                        subj_alloc_dict['valid_subj']))\n","    x_train = X[train_index]\n","    y_train = y[train_index]\n","    if (incl_val_group):\n","        if not suppress_warn:\n","            print(\"Warning: Due to limited subjects the validation group is a stratified\")\n","            print(\"90/10 split of the training group.  It is not subject independent.\")\n","        # split training into training + validate using stratify - note that the\n","        # validation set is not subject independent (hard to achieve with limited\n","        # subjects).   The test set however is subject independent and as a result\n","        # will have much lower accuracy.  Another option is to tag a few of the\n","        # activities for inclusion in validation.  See\n","        # https://github.com/imics-lab/Semi-Supervised-HAR-e4-Wristband\n","        # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n","        x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.10, random_state=42, stratify=y_train)\n","\n","    test_index = np.nonzero(np.isin(sub_num, subj_alloc_dict['test_subj']))\n","    x_test = X[test_index]\n","    y_test = y[test_index]\n","\n","    if (incl_val_group):\n","        log_info += utils.tabulate_numpy_arrays({'x_train': x_train, 'y_train': y_train,\n","                                       'x_valid': x_valid, 'y_valid': y_valid,\n","                                   'x_test': x_test, 'y_test': y_test})   \n","        return x_train, y_train, x_valid, y_valid, x_test, y_test\n","    else:\n","        log_info += utils.tabulate_numpy_arrays({'x_train': x_train, 'y_train': y_train,\n","                                   'x_test': x_test, 'y_test': y_test})\n","        return x_train, y_train, x_test, y_test\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Main is setup to be a demo and bit of unit test."],"metadata":{"id":"tncwIiZaB3j3"}},{"cell_type":"code","metadata":{"id":"MaT1dfqavvtk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684262486871,"user_tz":300,"elapsed":15579,"user":{"displayName":"Lee Hinkle","userId":"00071704663307985880"}},"outputId":"1b60a14f-7951-45cf-8a00-457f2c140575"},"source":["if __name__ == \"__main__\":\n","    verbose = False\n","    print(\"Get TWristAR using defaults - simple and easy!\")\n","    x_train, y_train, x_test, y_test \\\n","                             = twristar_load_dataset()\n","    print(utils.tabulate_numpy_arrays({'x_train': x_train, 'y_train': y_train,\n","                                   'x_test': x_test, 'y_test': y_test}))\n","    print ('\\n','-'*72)\n","\n","    print(\"Get TWristAR with one-hot-encoded labels - dimension of y will be 6\")\n","    x_train, y_train, x_test, y_test \\\n","                             = twristar_load_dataset(one_hot_encode=True)\n","    print(utils.tabulate_numpy_arrays({'x_train': x_train, 'y_train': y_train,\n","                                   'x_test': x_test, 'y_test': y_test}))\n","    print (\"Sum of the columns, # of one-hot instances\")\n","    print (y_train.sum(axis=0))\n","    y_labels = np.argmax(y_train, axis=-1) # undo one-hot encoding\n","    \n","    print(\"Back to integer encoded\")\n","    unique, counts = np.unique(y_labels, return_counts=True)\n","    print (np.asarray((unique, counts)).T)\n","\n","    print(\"Back to strings using xforms.get_ir2_y_string_labels and label_map\")\n","    y_strings = xforms.get_ir2_y_string_labels(y_labels, label_map = label_map_twristar)\n","    unique, counts = np.unique(y_strings, return_counts=True)\n","    print (np.asarray((unique, counts)).T)\n","    print ('\\n','-'*72)\n","\n","    print(\"Get TWristAR with validation group, info file, and four channels\\n\")\n","    x_train, y_train, x_valid, y_valid, x_test, y_test \\\n","                             = twristar_load_dataset(\n","                                 incl_val_group = True,\n","                                 keep_channel_list = ['accel_ttl','bvp',\n","                                                      'eda', 'p_temp'])\n","    print(utils.tabulate_numpy_arrays({'x_train': x_train, 'y_train': y_train,\n","                                       'x_valid': x_valid, 'y_valid': y_valid,\n","                                   'x_test': x_test, 'y_test': y_test}))\n","\n","    print(\"\\n----------- Contents of log_info ---------------\")\n","    print(log_info)\n","    print(\"\\n------------- End of log_info -----------------\")\n","    print(\"Get TWristAR with validation group, no warn, and bvp only\\n\")\n","    x_train, y_train, x_valid, y_valid, x_test, y_test \\\n","                             = twristar_load_dataset(\n","                                 incl_val_group = True,\n","                                 keep_channel_list = ['bvp'],\n","                                 suppress_warn = True)\n","    print(\"This is a no output config - silent execution\")\n","    print(utils.tabulate_numpy_arrays({'x_train': x_train, 'y_train': y_train,\n","                                       'x_valid': x_valid, 'y_valid': y_valid,\n","                                   'x_test': x_test, 'y_test': y_test}))\n","    print ('\\n','-'*72)\n","    print(\"Get TWristAR with validation group, and accel only\\n\")\n","    x_train, y_train, x_valid, y_valid, x_test, y_test \\\n","                             = twristar_load_dataset(\n","                                 incl_val_group = True,\n","                                 keep_channel_list = ['accel_x', 'accel_y', 'accel_z', 'accel_ttl'],\n","                                 suppress_warn = True)\n","    print(utils.tabulate_numpy_arrays({'x_train': x_train, 'y_train': y_train,\n","                                       'x_valid': x_valid, 'y_valid': y_valid,\n","                                   'x_test': x_test, 'y_test': y_test}))\n","    print(\"\\n----------- Contents of log_info ---------------\")\n","    print(log_info)\n","    print(\"\\n------------- End of log_info -----------------\")\n","    # Test the ability to get and process the unscripted free-form walks\n","    # These are generally treated as unlabeled sequences for our labeling work\n","    # It is setup so sub 1 walk is the train array, sub2 is the test array.\n","    # And they are in fact labeled for final validation.\n","    scripted = False # this is a global variable assigned at begining\n","    print ('\\n','-'*72)\n","    print(\"Get TWristAR Free-Form Walks - Test = Sub1, Train = Sub2\\n\")\n","    subj_alloc_dict = dict(train_subj = [1], valid_subj = [], test_subj = [2])\n","    x_train, y_train, x_test, y_test \\\n","                             = twristar_load_dataset(\n","                                 keep_channel_list = ['accel_x', 'accel_y', 'accel_z', 'accel_ttl'],\n","                                 suppress_warn = True)\n","    print(utils.tabulate_numpy_arrays({'x_train': x_train, 'y_train': y_train,\n","                                   'x_test': x_test, 'y_test': y_test}))\n","    scripted = True   # put it back where you found it!"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Get TWristAR using defaults - simple and easy!\n","Downloading TWristAR from Zenodo\n","Unzipping TWristAR file in . directory\n","array    shape          data type\n","-------  -------------  -----------\n","x_train  (2077, 96, 1)  float32\n","y_train  (2077, 1)      int8\n","x_test   (1091, 96, 1)  float32\n","y_test   (1091, 1)      int8\n","\n"," ------------------------------------------------------------------------\n","Get TWristAR with one-hot-encoded labels - dimension of y will be 6\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["array    shape          data type\n","-------  -------------  -----------\n","x_train  (2077, 96, 1)  float32\n","y_train  (2077, 6)      uint8\n","x_test   (1091, 96, 1)  float32\n","y_test   (1091, 6)      uint8\n","Sum of the columns, # of one-hot instances\n","[317 347 353 342 370 348]\n","Back to integer encoded\n","[[  0 317]\n"," [  1 347]\n"," [  2 353]\n"," [  3 342]\n"," [  4 370]\n"," [  5 348]]\n","Back to strings using xforms.get_ir2_y_string_labels and label_map\n","[['Downstairs' '317']\n"," ['Jogging' '347']\n"," ['Sitting' '353']\n"," ['Standing' '342']\n"," ['Upstairs' '370']\n"," ['Walking' '348']]\n","\n"," ------------------------------------------------------------------------\n","Get TWristAR with validation group, info file, and four channels\n","\n","Warning: Due to limited subjects the validation group is a stratified\n","90/10 split of the training group.  It is not subject independent.\n","array    shape          data type\n","-------  -------------  -----------\n","x_train  (1869, 96, 4)  float32\n","y_train  (1869, 1)      int8\n","x_valid  (208, 96, 4)   float32\n","y_valid  (208, 1)       int8\n","x_test   (1091, 96, 4)  float32\n","y_test   (1091, 1)      int8\n","\n","----------- Contents of log_info ---------------\n","Generated by TWristAR_load_data.ipynb\n","May 16, 2023\n","sub dict = {'train_subj': [1, 2], 'valid_subj': [], 'test_subj': [3]}\n","Keeping channels['accel_ttl', 'bvp', 'eda', 'p_temp']\n","Initial Arrays\n","array     shape          data type\n","--------  -------------  --------------\n","X         (3168, 96, 4)  float32\n","y         (3168, 1)      int8\n","sub       (3168, 1)      int16\n","ss_times  (3168, 2)      datetime64[ns]\n","array    shape          data type\n","-------  -------------  -----------\n","x_train  (1869, 96, 4)  float32\n","y_train  (1869, 1)      int8\n","x_valid  (208, 96, 4)   float32\n","y_valid  (208, 1)       int8\n","x_test   (1091, 96, 4)  float32\n","y_test   (1091, 1)      int8\n","\n","------------- End of log_info -----------------\n","Get TWristAR with validation group, no warn, and bvp only\n","\n","This is a no output config - silent execution\n","array    shape          data type\n","-------  -------------  -----------\n","x_train  (1869, 96, 1)  float32\n","y_train  (1869, 1)      int8\n","x_valid  (208, 96, 1)   float32\n","y_valid  (208, 1)       int8\n","x_test   (1091, 96, 1)  float32\n","y_test   (1091, 1)      int8\n","\n"," ------------------------------------------------------------------------\n","Get TWristAR with validation group, and accel only\n","\n","array    shape          data type\n","-------  -------------  -----------\n","x_train  (1869, 96, 4)  float32\n","y_train  (1869, 1)      int8\n","x_valid  (208, 96, 4)   float32\n","y_valid  (208, 1)       int8\n","x_test   (1091, 96, 4)  float32\n","y_test   (1091, 1)      int8\n","\n","----------- Contents of log_info ---------------\n","Generated by TWristAR_load_data.ipynb\n","May 16, 2023\n","sub dict = {'train_subj': [1, 2], 'valid_subj': [], 'test_subj': [3]}\n","Keeping channels['accel_x', 'accel_y', 'accel_z', 'accel_ttl']\n","Initial Arrays\n","array     shape          data type\n","--------  -------------  --------------\n","X         (3168, 96, 4)  float32\n","y         (3168, 1)      int8\n","sub       (3168, 1)      int16\n","ss_times  (3168, 2)      datetime64[ns]\n","array    shape          data type\n","-------  -------------  -----------\n","x_train  (1869, 96, 4)  float32\n","y_train  (1869, 1)      int8\n","x_valid  (208, 96, 4)   float32\n","y_valid  (208, 1)       int8\n","x_test   (1091, 96, 4)  float32\n","y_test   (1091, 1)      int8\n","\n","------------- End of log_info -----------------\n","\n"," ------------------------------------------------------------------------\n","Get TWristAR Free-Form Walks - Test = Sub1, Train = Sub2\n","\n","array    shape         data type\n","-------  ------------  -----------\n","x_train  (660, 96, 4)  float32\n","y_train  (660, 1)      int8\n","x_test   (736, 96, 4)  float32\n","y_test   (736, 1)      int8\n"]}]}]}